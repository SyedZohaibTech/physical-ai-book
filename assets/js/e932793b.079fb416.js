"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[4645],{2877:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","href":"/physical-ai-book/docs/intro","label":"Introduction","docId":"intro","unlisted":false},{"type":"category","label":"Module 1: The Robotic Nervous System (ROS 2)","items":[{"type":"link","href":"/physical-ai-book/docs/module-1-ros2/intro-to-ros2","label":"Introduction to ROS 2","docId":"module-1-ros2/intro-to-ros2","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-1-ros2/nodes-topics-services","label":"Nodes, Topics, Services","docId":"module-1-ros2/nodes-topics-services","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-1-ros2/rclpy-for-control","label":"Controlling with rclpy","docId":"module-1-ros2/rclpy-for-control","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-1-ros2/humanoid-urdf-models","label":"Humanoid URDF Models","docId":"module-1-ros2/humanoid-urdf-models","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-1-ros2/ai-agents-to-controllers","label":"AI Agents to Controllers","docId":"module-1-ros2/ai-agents-to-controllers","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-1-ros2/exercises","label":"Exercises","docId":"module-1-ros2/exercises","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2: The Digital Twin (Gazebo & Unity)","items":[{"type":"link","href":"/physical-ai-book/docs/module-2-digital-twin/what-is-a-digital-twin","label":"What is a Digital Twin?","docId":"module-2-digital-twin/what-is-a-digital-twin","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-2-digital-twin/gazebo-physics","label":"Gazebo Physics","docId":"module-2-digital-twin/gazebo-physics","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-2-digital-twin/unity-for-hri","label":"Unity for HRI","docId":"module-2-digital-twin/unity-for-hri","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-2-digital-twin/sensor-simulation","label":"Sensor Simulation","docId":"module-2-digital-twin/sensor-simulation","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-2-digital-twin/complete-humanoid-simulation","label":"Complete Humanoid Simulation","docId":"module-2-digital-twin/complete-humanoid-simulation","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-2-digital-twin/exercises","label":"Exercises","docId":"module-2-digital-twin/exercises","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)","items":[{"type":"link","href":"/physical-ai-book/docs/module-3-isaac/intro-to-isaac","label":"Intro to Isaac","docId":"module-3-isaac/intro-to-isaac","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-3-isaac/isaac-sim-photorealism","label":"Isaac Sim Photorealism","docId":"module-3-isaac/isaac-sim-photorealism","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-3-isaac/isaac-ros-perception","label":"Isaac ROS Perception","docId":"module-3-isaac/isaac-ros-perception","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-3-isaac/vslam-and-depth","label":"VSLAM and Depth","docId":"module-3-isaac/vslam-and-depth","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-3-isaac/nav2-path-planning","label":"Nav2 Path Planning","docId":"module-3-isaac/nav2-path-planning","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-3-isaac/ai-controlled-humanoid-brain","label":"AI-Controlled Humanoid Brain","docId":"module-3-isaac/ai-controlled-humanoid-brain","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-3-isaac/exercises","label":"Exercises","docId":"module-3-isaac/exercises","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4: Vision-Language-Action (VLA)","items":[{"type":"link","href":"/physical-ai-book/docs/module-4-vla/vla-future-of-robotics","label":"VLA Future of Robotics","docId":"module-4-vla/vla-future-of-robotics","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-4-vla/whisper-voice-commands","label":"Whisper Voice Commands","docId":"module-4-vla/whisper-voice-commands","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-4-vla/llm-to-ros-actions","label":"LLM to ROS Actions","docId":"module-4-vla/llm-to-ros-actions","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-4-vla/cognitive-planning-pipelines","label":"Cognitive Planning Pipelines","docId":"module-4-vla/cognitive-planning-pipelines","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-4-vla/integrating-vla","label":"Integrating VLA","docId":"module-4-vla/integrating-vla","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/module-4-vla/exercises","label":"Exercises","docId":"module-4-vla/exercises","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Capstone Project: The Autonomous Humanoid","items":[{"type":"link","href":"/physical-ai-book/docs/capstone-project/project-walkthrough","label":"Project Walkthrough","docId":"capstone-project/project-walkthrough","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"link","href":"/physical-ai-book/docs/glossary","label":"Glossary","docId":"glossary","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/appendix","label":"Appendix","docId":"appendix","unlisted":false}]},"docs":{"appendix":{"id":"appendix","title":"Appendix","description":"This appendix provides quick reference guides and supplementary information to assist you in your robotics development journey.","sidebar":"tutorialSidebar"},"capstone-project/project-walkthrough":{"id":"capstone-project/project-walkthrough","title":"1. Capstone Project Walkthrough","description":"Congratulations on making it to the Capstone Project! This is where you\'ll bring together all the knowledge and skills you\'ve gained throughout the textbook to build a fully autonomous humanoid assistant in a simulated environment.","sidebar":"tutorialSidebar"},"glossary":{"id":"glossary","title":"Glossary","description":"This glossary provides definitions for key terms and acronyms used throughout the \\"Physical AI & Humanoid Robotics\\" textbook.","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Introduction","description":"Welcome to the frontier of artificial intelligence and robotics. This open-source textbook is your guide to building intelligent humanoid robots that can perceive, understand, and interact with the complex physical world. As we stand on the cusp of a new era in robotics, the ability to bridge the gap between AI algorithms and physical embodiment has never been more critical.","sidebar":"tutorialSidebar"},"module-1-ros2/ai-agents-to-controllers":{"id":"module-1-ros2/ai-agents-to-controllers","title":"5. From AI Agents to Robot Controllers","description":"We\'ve defined what our robot looks like with URDF, and we know how to write Python nodes with rclpy. Now, how do we command the robot to move?","sidebar":"tutorialSidebar"},"module-1-ros2/exercises":{"id":"module-1-ros2/exercises","title":"6. Module 1 Exercises","description":"It\'s time to put your new knowledge into practice. These exercises are designed to solidify your understanding of the core ROS 2 concepts covered in this module.","sidebar":"tutorialSidebar"},"module-1-ros2/humanoid-urdf-models":{"id":"module-1-ros2/humanoid-urdf-models","title":"4. Describing Your Robot with URDF","description":"So far, we\'ve created abstract nodes that pass data around. But how do we tell ROS what our robot actually looks like? How do we define its shape, its joints, and how all its parts connect?","sidebar":"tutorialSidebar"},"module-1-ros2/intro-to-ros2":{"id":"module-1-ros2/intro-to-ros2","title":"1. Introduction to ROS 2","description":"Welcome to the first module of our journey into Physical AI. Before our humanoid robot can learn to see, think, or act, it needs a nervous system. In the world of modern robotics, that nervous system is the Robot Operating System (ROS).","sidebar":"tutorialSidebar"},"module-1-ros2/nodes-topics-services":{"id":"module-1-ros2/nodes-topics-services","title":"2. Nodes, Topics, Services, & Actions","description":"In the last chapter, we introduced the ROS graph as a network of independent programs. Now, let\'s break down the four fundamental concepts that make this graph work: Nodes, Topics, Services, and Actions.","sidebar":"tutorialSidebar"},"module-1-ros2/rclpy-for-control":{"id":"module-1-ros2/rclpy-for-control","title":"3. Writing Your First ROS 2 Node with rclpy","description":"Theory is great, but robotics is all about building things that work. In this chapter, we\'ll write our first ROS 2 nodes using rclpy, the official Python client library for ROS 2. We\'ll create a classic \\"talker\\" (publisher) and \\"listener\\" (subscriber) to see topics in action.","sidebar":"tutorialSidebar"},"module-2-digital-twin/complete-humanoid-simulation":{"id":"module-2-digital-twin/complete-humanoid-simulation","title":"5. Building a Complete Humanoid Simulation","description":"We\'ve learned about physics in Gazebo, rendering in Unity, and simulating sensors. Now it\'s time to assemble all these components into a complete, end-to-end digital twin of our humanoid robot.","sidebar":"tutorialSidebar"},"module-2-digital-twin/exercises":{"id":"module-2-digital-twin/exercises","title":"6. Module 2 Exercises","description":"These exercises will help you master the fundamentals of creating and interacting with robotic simulations in Gazebo. You will need a working ROS 2 and Gazebo installation to complete them.","sidebar":"tutorialSidebar"},"module-2-digital-twin/gazebo-physics":{"id":"module-2-digital-twin/gazebo-physics","title":"2. Simulating Physics with Gazebo","description":"In the last chapter, we defined a Digital Twin as a physics-based simulation of our robot. Now, let\'s get our hands dirty with the tool that provides that physics: Gazebo.","sidebar":"tutorialSidebar"},"module-2-digital-twin/sensor-simulation":{"id":"module-2-digital-twin/sensor-simulation","title":"4. Simulating Sensors","description":"A robot is blind and deaf without its sensors. A critical function of a digital twin is to generate realistic sensor data that our perception and navigation algorithms can consume. In this chapter, we\'ll explore how to add and configure common robotic sensors in our simulators.","sidebar":"tutorialSidebar"},"module-2-digital-twin/unity-for-hri":{"id":"module-2-digital-twin/unity-for-hri","title":"3. High-Fidelity Rendering with Unity","description":"Gazebo is a fantastic tool for simulating physics, but its graphics are relatively simple. For many cutting-edge robotics applications, visual realism is not just a \\"nice-to-have\\"\u2014it\'s a necessity. This is where game engines like Unity come in.","sidebar":"tutorialSidebar"},"module-2-digital-twin/what-is-a-digital-twin":{"id":"module-2-digital-twin/what-is-a-digital-twin","title":"1. What is a Digital Twin?","description":"In the first module, we learned how to build the \\"nervous system\\" of a robot using ROS 2 and describe its physical form using URDF. We even made a virtual model wave at us in RViz.","sidebar":"tutorialSidebar"},"module-3-isaac/ai-controlled-humanoid-brain":{"id":"module-3-isaac/ai-controlled-humanoid-brain","title":"6. The AI-Controlled Humanoid Brain","description":"We\'ve assembled a powerful set of tools in this module: a photorealistic simulator (Isaac Sim), a suite of GPU-accelerated perception nodes (Isaac ROS), and a robust navigation stack (Nav2). Now it\'s time to assemble them into a cohesive \\"brain\\" for our robot.","sidebar":"tutorialSidebar"},"module-3-isaac/exercises":{"id":"module-3-isaac/exercises","title":"7. Module 3 Exercises","description":"The concepts in this module are best understood by working directly with the software. These exercises will guide you through using Isaac Sim and the Isaac ROS GEMs. You will need a computer with a compatible NVIDIA GPU and a full installation of Isaac Sim and Isaac ROS.","sidebar":"tutorialSidebar"},"module-3-isaac/intro-to-isaac":{"id":"module-3-isaac/intro-to-isaac","title":"1. Introduction to the AI-Robot Brain (NVIDIA Isaac)","description":"Welcome to Module 3. So far, we have built a robot\'s nervous system with ROS 2 and given it a virtual body in a simulated world. Now, it\'s time to give it a brain. In modern robotics, the \\"brain\\" is a complex system of perception, planning, and control algorithms, many of which are powered by AI and accelerated by specialized hardware.","sidebar":"tutorialSidebar"},"module-3-isaac/isaac-ros-perception":{"id":"module-3-isaac/isaac-ros-perception","title":"3. GPU-Accelerated Perception with Isaac ROS","description":"Running a photorealistic simulation in Isaac Sim is only half the story. The data generated by the simulator needs to be processed by our robot\'s brain. For perception tasks like computer vision and 3D sensing, the amount of data can be enormous, easily overwhelming a CPU.","sidebar":"tutorialSidebar"},"module-3-isaac/isaac-sim-photorealism":{"id":"module-3-isaac/isaac-sim-photorealism","title":"2. Isaac Sim and Photorealistic Rendering","description":"NVIDIA\'s Isaac Sim is not just another robotics simulator; it\'s a platform built for the age of AI. Its core strength lies in its ability to generate physically-based, photorealistic sensor data. This is a game-changer for training and testing the AI models that power modern robots.","sidebar":"tutorialSidebar"},"module-3-isaac/nav2-path-planning":{"id":"module-3-isaac/nav2-path-planning","title":"5. Autonomous Navigation with Nav2","description":"We have given our robot the ability to \\"see\\" and \\"locate\\" itself in a map. The next logical step is to give it the ability to move from point A to point B on its own. This is the domain of navigation, and in ROS 2, the industry-standard solution is the Navigation2 stack, or Nav2.","sidebar":"tutorialSidebar"},"module-3-isaac/vslam-and-depth":{"id":"module-3-isaac/vslam-and-depth","title":"4. VSLAM and Depth Perception","description":"Now that we have a GPU-accelerated perception pipeline, let\'s use it to give our robot two critical abilities: the ability to know where it is, and the ability to \\"see\\" in 3D.","sidebar":"tutorialSidebar"},"module-4-vla/cognitive-planning-pipelines":{"id":"module-4-vla/cognitive-planning-pipelines","title":"4. Cognitive Planning Pipelines","description":"We now have the ability to translate voice commands into text using Whisper and to generate a sequence of abstract actions from an LLM (or our mock LLM). The next challenge is to build a Cognitive Planning Pipeline that takes this abstract action plan and translates it into concrete, executable ROS 2 commands for our robot.","sidebar":"tutorialSidebar"},"module-4-vla/exercises":{"id":"module-4-vla/exercises","title":"6. Module 4 Exercises","description":"These exercises will challenge you to build and integrate the Vision-Language-Action (VLA) pipeline components we\'ve discussed.","sidebar":"tutorialSidebar"},"module-4-vla/integrating-vla":{"id":"module-4-vla/integrating-vla","title":"5. Integrating Visual Perception into VLA","description":"Our VLA pipeline can now understand spoken language and translate it into a sequence of abstract actions. However, the actions currently rely on predefined locations (\\"kitchen\\", \\"bedroom\\") and abstract object names (\\"mug\\"). For true autonomy, our robot needs to visually perceive these objects and their locations in the real world.","sidebar":"tutorialSidebar"},"module-4-vla/llm-to-ros-actions":{"id":"module-4-vla/llm-to-ros-actions","title":"3. LLM to ROS Actions: The Cognitive Core","description":"We can now turn spoken commands into text. The next crucial step in our VLA pipeline is to convert this natural language text into a sequence of executable robot actions. This is the cognitive core of our robot\'s intelligence, typically handled by a Large Language Model (LLM).","sidebar":"tutorialSidebar"},"module-4-vla/vla-future-of-robotics":{"id":"module-4-vla/vla-future-of-robotics","title":"1. VLA: The Future of Robotics","description":"So far, our robot has a nervous system (ROS 2), a physical body (URDF), a simulated world (Gazebo/Isaac Sim), and a brain that perceives its environment and plans paths (Isaac ROS/Nav2). What\'s missing? The ability to understand and execute high-level, human-like commands.","sidebar":"tutorialSidebar"},"module-4-vla/whisper-voice-commands":{"id":"module-4-vla/whisper-voice-commands","title":"2. Voice Commands with OpenAI Whisper","description":"The first step in any Vision-Language-Action (VLA) pipeline is often to convert human input into a format the robot can understand. For spoken commands, this means speech-to-text. In this chapter, we\'ll integrate OpenAI\'s powerful Whisper model into our ROS 2 system to enable our robot to understand voice commands.","sidebar":"tutorialSidebar"},"tutorial-basics/congratulations":{"id":"tutorial-basics/congratulations","title":"Congratulations!","description":"You have just learned the basics of Docusaurus and made some changes to the initial template."},"tutorial-basics/create-a-blog-post":{"id":"tutorial-basics/create-a-blog-post","title":"Create a Blog Post","description":"Docusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed..."},"tutorial-basics/create-a-document":{"id":"tutorial-basics/create-a-document","title":"Create a Document","description":"Documents are groups of pages connected through:"},"tutorial-basics/create-a-page":{"id":"tutorial-basics/create-a-page","title":"Create a Page","description":"Add Markdown or React files to src/pages to create a standalone page:"},"tutorial-basics/deploy-your-site":{"id":"tutorial-basics/deploy-your-site","title":"Deploy your site","description":"Docusaurus is a static-site-generator (also called Jamstack)."},"tutorial-basics/markdown-features":{"id":"tutorial-basics/markdown-features","title":"Markdown Features","description":"Docusaurus supports Markdown and a few additional features."},"tutorial-extras/manage-docs-versions":{"id":"tutorial-extras/manage-docs-versions","title":"Manage Docs Versions","description":"Docusaurus can manage multiple versions of your docs."},"tutorial-extras/translate-your-site":{"id":"tutorial-extras/translate-your-site","title":"Translate your site","description":"Let\'s translate docs/intro.md to French."}}}}')}}]);