"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[3813],{2375:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>p,frontMatter:()=>t,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"module-4-vla/whisper-voice-commands","title":"2. Voice Commands with OpenAI Whisper","description":"The first step in any Vision-Language-Action (VLA) pipeline is often to convert human input into a format the robot can understand. For spoken commands, this means speech-to-text. In this chapter, we\'ll integrate OpenAI\'s powerful Whisper model into our ROS 2 system to enable our robot to understand voice commands.","source":"@site/docs/module-4-vla/2-whisper-voice-commands.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/whisper-voice-commands","permalink":"/physical-ai-book/docs/module-4-vla/whisper-voice-commands","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-ai-book/physical-ai-book/tree/main/docs/module-4-vla/2-whisper-voice-commands.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"2. Voice Commands with OpenAI Whisper","sidebar_label":"Whisper Voice Commands","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"VLA Future of Robotics","permalink":"/physical-ai-book/docs/module-4-vla/vla-future-of-robotics"},"next":{"title":"LLM to ROS Actions","permalink":"/physical-ai-book/docs/module-4-vla/llm-to-ros-actions"}}');var o=i(4848),r=i(8453);const t={title:"2. Voice Commands with OpenAI Whisper",sidebar_label:"Whisper Voice Commands",sidebar_position:2},a="2. Voice Commands with OpenAI Whisper",d={},l=[{value:"Why Whisper?",id:"why-whisper",level:2},{value:"The Speech-to-Text ROS 2 Node",id:"the-speech-to-text-ros-2-node",level:2},{value:"1. Audio Capture with <code>sounddevice</code>",id:"1-audio-capture-with-sounddevice",level:3},{value:"2. Transcribing with Whisper",id:"2-transcribing-with-whisper",level:3},{value:"3. Integrating into ROS 2",id:"3-integrating-into-ros-2",level:3},{value:"Setup and Running",id:"setup-and-running",level:3}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"2-voice-commands-with-openai-whisper",children:"2. Voice Commands with OpenAI Whisper"})}),"\n",(0,o.jsxs)(n.p,{children:["The first step in any Vision-Language-Action (VLA) pipeline is often to convert human input into a format the robot can understand. For spoken commands, this means ",(0,o.jsx)(n.strong,{children:"speech-to-text"}),". In this chapter, we'll integrate OpenAI's powerful ",(0,o.jsx)(n.strong,{children:"Whisper"})," model into our ROS 2 system to enable our robot to understand voice commands."]}),"\n",(0,o.jsx)(n.h2,{id:"why-whisper",children:"Why Whisper?"}),"\n",(0,o.jsx)(n.p,{children:"Whisper is a general-purpose speech recognition model developed by OpenAI. It is trained on a large dataset of diverse audio and is capable of transcribing speech into text with high accuracy, even in noisy environments or with different accents. Crucially for robotics, it can run locally on reasonable hardware, providing low-latency transcription."}),"\n",(0,o.jsx)(n.h2,{id:"the-speech-to-text-ros-2-node",children:"The Speech-to-Text ROS 2 Node"}),"\n",(0,o.jsx)(n.p,{children:"We will create a simple ROS 2 Python node that:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Captures audio from the microphone."}),"\n",(0,o.jsx)(n.li,{children:"Processes the audio with the Whisper model to generate text."}),"\n",(0,o.jsx)(n.li,{children:"Publishes the transcribed text to a ROS 2 topic."}),"\n"]}),"\n",(0,o.jsxs)(n.h3,{id:"1-audio-capture-with-sounddevice",children:["1. Audio Capture with ",(0,o.jsx)(n.code,{children:"sounddevice"})]}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"sounddevice"})," Python library provides a simple way to capture audio from your system's microphone."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Minimal example of audio capture\nimport sounddevice as sd\nimport numpy as np\n\nduration = 3  # seconds\nfs = 16000    # Sample rate (Hz) - Whisper prefers 16kHz\n\nprint(\"Recording for 3 seconds...\")\nrecording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='float32')\nsd.wait() # Wait until recording is finished\nprint(\"Recording complete.\")\n\n# You now have 'recording' as a numpy array of audio samples\n"})}),"\n",(0,o.jsx)(n.h3,{id:"2-transcribing-with-whisper",children:"2. Transcribing with Whisper"}),"\n",(0,o.jsx)(n.p,{children:"Once you have the audio as a NumPy array, you can pass it to the Whisper model."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import whisper\n\n# Load the desired Whisper model (e.g., 'base', 'small', 'medium', 'large')\n# 'base' is good for local inference on many CPUs\nmodel = whisper.load_model(\"base\") \n\n# Make sure the audio is in the correct format (16kHz, mono)\n# 'recording' should be a numpy array of float32 samples\naudio_segment = recording.flatten() \n\nprint(\"Transcribing...\")\nresult = model.transcribe(audio_segment)\ntranscribed_text = result[\"text\"]\nprint(f\"Transcribed: {transcribed_text}\")\n"})}),"\n",(0,o.jsx)(n.h3,{id:"3-integrating-into-ros-2",children:"3. Integrating into ROS 2"}),"\n",(0,o.jsx)(n.p,{children:"Now, let's combine these into a ROS 2 node. This node will run continuously, listening for a trigger (e.g., a specific key press or a command on another topic), record a short audio clip, transcribe it, and then publish the result."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# ~/ros2_ws/src/voice_command_pkg/voice_command_pkg/whisper_node.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport sounddevice as sd\nimport numpy as np\nimport whisper\nimport threading\nimport time\n\nclass WhisperNode(Node):\n\n    def __init__(self):\n        super().__init__('whisper_node')\n        self.publisher_ = self.create_publisher(String, '/voice_command', 10)\n        self.declare_parameter('audio_duration', 3.0)\n        self.declare_parameter('sample_rate', 16000)\n        self.declare_parameter('whisper_model', 'base')\n\n        self.audio_duration = self.get_parameter('audio_duration').value\n        self.sample_rate = self.get_parameter('sample_rate').value\n        self.whisper_model_name = self.get_parameter('whisper_model').value\n\n        self.get_logger().info(f\"Loading Whisper model: {self.whisper_model_name}...\")\n        self.whisper_model = whisper.load_model(self.whisper_model_name)\n        self.get_logger().info(\"Whisper model loaded.\")\n\n        # Example: Trigger recording every 5 seconds for demonstration\n        self.timer = self.create_timer(5.0, self.timer_callback)\n        self.get_logger().info('Whisper Node has been started. Recording triggered every 5s.')\n\n    def timer_callback(self):\n        self.get_logger().info(\"Recording audio...\")\n        try:\n            recording = sd.rec(int(self.audio_duration * self.sample_rate),\n                               samplerate=self.sample_rate,\n                               channels=1,\n                               dtype='float32')\n            sd.wait() # Wait until recording is finished\n            self.get_logger().info(\"Recording complete. Transcribing...\")\n\n            audio_segment = recording.flatten()\n            result = self.whisper_model.transcribe(audio_segment)\n            transcribed_text = result[\"text\"]\n\n            msg = String()\n            msg.data = transcribed_text\n            self.publisher_.publish(msg)\n            self.get_logger().info(f'Published: \"{msg.data}\"')\n\n        except Exception as e:\n            self.get_logger().error(f\"Error during transcription: {e}\")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    whisper_node = WhisperNode()\n    rclpy.spin(whisper_node)\n    whisper_node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"setup-and-running",children:"Setup and Running"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Install dependencies"}),":","\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"pip install sounddevice numpy openai-whisper\n# You might need portaudio development files:\n# sudo apt-get install libportaudio2 python3-pyaudio\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Create ROS 2 package"}),":","\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\nros2 pkg create --build-type ament_python voice_command_pkg --dependencies rclpy std_msgs\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Place code"}),": Put the ",(0,o.jsx)(n.code,{children:"whisper_node.py"})," file into ",(0,o.jsx)(n.code,{children:"~/ros2_ws/src/voice_command_pkg/voice_command_pkg/"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsxs)(n.strong,{children:["Update ",(0,o.jsx)(n.code,{children:"setup.py"})]}),": Add an entry point for your node in ",(0,o.jsx)(n.code,{children:"~/ros2_ws/src/voice_command_pkg/setup.py"})," under ",(0,o.jsx)(n.code,{children:"entry_points"}),":","\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"entry_points={\n    'console_scripts': [\n        'whisper_node = voice_command_pkg.whisper_node:main',\n    ],\n},\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Build"}),":","\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws\ncolcon build\nsource install/setup.bash\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Run"}),":","\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"ros2 run voice_command_pkg whisper_node\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Verify"}),": In another terminal, echo the topic:","\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /voice_command\n"})}),"\n","Speak into your microphone when the node indicates it's recording. You should see your spoken words appear on the ",(0,o.jsx)(n.code,{children:"/voice_command"})," topic."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["This ",(0,o.jsx)(n.code,{children:"whisper_node"})," now forms the crucial first link in our VLA chain, translating the messy world of human speech into clean, structured text that our robot's AI can process. In the next chapter, we'll see how to take this text and turn it into executable robot actions."]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>a});var s=i(6540);const o={},r=s.createContext(o);function t(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:t(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);