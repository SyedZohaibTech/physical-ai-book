"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[152],{5961(e,r,n){n.r(r),n.d(r,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>s,metadata:()=>a,toc:()=>m});const a=JSON.parse('{"id":"module3/visual-slam","title":"Visual SLAM for Humanoid Navigation","description":"Visual Simultaneous Localization and Mapping (SLAM) is a critical technology for humanoid robots operating in unknown or dynamic environments. It enables robots to build maps of their surroundings while simultaneously determining their position within those maps, all using visual information from cameras.","source":"@site/docs/module3/visual-slam.md","sourceDirName":"module3","slug":"/module3/visual-slam","permalink":"/docs/module3/visual-slam","draft":false,"unlisted":false,"editUrl":"https://github.com/SyedZohaibTech/physical-ai-book/edit/main/docs/module3/visual-slam.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Visual SLAM for Humanoid Navigation"},"sidebar":"tutorialSidebar","previous":{"title":"Isaac ROS Integration","permalink":"/docs/module3/isaac-ros"},"next":{"title":"Navigation 2 (Nav2) for Humanoid Robots","permalink":"/docs/module3/nav2-planning"}}');var t=n(4848),i=n(8453);const s={sidebar_position:4,title:"Visual SLAM for Humanoid Navigation"},o="Visual SLAM for Humanoid Navigation",l={},m=[{value:"Introduction to Visual SLAM",id:"introduction-to-visual-slam",level:2},{value:"Core Components of Visual SLAM",id:"core-components-of-visual-slam",level:3},{value:"Types of Visual SLAM",id:"types-of-visual-slam",level:3},{value:"Visual SLAM Algorithms",id:"visual-slam-algorithms",level:2},{value:"1. Feature-Based SLAM",id:"1-feature-based-slam",level:3},{value:"2. Direct SLAM",id:"2-direct-slam",level:3},{value:"Popular Visual SLAM Systems",id:"popular-visual-slam-systems",level:2},{value:"1. ORB-SLAM",id:"1-orb-slam",level:3},{value:"2. LSD-SLAM (Direct Method)",id:"2-lsd-slam-direct-method",level:3},{value:"Visual SLAM for Humanoid Robots",id:"visual-slam-for-humanoid-robots",level:2},{value:"1. Multi-Camera SLAM",id:"1-multi-camera-slam",level:3},{value:"2. RGB-D SLAM for Humanoid Robots",id:"2-rgb-d-slam-for-humanoid-robots",level:3},{value:"Challenges in Humanoid Visual SLAM",id:"challenges-in-humanoid-visual-slam",level:2},{value:"1. Motion Blur and Fast Movement",id:"1-motion-blur-and-fast-movement",level:3},{value:"2. Dynamic Objects",id:"2-dynamic-objects",level:3},{value:"Integration with Navigation Systems",id:"integration-with-navigation-systems",level:2},{value:"1. Path Planning from SLAM Maps",id:"1-path-planning-from-slam-maps",level:3}];function p(e){const r={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(r.header,{children:(0,t.jsx)(r.h1,{id:"visual-slam-for-humanoid-navigation",children:"Visual SLAM for Humanoid Navigation"})}),"\n",(0,t.jsx)(r.p,{children:"Visual Simultaneous Localization and Mapping (SLAM) is a critical technology for humanoid robots operating in unknown or dynamic environments. It enables robots to build maps of their surroundings while simultaneously determining their position within those maps, all using visual information from cameras."}),"\n",(0,t.jsx)(r.h2,{id:"introduction-to-visual-slam",children:"Introduction to Visual SLAM"}),"\n",(0,t.jsx)(r.p,{children:"Visual SLAM is a technique that allows robots to understand and navigate their environment using visual sensors. For humanoid robots, this is particularly important as they need to operate in human-centric environments that are often unstructured and dynamic."}),"\n",(0,t.jsx)(r.h3,{id:"core-components-of-visual-slam",children:"Core Components of Visual SLAM"}),"\n",(0,t.jsxs)(r.ol,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Tracking"}),": Estimating the camera's motion between frames"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Mapping"}),": Building a representation of the environment"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Loop Closure"}),": Recognizing previously visited locations"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Optimization"}),": Refining the map and trajectory estimates"]}),"\n"]}),"\n",(0,t.jsx)(r.h3,{id:"types-of-visual-slam",children:"Types of Visual SLAM"}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Monocular SLAM"}),": Uses a single camera, requires motion for depth estimation"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Stereo SLAM"}),": Uses stereo cameras for direct depth estimation"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"RGB-D SLAM"}),": Uses RGB-D cameras for depth and color information"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Multi-Camera SLAM"}),": Uses multiple cameras for wider field of view"]}),"\n"]}),"\n",(0,t.jsx)(r.h2,{id:"visual-slam-algorithms",children:"Visual SLAM Algorithms"}),"\n",(0,t.jsx)(r.h3,{id:"1-feature-based-slam",children:"1. Feature-Based SLAM"}),"\n",(0,t.jsx)(r.p,{children:"Feature-based methods detect and track distinctive features in the environment:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-python",children:"import cv2\r\nimport numpy as np\r\nfrom scipy.spatial.transform import Rotation as R\r\n\r\nclass FeatureBasedSLAM:\r\n    def __init__(self):\r\n        # Feature detector and descriptor\r\n        self.detector = cv2.SIFT_create()\r\n        self.matcher = cv2.BFMatcher()\r\n        \r\n        # Camera parameters\r\n        self.fx = 525.0  # Focal length x\r\n        self.fy = 525.0  # Focal length y\r\n        self.cx = 319.5  # Principal point x\r\n        self.cy = 239.5  # Principal point y\r\n        \r\n        # Pose estimation\r\n        self.current_pose = np.eye(4)\r\n        self.keyframes = []\r\n        self.map_points = []\r\n        \r\n    def process_frame(self, image, timestamp):\r\n        # Detect features\r\n        keypoints, descriptors = self.detector.detectAndCompute(image, None)\r\n        \r\n        if len(self.keyframes) == 0:\r\n            # First frame - add as keyframe\r\n            self.keyframes.append({\r\n                'image': image,\r\n                'keypoints': keypoints,\r\n                'descriptors': descriptors,\r\n                'pose': self.current_pose.copy(),\r\n                'timestamp': timestamp\r\n            })\r\n            return self.current_pose\r\n        \r\n        # Match features with previous keyframe\r\n        prev_keyframe = self.keyframes[-1]\r\n        matches = self.matcher.knnMatch(\r\n            prev_keyframe['descriptors'], \r\n            descriptors, \r\n            k=2\r\n        )\r\n        \r\n        # Apply Lowe's ratio test\r\n        good_matches = []\r\n        for match_pair in matches:\r\n            if len(match_pair) == 2:\r\n                m, n = match_pair\r\n                if m.distance < 0.7 * n.distance:\r\n                    good_matches.append(m)\r\n        \r\n        if len(good_matches) >= 10:\r\n            # Extract matched points\r\n            src_points = np.float32([prev_keyframe['keypoints'][m.queryIdx].pt \r\n                                   for m in good_matches]).reshape(-1, 1, 2)\r\n            dst_points = np.float32([keypoints[m.trainIdx].pt \r\n                                   for m in good_matches]).reshape(-1, 1, 2)\r\n            \r\n            # Estimate essential matrix\r\n            E, mask = cv2.findEssentialMat(\r\n                src_points, dst_points, \r\n                focal=self.fx, pp=(self.cx, self.cy),\r\n                method=cv2.RANSAC, threshold=1.0\r\n            )\r\n            \r\n            if E is not None:\r\n                # Decompose essential matrix to get rotation and translation\r\n                _, R, t, _ = cv2.recoverPose(\r\n                    E, src_points, dst_points,\r\n                    focal=self.fx, pp=(self.cx, self.cy)\r\n                )\r\n                \r\n                # Create transformation matrix\r\n                T = np.eye(4)\r\n                T[:3, :3] = R\r\n                T[:3, 3] = t.flatten()\r\n                \r\n                # Update current pose\r\n                self.current_pose = self.current_pose @ T\r\n                \r\n                # Add as keyframe if movement is significant\r\n                if np.linalg.norm(t) > 0.1 or np.trace(R) < 2.9:\r\n                    self.keyframes.append({\r\n                        'image': image,\r\n                        'keypoints': keypoints,\r\n                        'descriptors': descriptors,\r\n                        'pose': self.current_pose.copy(),\r\n                        'timestamp': timestamp\r\n                    })\r\n        \r\n        return self.current_pose\n"})}),"\n",(0,t.jsx)(r.h3,{id:"2-direct-slam",children:"2. Direct SLAM"}),"\n",(0,t.jsx)(r.p,{children:"Direct methods work with pixel intensities rather than features:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-python",children:'import numpy as np\r\nimport cv2\r\n\r\nclass DirectSLAM:\r\n    def __init__(self, width, height):\r\n        self.width = width\r\n        self.height = height\r\n        self.current_frame = None\r\n        self.reference_frame = None\r\n        self.depth_map = None\r\n        self.pose = np.eye(4)\r\n        \r\n    def initialize_depth(self, depth_init):\r\n        """Initialize depth map with initial estimate"""\r\n        self.depth_map = depth_init\r\n    \r\n    def estimate_motion(self, current_img, ref_img, K):\r\n        """Estimate motion between current and reference frames"""\r\n        # Convert to grayscale if needed\r\n        if len(current_img.shape) == 3:\r\n            current_gray = cv2.cvtColor(current_img, cv2.COLOR_BGR2GRAY).astype(np.float32)\r\n        else:\r\n            current_gray = current_img.astype(np.float32)\r\n        \r\n        if len(ref_img.shape) == 3:\r\n            ref_gray = cv2.cvtColor(ref_img, cv2.COLOR_BGR2GRAY).astype(np.float32)\r\n        else:\r\n            ref_gray = ref_img.astype(np.float32)\r\n        \r\n        # Calculate image gradients\r\n        ref_grad_x = cv2.Sobel(ref_gray, cv2.CV_32F, 1, 0, ksize=3)\r\n        ref_grad_y = cv2.Sobel(ref_gray, cv2.CV_32F, 0, 1, ksize=3)\r\n        \r\n        # Initialize pose change estimate\r\n        xi = np.zeros(6)  # [rho, phi] where rho=[tx, ty, tz], phi=[rx, ry, rz]\r\n        \r\n        # Iterative optimization (simplified Gauss-Newton)\r\n        for iteration in range(10):\r\n            # Calculate Jacobian and residuals\r\n            J, residuals = self.compute_jacobian_residuals(\r\n                ref_gray, current_gray, ref_grad_x, ref_grad_y, K, xi\r\n            )\r\n            \r\n            if np.linalg.norm(residuals) < 1e-6:\r\n                break\r\n                \r\n            # Solve normal equations\r\n            JTJ = J.T @ J\r\n            JTr = J.T @ residuals\r\n            \r\n            try:\r\n                delta_xi = np.linalg.solve(JTJ, JTr)\r\n                xi += delta_xi\r\n                \r\n                # Check for convergence\r\n                if np.linalg.norm(delta_xi) < 1e-6:\r\n                    break\r\n            except np.linalg.LinAlgError:\r\n                break\r\n        \r\n        # Convert twist vector to transformation matrix\r\n        T = self.twist_to_transform(xi)\r\n        return T\r\n    \r\n    def compute_jacobian_residuals(self, ref_img, curr_img, grad_x, grad_y, K, xi):\r\n        """Compute Jacobian and residuals for optimization"""\r\n        # This is a simplified implementation\r\n        # In practice, this would involve complex geometric calculations\r\n        # and would be optimized for GPU computation\r\n        \r\n        # Generate 3D points from depth and camera parameters\r\n        height, width = ref_img.shape\r\n        \r\n        # Create coordinate grids\r\n        x_coords, y_coords = np.meshgrid(np.arange(width), np.arange(height))\r\n        \r\n        # Convert to normalized coordinates\r\n        x_norm = (x_coords - K[0, 2]) / K[0, 0]\r\n        y_norm = (y_coords - K[1, 2]) / K[1, 1]\r\n        \r\n        # Calculate Jacobian components\r\n        # (Simplified - real implementation would be more complex)\r\n        J = np.random.rand(height * width, 6).astype(np.float32)\r\n        residuals = np.random.rand(height * width).astype(np.float32)\r\n        \r\n        return J, residuals\r\n    \r\n    def twist_to_transform(self, xi):\r\n        """Convert twist vector to transformation matrix"""\r\n        rho = xi[:3]  # translation\r\n        phi = xi[3:]  # rotation\r\n        \r\n        # Create skew-symmetric matrix for rotation vector\r\n        phi_skew = np.array([\r\n            [0, -phi[2], phi[1]],\r\n            [phi[2], 0, -phi[0]],\r\n            [-phi[1], phi[0], 0]\r\n        ])\r\n        \r\n        # Calculate rotation matrix using Rodrigues\' formula\r\n        angle = np.linalg.norm(phi)\r\n        if angle < 1e-9:\r\n            R = np.eye(3)\r\n        else:\r\n            axis = phi / angle\r\n            K = phi_skew\r\n            R = np.eye(3) + np.sin(angle) * K + (1 - np.cos(angle)) * (K @ K)\r\n        \r\n        # Calculate transformation matrix\r\n        T = np.eye(4)\r\n        T[:3, :3] = R\r\n        T[:3, 3] = rho\r\n        \r\n        return T\n'})}),"\n",(0,t.jsx)(r.h2,{id:"popular-visual-slam-systems",children:"Popular Visual SLAM Systems"}),"\n",(0,t.jsx)(r.h3,{id:"1-orb-slam",children:"1. ORB-SLAM"}),"\n",(0,t.jsx)(r.p,{children:"ORB-SLAM is a popular feature-based SLAM system:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-python",children:"import cv2\r\nimport numpy as np\r\n\r\nclass ORB_SLAM_Interface:\r\n    def __init__(self):\r\n        # ORB feature detector\r\n        self.orb = cv2.ORB_create(nfeatures=2000)\r\n        self.bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\r\n        \r\n        # SLAM state\r\n        self.keyframes = []\r\n        self.map_points = {}\r\n        self.current_pose = np.eye(4)\r\n        \r\n    def track_frame(self, image, timestamp):\r\n        # Detect ORB features\r\n        keypoints, descriptors = self.orb.detectAndCompute(image, None)\r\n        \r\n        if len(self.keyframes) == 0:\r\n            # Initialize first keyframe\r\n            self.keyframes.append({\r\n                'image': image,\r\n                'keypoints': keypoints,\r\n                'descriptors': descriptors,\r\n                'pose': self.current_pose.copy(),\r\n                'timestamp': timestamp\r\n            })\r\n            return self.current_pose\r\n        \r\n        # Match with previous keyframe\r\n        prev_frame = self.keyframes[-1]\r\n        matches = self.bf.match(prev_frame['descriptors'], descriptors)\r\n        \r\n        # Sort matches by distance\r\n        matches = sorted(matches, key=lambda x: x.distance)\r\n        \r\n        if len(matches) >= 10:\r\n            # Extract matched points\r\n            src_points = np.float32([prev_frame['keypoints'][m.queryIdx].pt \r\n                                   for m in matches]).reshape(-1, 1, 2)\r\n            dst_points = np.float32([keypoints[m.trainIdx].pt \r\n                                   for m in matches]).reshape(-1, 1, 2)\r\n            \r\n            # Estimate transformation\r\n            transformation, mask = cv2.findHomography(\r\n                src_points, dst_points, cv2.RANSAC, 5.0\r\n            )\r\n            \r\n            if transformation is not None:\r\n                # Update pose (simplified - real implementation would be more complex)\r\n                # This is a placeholder for actual pose estimation\r\n                self.current_pose = self.update_pose(transformation)\r\n                \r\n                # Add as keyframe if significant movement\r\n                if self.is_keyframe_significant():\r\n                    self.keyframes.append({\r\n                        'image': image,\r\n                        'keypoints': keypoints,\r\n                        'descriptors': descriptors,\r\n                        'pose': self.current_pose.copy(),\r\n                        'timestamp': timestamp\r\n                    })\r\n        \r\n        return self.current_pose\r\n    \r\n    def update_pose(self, transformation):\r\n        \"\"\"Update current pose based on transformation\"\"\"\r\n        # Simplified pose update - real implementation would use\r\n        # more sophisticated optimization\r\n        new_pose = self.current_pose.copy()\r\n        # Apply transformation to pose\r\n        return new_pose\r\n    \r\n    def is_keyframe_significant(self):\r\n        \"\"\"Check if current frame is significantly different from last keyframe\"\"\"\r\n        # Check if enough time has passed or enough movement occurred\r\n        return len(self.keyframes) % 10 == 0  # Simplified: every 10th frame\n"})}),"\n",(0,t.jsx)(r.h3,{id:"2-lsd-slam-direct-method",children:"2. LSD-SLAM (Direct Method)"}),"\n",(0,t.jsx)(r.p,{children:"LSD-SLAM uses direct intensity-based tracking:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-python",children:'import numpy as np\r\nimport cv2\r\n\r\nclass LSD_SLAM:\r\n    def __init__(self):\r\n        self.reference_frame = None\r\n        self.depth_map = None\r\n        self.pose = np.eye(4)\r\n        self.K = np.array([[525.0, 0, 319.5],\r\n                          [0, 525.0, 239.5],\r\n                          [0, 0, 1.0]])  # Camera intrinsic matrix\r\n        \r\n    def initialize_depth(self, image):\r\n        """Initialize depth map using initial frame"""\r\n        # Simplified initialization - real implementation would use\r\n        # more sophisticated methods like stereo or structure from motion\r\n        height, width = image.shape[:2]\r\n        self.depth_map = np.ones((height, width), dtype=np.float32) * 2.0  # Default depth of 2m\r\n    \r\n    def process_frame(self, image):\r\n        """Process a new frame and update SLAM state"""\r\n        if self.reference_frame is None:\r\n            # Initialize with first frame\r\n            self.reference_frame = image.astype(np.float32)\r\n            self.initialize_depth(image)\r\n            return self.pose\r\n        \r\n        # Estimate motion relative to reference frame\r\n        motion = self.estimate_motion(image, self.reference_frame)\r\n        \r\n        # Update global pose\r\n        self.pose = self.pose @ motion\r\n        \r\n        # Update reference frame if needed\r\n        if self.should_update_reference():\r\n            self.reference_frame = image.astype(np.float32)\r\n        \r\n        return self.pose\r\n    \r\n    def estimate_motion(self, current_frame, ref_frame):\r\n        """Estimate motion between frames using direct method"""\r\n        # Calculate image gradients\r\n        ref_grad_x = cv2.Sobel(ref_frame, cv2.CV_32F, 1, 0, ksize=3)\r\n        ref_grad_y = cv2.Sobel(ref_frame, cv2.CV_32F, 0, 1, ksize=3)\r\n        \r\n        # Sample points for motion estimation (use all points for simplicity)\r\n        height, width = ref_frame.shape\r\n        y_coords, x_coords = np.mgrid[0:height, 0:width]\r\n        \r\n        # Calculate Jacobian and residuals for optimization\r\n        # This is a simplified version - real implementation would be more complex\r\n        motion_estimate = np.eye(4)\r\n        \r\n        # Return identity for now (placeholder)\r\n        return motion_estimate\r\n    \r\n    def should_update_reference(self):\r\n        """Determine if reference frame should be updated"""\r\n        # Simplified: update every 20 frames\r\n        return len(self.keyframes) % 20 == 0\n'})}),"\n",(0,t.jsx)(r.h2,{id:"visual-slam-for-humanoid-robots",children:"Visual SLAM for Humanoid Robots"}),"\n",(0,t.jsx)(r.h3,{id:"1-multi-camera-slam",children:"1. Multi-Camera SLAM"}),"\n",(0,t.jsx)(r.p,{children:"Humanoid robots often have multiple cameras for 360-degree perception:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-python",children:'import numpy as np\r\nfrom collections import deque\r\n\r\nclass MultiCameraSLAM:\r\n    def __init__(self):\r\n        # Multiple camera configurations\r\n        self.cameras = {\r\n            \'front\': {\'K\': np.eye(3), \'T\': np.eye(4)},  # Front camera\r\n            \'left\': {\'K\': np.eye(3), \'T\': np.eye(4)},   # Left camera\r\n            \'right\': {\'K\': np.eye(3), \'T\': np.eye(4)},  # Right camera\r\n            \'rear\': {\'K\': np.eye(3), \'T\': np.eye(4)}    # Rear camera\r\n        }\r\n        \r\n        # Individual SLAM systems for each camera\r\n        self.slam_systems = {\r\n            name: FeatureBasedSLAM() for name in self.cameras.keys()\r\n        }\r\n        \r\n        # Global map and pose\r\n        self.global_map = {}\r\n        self.global_pose = np.eye(4)\r\n        \r\n        # Synchronization buffer\r\n        self.frame_buffer = {name: deque(maxlen=5) for name in self.cameras.keys()}\r\n    \r\n    def process_multi_camera_frame(self, frames, timestamp):\r\n        """\r\n        Process frames from multiple cameras\r\n        frames: dict with camera names as keys and images as values\r\n        """\r\n        # Process each camera individually\r\n        camera_poses = {}\r\n        for cam_name, image in frames.items():\r\n            pose = self.slam_systems[cam_name].process_frame(image, timestamp)\r\n            camera_poses[cam_name] = pose\r\n        \r\n        # Fuse poses from all cameras to get global pose\r\n        global_pose = self.fuse_camera_poses(camera_poses)\r\n        \r\n        # Update global map\r\n        self.update_global_map(camera_poses, frames)\r\n        \r\n        return global_pose\r\n    \r\n    def fuse_camera_poses(self, camera_poses):\r\n        """Fuse poses from multiple cameras"""\r\n        # Simple averaging approach (in practice, more sophisticated fusion would be used)\r\n        fused_pose = np.eye(4)\r\n        \r\n        # Extract translations and rotations\r\n        translations = []\r\n        rotations = []\r\n        \r\n        for cam_name, pose in camera_poses.items():\r\n            # Transform camera pose to global frame\r\n            T_cam_to_global = self.cameras[cam_name][\'T\']\r\n            global_pose_cam = T_cam_to_global @ pose\r\n            \r\n            translations.append(global_pose_cam[:3, 3])\r\n            rotations.append(global_pose_cam[:3, :3])\r\n        \r\n        # Average translations\r\n        avg_translation = np.mean(translations, axis=0)\r\n        \r\n        # Average rotations (using quaternion averaging)\r\n        avg_rotation = self.average_rotations(rotations)\r\n        \r\n        # Construct fused pose\r\n        fused_pose[:3, :3] = avg_rotation\r\n        fused_pose[:3, 3] = avg_translation\r\n        \r\n        return fused_pose\r\n    \r\n    def average_rotations(self, rotations):\r\n        """Average multiple rotation matrices"""\r\n        # Convert to quaternions for averaging\r\n        quats = [self.rotation_matrix_to_quaternion(R) for R in rotations]\r\n        \r\n        # Average quaternions (weighted average to handle antipodal property)\r\n        avg_quat = np.mean(quats, axis=0)\r\n        avg_quat = avg_quat / np.linalg.norm(avg_quat)  # Normalize\r\n        \r\n        # Convert back to rotation matrix\r\n        return self.quaternion_to_rotation_matrix(avg_quat)\r\n    \r\n    def rotation_matrix_to_quaternion(self, R):\r\n        """Convert rotation matrix to quaternion"""\r\n        # Using the algorithm from "Quaternion kinematics for the error-state Kalman filter"\r\n        trace = np.trace(R)\r\n        if trace > 0:\r\n            s = np.sqrt(trace + 1.0) * 2  # s = 4 * qw\r\n            qw = 0.25 * s\r\n            qx = (R[2, 1] - R[1, 2]) / s\r\n            qy = (R[0, 2] - R[2, 0]) / s\r\n            qz = (R[1, 0] - R[0, 1]) / s\r\n        else:\r\n            if R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:\r\n                s = np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2]) * 2\r\n                qw = (R[2, 1] - R[1, 2]) / s\r\n                qx = 0.25 * s\r\n                qy = (R[0, 1] + R[1, 0]) / s\r\n                qz = (R[0, 2] + R[2, 0]) / s\r\n            elif R[1, 1] > R[2, 2]:\r\n                s = np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2]) * 2\r\n                qw = (R[0, 2] - R[2, 0]) / s\r\n                qx = (R[0, 1] + R[1, 0]) / s\r\n                qy = 0.25 * s\r\n                qz = (R[1, 2] + R[2, 1]) / s\r\n            else:\r\n                s = np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1]) * 2\r\n                qw = (R[1, 0] - R[0, 1]) / s\r\n                qx = (R[0, 2] + R[2, 0]) / s\r\n                qy = (R[1, 2] + R[2, 1]) / s\r\n                qz = 0.25 * s\r\n        \r\n        return np.array([qw, qx, qy, qz])\r\n    \r\n    def quaternion_to_rotation_matrix(self, q):\r\n        """Convert quaternion to rotation matrix"""\r\n        qw, qx, qy, qz = q\r\n        R = np.array([\r\n            [1 - 2*(qy*qy + qz*qz), 2*(qx*qy - qw*qz), 2*(qx*qz + qw*qy)],\r\n            [2*(qx*qy + qw*qz), 1 - 2*(qx*qx + qz*qz), 2*(qy*qz - qw*qx)],\r\n            [2*(qx*qz - qw*qy), 2*(qy*qz + qw*qx), 1 - 2*(qx*qx + qy*qy)]\r\n        ])\r\n        return R\r\n    \r\n    def update_global_map(self, camera_poses, frames):\r\n        """Update global map using data from all cameras"""\r\n        # This would implement map fusion from multiple camera views\r\n        # In practice, this involves complex data association and map merging\r\n        pass\n'})}),"\n",(0,t.jsx)(r.h3,{id:"2-rgb-d-slam-for-humanoid-robots",children:"2. RGB-D SLAM for Humanoid Robots"}),"\n",(0,t.jsx)(r.p,{children:"RGB-D SLAM leverages depth information for more accurate mapping:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-python",children:'import numpy as np\r\nimport open3d as o3d\r\nfrom scipy.spatial.transform import Rotation as R\r\n\r\nclass RGBDSLAM:\r\n    def __init__(self):\r\n        self.voxel_size = 0.05  # 5cm voxel size\r\n        self.keyframes = []\r\n        self.global_map = o3d.geometry.PointCloud()\r\n        self.pose_graph = []\r\n        self.current_pose = np.eye(4)\r\n        \r\n    def process_rgbd_frame(self, rgb_image, depth_image, intrinsic, timestamp):\r\n        """Process RGB-D frame and update SLAM state"""\r\n        # Create RGBD image\r\n        rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(\r\n            o3d.geometry.Image(rgb_image),\r\n            o3d.geometry.Image(depth_image),\r\n            depth_scale=1000.0,  # Adjust based on your depth sensor\r\n            depth_trunc=3.0,\r\n            convert_rgb_to_intensity=False\r\n        )\r\n        \r\n        # Create point cloud from RGBD\r\n        pcd = o3d.geometry.PointCloud.create_from_rgbd_image(\r\n            rgbd,\r\n            o3d.camera.PinholeCameraIntrinsic(\r\n                width=rgb_image.shape[1],\r\n                height=rgb_image.shape[0],\r\n                fx=intrinsic[0, 0],\r\n                fy=intrinsic[1, 1],\r\n                cx=intrinsic[0, 2],\r\n                cy=intrinsic[1, 2]\r\n            )\r\n        )\r\n        \r\n        # Downsample point cloud\r\n        pcd_down = pcd.voxel_down_sample(voxel_size=self.voxel_size)\r\n        \r\n        if len(self.keyframes) == 0:\r\n            # First frame\r\n            self.keyframes.append({\r\n                \'pointcloud\': pcd_down,\r\n                \'pose\': self.current_pose.copy(),\r\n                \'timestamp\': timestamp\r\n            })\r\n            self.global_map += pcd_down.transform(self.current_pose)\r\n            return self.current_pose\r\n        \r\n        # Find transformation to previous keyframe\r\n        prev_pcd = self.keyframes[-1][\'pointcloud\']\r\n        \r\n        # Estimate transformation using ICP\r\n        transformation = self.estimate_transformation_icp(\r\n            pcd_down, prev_pcd, self.current_pose\r\n        )\r\n        \r\n        # Update current pose\r\n        self.current_pose = self.current_pose @ transformation\r\n        \r\n        # Add as keyframe if movement is significant\r\n        if self.is_significant_movement(transformation):\r\n            self.keyframes.append({\r\n                \'pointcloud\': pcd_down,\r\n                \'pose\': self.current_pose.copy(),\r\n                \'timestamp\': timestamp\r\n            })\r\n            \r\n            # Add to global map\r\n            pcd_transformed = pcd_down.transform(self.current_pose)\r\n            self.global_map += pcd_transformed\r\n        \r\n        return self.current_pose\r\n    \r\n    def estimate_transformation_icp(self, source, target, init_pose):\r\n        """Estimate transformation using ICP"""\r\n        # Perform ICP registration\r\n        reg_p2p = o3d.pipelines.registration.registration_icp(\r\n            source, target, max_correspondence_distance=0.2,\r\n            init=init_pose,\r\n            estimation_method=o3d.pipelines.registration.TransformationEstimationPointToPoint(),\r\n            criteria=o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=100)\r\n        )\r\n        \r\n        return reg_p2p.transformation\r\n    \r\n    def is_significant_movement(self, transformation):\r\n        """Check if movement is significant to add a new keyframe"""\r\n        # Check translation magnitude\r\n        translation = transformation[:3, 3]\r\n        trans_magnitude = np.linalg.norm(translation)\r\n        \r\n        # Check rotation magnitude\r\n        rotation = transformation[:3, :3]\r\n        # Convert to axis-angle to get rotation magnitude\r\n        r = R.from_matrix(rotation)\r\n        rotation_vec = r.as_rotvec()\r\n        rotation_magnitude = np.linalg.norm(rotation_vec)\r\n        \r\n        # Return True if either translation or rotation is significant\r\n        return trans_magnitude > 0.1 or rotation_magnitude > 0.1  # 10cm or ~5.7 degrees\r\n    \r\n    def optimize_pose_graph(self):\r\n        """Optimize the pose graph to reduce drift"""\r\n        # This would implement pose graph optimization\r\n        # In practice, this uses sophisticated optimization techniques\r\n        pass\n'})}),"\n",(0,t.jsx)(r.h2,{id:"challenges-in-humanoid-visual-slam",children:"Challenges in Humanoid Visual SLAM"}),"\n",(0,t.jsx)(r.h3,{id:"1-motion-blur-and-fast-movement",children:"1. Motion Blur and Fast Movement"}),"\n",(0,t.jsx)(r.p,{children:"Humanoid robots can move quickly, causing motion blur:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-python",children:'import cv2\r\nimport numpy as np\r\n\r\ndef reduce_motion_blur(image):\r\n    """Apply deblurring techniques to reduce motion blur"""\r\n    # Apply Wiener deconvolution (simplified)\r\n    # In practice, this would use more sophisticated deblurring algorithms\r\n    \r\n    # Create a simple motion blur kernel\r\n    kernel_size = 15\r\n    kernel = np.zeros((kernel_size, kernel_size))\r\n    kernel[int((kernel_size-1)/2), :] = np.ones(kernel_size)\r\n    kernel = kernel / kernel_size\r\n    \r\n    # Apply Wiener filter\r\n    deblurred = cv2.filter2D(image, -1, kernel)\r\n    \r\n    return deblurred\n'})}),"\n",(0,t.jsx)(r.h3,{id:"2-dynamic-objects",children:"2. Dynamic Objects"}),"\n",(0,t.jsx)(r.p,{children:"Humanoid robots operate in environments with moving objects:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-python",children:'class DynamicObjectFilter:\r\n    def __init__(self):\r\n        self.background_subtractor = cv2.createBackgroundSubtractorMOG2()\r\n        self.motion_threshold = 50\r\n    \r\n    def filter_dynamic_objects(self, current_frame, prev_frame):\r\n        """Filter out dynamic objects from SLAM processing"""\r\n        # Use background subtraction to identify static regions\r\n        fg_mask = self.background_subtractor.apply(current_frame)\r\n        \r\n        # Also use frame differencing for immediate motion detection\r\n        if prev_frame is not None:\r\n            diff = cv2.absdiff(current_frame, prev_frame)\r\n            diff_gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\r\n            _, motion_mask = cv2.threshold(diff_gray, self.motion_threshold, 255, cv2.THRESH_BINARY)\r\n            \r\n            # Combine masks to exclude dynamic regions\r\n            combined_mask = cv2.bitwise_and(fg_mask, motion_mask)\r\n        else:\r\n            combined_mask = fg_mask\r\n        \r\n        # Return mask of static regions\r\n        return combined_mask\n'})}),"\n",(0,t.jsx)(r.h2,{id:"integration-with-navigation-systems",children:"Integration with Navigation Systems"}),"\n",(0,t.jsx)(r.h3,{id:"1-path-planning-from-slam-maps",children:"1. Path Planning from SLAM Maps"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-python",children:'import numpy as np\r\nfrom scipy.spatial import KDTree\r\n\r\nclass SLAMPathPlanner:\r\n    def __init__(self, slam_system):\r\n        self.slam_system = slam_system\r\n        self.occupancy_grid = None\r\n        self.path = []\r\n    \r\n    def create_occupancy_grid(self, resolution=0.1):\r\n        """Create occupancy grid from SLAM map for path planning"""\r\n        # Convert point cloud to occupancy grid\r\n        points = np.asarray(self.slam_system.global_map.points)\r\n        \r\n        # Determine grid dimensions\r\n        min_bounds = np.min(points, axis=0)\r\n        max_bounds = np.max(points, axis=0)\r\n        \r\n        # Create grid\r\n        grid_size = np.ceil((max_bounds[:2] - min_bounds[:2]) / resolution).astype(int)\r\n        self.occupancy_grid = np.zeros(grid_size)\r\n        \r\n        # Populate grid with obstacles\r\n        for point in points:\r\n            grid_x = int((point[0] - min_bounds[0]) / resolution)\r\n            grid_y = int((point[1] - min_bounds[1]) / resolution)\r\n            \r\n            if 0 <= grid_x < grid_size[0] and 0 <= grid_y < grid_size[1]:\r\n                self.occupancy_grid[grid_x, grid_y] = 1  # Occupied\r\n    \r\n    def plan_path(self, start, goal):\r\n        """Plan path using A* on the occupancy grid"""\r\n        # This would implement A* or other path planning algorithm\r\n        # on the occupancy grid created from SLAM map\r\n        \r\n        # Simplified: return straight line as path\r\n        path = [start, goal]\r\n        return path\n'})}),"\n",(0,t.jsx)(r.p,{children:"Visual SLAM is fundamental for humanoid robots operating in unknown environments. It enables these robots to navigate, avoid obstacles, and perform tasks in human-centric spaces where traditional mapping approaches might fail. The combination of visual perception and mapping allows humanoid robots to build understanding of their environment and use this information for navigation and task execution."})]})}function c(e={}){const{wrapper:r}={...(0,i.R)(),...e.components};return r?(0,t.jsx)(r,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},8453(e,r,n){n.d(r,{R:()=>s,x:()=>o});var a=n(6540);const t={},i=a.createContext(t);function s(e){const r=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),a.createElement(i.Provider,{value:r},e.children)}}}]);