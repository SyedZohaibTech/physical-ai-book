"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[960],{8453(n,e,r){r.d(e,{R:()=>i,x:()=>s});var t=r(6540);const o={},a=t.createContext(o);function i(n){const e=t.useContext(a);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:i(n.components),t.createElement(a.Provider,{value:e},n.children)}},8987(n,e,r){r.r(e),r.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>d,frontMatter:()=>i,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module4/voice-to-action","title":"Voice-to-Action Systems for Humanoid Robots","description":"Voice-to-Action systems enable humanoid robots to understand spoken language and convert it into executable actions. This capability is crucial for natural human-robot interaction, allowing users to command robots using everyday language rather than specialized interfaces.","source":"@site/docs/module4/voice-to-action.md","sourceDirName":"module4","slug":"/module4/voice-to-action","permalink":"/docs/module4/voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/SyedZohaibTech/physical-ai-book/edit/main/docs/module4/voice-to-action.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Voice-to-Action Systems for Humanoid Robots"},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to Vision-Language-Action Systems","permalink":"/docs/module4/introduction"},"next":{"title":"LLM-Based Task Planning for Humanoid Robots","permalink":"/docs/module4/llm-planning"}}');var o=r(4848),a=r(8453);const i={sidebar_position:2,title:"Voice-to-Action Systems for Humanoid Robots"},s="Voice-to-Action Systems for Humanoid Robots",c={},l=[{value:"Understanding Voice-to-Action Systems",id:"understanding-voice-to-action-systems",level:2},{value:"Speech Recognition for Robotics",id:"speech-recognition-for-robotics",level:2},{value:"1. Audio Processing Pipeline",id:"1-audio-processing-pipeline",level:3},{value:"2. Robust Speech Recognition",id:"2-robust-speech-recognition",level:3},{value:"Natural Language Understanding",id:"natural-language-understanding",level:2},{value:"1. Command Parsing",id:"1-command-parsing",level:3},{value:"2. Contextual Understanding",id:"2-contextual-understanding",level:3},{value:"Action Planning from Voice Commands",id:"action-planning-from-voice-commands",level:2},{value:"1. Semantic Action Mapping",id:"1-semantic-action-mapping",level:3},{value:"2. Task Planning System",id:"2-task-planning-system",level:3},{value:"Integration with Humanoid Control Systems",id:"integration-with-humanoid-control-systems",level:2},{value:"1. High-Level Voice Command Interface",id:"1-high-level-voice-command-interface",level:3},{value:"Advanced Voice Processing Techniques",id:"advanced-voice-processing-techniques",level:2},{value:"1. Wake Word Detection",id:"1-wake-word-detection",level:3},{value:"2. Multi-Turn Dialogue Management",id:"2-multi-turn-dialogue-management",level:3}];function m(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"voice-to-action-systems-for-humanoid-robots",children:"Voice-to-Action Systems for Humanoid Robots"})}),"\n",(0,o.jsx)(e.p,{children:"Voice-to-Action systems enable humanoid robots to understand spoken language and convert it into executable actions. This capability is crucial for natural human-robot interaction, allowing users to command robots using everyday language rather than specialized interfaces."}),"\n",(0,o.jsx)(e.h2,{id:"understanding-voice-to-action-systems",children:"Understanding Voice-to-Action Systems"}),"\n",(0,o.jsx)(e.p,{children:"Voice-to-Action systems process natural language commands and translate them into specific robot behaviors. This involves multiple stages:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Speech Recognition"}),": Converting audio to text"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Natural Language Understanding"}),": Interpreting the meaning"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Action Planning"}),": Generating executable actions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Execution"}),": Performing the requested tasks"]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"For humanoid robots, this creates a natural interaction modality that matches human communication patterns."}),"\n",(0,o.jsx)(e.h2,{id:"speech-recognition-for-robotics",children:"Speech Recognition for Robotics"}),"\n",(0,o.jsx)(e.h3,{id:"1-audio-processing-pipeline",children:"1. Audio Processing Pipeline"}),"\n",(0,o.jsx)(e.p,{children:"The basic audio processing pipeline for robotics:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import numpy as np\r\nimport speech_recognition as sr\r\nfrom queue import Queue\r\nimport threading\r\nimport time\r\n\r\nclass AudioProcessor:\r\n    def __init__(self):\r\n        self.recognizer = sr.Recognizer()\r\n        self.microphone = sr.Microphone()\r\n        self.audio_queue = Queue()\r\n        self.is_listening = False\r\n        \r\n        # Adjust for ambient noise\r\n        with self.microphone as source:\r\n            self.recognizer.adjust_for_ambient_noise(source)\r\n    \r\n    def start_listening(self):\r\n        """Start continuous listening for commands"""\r\n        self.is_listening = True\r\n        self.listening_thread = threading.Thread(target=self._listen_continuously)\r\n        self.listening_thread.start()\r\n    \r\n    def _listen_continuously(self):\r\n        """Continuously listen for audio and put on queue"""\r\n        with self.microphone as source:\r\n            while self.is_listening:\r\n                try:\r\n                    audio = self.recognizer.listen(source, timeout=1, phrase_time_limit=5)\r\n                    self.audio_queue.put(audio)\r\n                except sr.WaitTimeoutError:\r\n                    continue  # Keep listening\r\n    \r\n    def stop_listening(self):\r\n        """Stop the listening process"""\r\n        self.is_listening = False\r\n        if hasattr(self, \'listening_thread\'):\r\n            self.listening_thread.join()\r\n    \r\n    def get_audio_text(self, audio):\r\n        """Convert audio to text"""\r\n        try:\r\n            text = self.recognizer.recognize_google(audio)\r\n            return text\r\n        except sr.UnknownValueError:\r\n            return None\r\n        except sr.RequestError as e:\r\n            print(f"Error with speech recognition service: {e}")\r\n            return None\n'})}),"\n",(0,o.jsx)(e.h3,{id:"2-robust-speech-recognition",children:"2. Robust Speech Recognition"}),"\n",(0,o.jsx)(e.p,{children:"For robotic applications, robust speech recognition is essential:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import webrtcvad\r\nimport collections\r\nimport pyaudio\r\nimport numpy as np\r\nfrom scipy import signal\r\n\r\nclass RobustSpeechRecognizer:\r\n    def __init__(self):\r\n        # Initialize VAD (Voice Activity Detection)\r\n        self.vad = webrtcvad.Vad()\r\n        self.vad.set_mode(3)  # Aggressive VAD\r\n        \r\n        # Audio parameters\r\n        self.rate = 16000\r\n        self.chunk_duration_ms = 30  # Supports 10, 20 and 30 (ms)\r\n        self.chunk_size = int(self.rate * self.chunk_duration_ms / 1000)\r\n        self.window_duration_ms = 2000\r\n        self.p = pyaudio.PyAudio()\r\n        \r\n        # Ring buffer for audio chunks\r\n        self.ring_buffer = collections.deque(maxlen=self.window_duration_ms // self.chunk_duration_ms)\r\n        \r\n        # Initialize speech recognizer\r\n        self.recognizer = sr.Recognizer()\r\n        self.microphone = sr.Microphone()\r\n    \r\n    def detect_voice_activity(self, data, sample_rate):\r\n        """Detect if audio chunk contains voice activity"""\r\n        return self.vad.is_speech(data, sample_rate)\r\n    \r\n    def preprocess_audio(self, audio_data):\r\n        """Preprocess audio to improve recognition quality"""\r\n        # Apply noise reduction\r\n        # Convert to appropriate format for speech recognition\r\n        return audio_data\r\n    \r\n    def recognize_speech(self, audio_data):\r\n        """Recognize speech from audio data"""\r\n        try:\r\n            # Convert to audio file format for recognition\r\n            # This is a simplified version - in practice, you\'d use proper audio conversion\r\n            text = self.recognizer.recognize_google(audio_data)\r\n            return text\r\n        except Exception as e:\r\n            print(f"Speech recognition error: {e}")\r\n            return None\n'})}),"\n",(0,o.jsx)(e.h2,{id:"natural-language-understanding",children:"Natural Language Understanding"}),"\n",(0,o.jsx)(e.h3,{id:"1-command-parsing",children:"1. Command Parsing"}),"\n",(0,o.jsx)(e.p,{children:"Parsing natural language commands into structured actions:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import re\r\nfrom dataclasses import dataclass\r\nfrom typing import List, Optional\r\n\r\n@dataclass\r\nclass Command:\r\n    action: str\r\n    target: Optional[str] = None\r\n    location: Optional[str] = None\r\n    parameters: Optional[dict] = None\r\n\r\nclass CommandParser:\r\n    def __init__(self):\r\n        # Define action patterns\r\n        self.action_patterns = {\r\n            'move': [\r\n                r'go to (.+)',\r\n                r'move to (.+)',\r\n                r'walk to (.+)',\r\n                r'go (.+)',\r\n                r'navigate to (.+)'\r\n            ],\r\n            'grasp': [\r\n                r'pick up (.+)',\r\n                r'grab (.+)',\r\n                r'take (.+)',\r\n                r'pick (.+) up',\r\n                r'grasp (.+)'\r\n            ],\r\n            'place': [\r\n                r'put (.+) on (.+)',\r\n                r'place (.+) on (.+)',\r\n                r'put (.+) at (.+)',\r\n                r'place (.+) at (.+)'\r\n            ],\r\n            'follow': [\r\n                r'follow (.+)',\r\n                r'follow me',\r\n                r'come with me'\r\n            ],\r\n            'wait': [\r\n                r'wait here',\r\n                r'wait for me',\r\n                r'stop',\r\n                r'pause'\r\n            ],\r\n            'greet': [\r\n                r'say hello to (.+)',\r\n                r'greet (.+)',\r\n                r'hello (.+)',\r\n                r'wave to (.+)'\r\n            ]\r\n        }\r\n    \r\n    def parse_command(self, text: str) -> Optional[Command]:\r\n        \"\"\"Parse natural language text into a structured command\"\"\"\r\n        text = text.lower().strip()\r\n        \r\n        for action, patterns in self.action_patterns.items():\r\n            for pattern in patterns:\r\n                match = re.search(pattern, text)\r\n                if match:\r\n                    groups = match.groups()\r\n                    \r\n                    if action == 'move':\r\n                        return Command(action=action, location=groups[0])\r\n                    elif action == 'grasp':\r\n                        return Command(action=action, target=groups[0])\r\n                    elif action == 'place':\r\n                        return Command(action=action, target=groups[0], location=groups[1])\r\n                    elif action == 'follow':\r\n                        return Command(action=action, target=groups[0] if groups else 'user')\r\n                    elif action == 'wait':\r\n                        return Command(action=action)\r\n                    elif action == 'greet':\r\n                        return Command(action=action, target=groups[0])\r\n        \r\n        return None  # Command not recognized\n"})}),"\n",(0,o.jsx)(e.h3,{id:"2-contextual-understanding",children:"2. Contextual Understanding"}),"\n",(0,o.jsx)(e.p,{children:"Understanding commands in context:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"class ContextualUnderstanding:\r\n    def __init__(self):\r\n        self.current_context = {\r\n            'location': 'unknown',\r\n            'objects': [],\r\n            'people': [],\r\n            'task': None\r\n        }\r\n        self.command_history = []\r\n        \r\n    def update_context(self, new_info):\r\n        \"\"\"Update the current context with new information\"\"\"\r\n        self.current_context.update(new_info)\r\n    \r\n    def resolve_pronouns(self, text, context):\r\n        \"\"\"Resolve pronouns based on context\"\"\"\r\n        # Simple pronoun resolution\r\n        text = text.replace('it', context.get('last_object', 'it'))\r\n        text = text.replace('there', context.get('last_location', 'there'))\r\n        text = text.replace('him', context.get('last_person', 'him'))\r\n        text = text.replace('her', context.get('last_person', 'her'))\r\n        return text\r\n    \r\n    def disambiguate_command(self, command, context):\r\n        \"\"\"Resolve ambiguities in the command based on context\"\"\"\r\n        if command.action == 'move' and command.location == 'there':\r\n            command.location = context.get('last_location', command.location)\r\n        \r\n        if command.action == 'grasp' and command.target == 'it':\r\n            command.target = context.get('last_object', command.target)\r\n        \r\n        return command\r\n    \r\n    def parse_with_context(self, text, context):\r\n        \"\"\"Parse command considering the current context\"\"\"\r\n        # Resolve pronouns\r\n        resolved_text = self.resolve_pronouns(text, context)\r\n        \r\n        # Parse the resolved text\r\n        parser = CommandParser()\r\n        command = parser.parse_command(resolved_text)\r\n        \r\n        if command:\r\n            # Disambiguate based on context\r\n            command = self.disambiguate_command(command, context)\r\n            \r\n            # Update command history\r\n            self.command_history.append(command)\r\n        \r\n        return command\n"})}),"\n",(0,o.jsx)(e.h2,{id:"action-planning-from-voice-commands",children:"Action Planning from Voice Commands"}),"\n",(0,o.jsx)(e.h3,{id:"1-semantic-action-mapping",children:"1. Semantic Action Mapping"}),"\n",(0,o.jsx)(e.p,{children:"Mapping natural language commands to robot actions:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import json\r\nfrom typing import Dict, Any\r\n\r\nclass SemanticActionMapper:\r\n    def __init__(self):\r\n        # Load semantic mappings\r\n        self.action_mappings = self.load_mappings()\r\n        \r\n        # Define action execution interfaces\r\n        self.action_executors = {\r\n            'move': self.execute_move,\r\n            'grasp': self.execute_grasp,\r\n            'place': self.execute_place,\r\n            'follow': self.execute_follow,\r\n            'wait': self.execute_wait,\r\n            'greet': self.execute_greet\r\n        }\r\n    \r\n    def load_mappings(self):\r\n        \"\"\"Load semantic mappings from configuration\"\"\"\r\n        # This would typically load from a file or database\r\n        return {\r\n            'move': {\r\n                'locations': {\r\n                    'kitchen': '/map/kitchen',\r\n                    'living room': '/map/living_room',\r\n                    'bedroom': '/map/bedroom',\r\n                    'dining room': '/map/dining_room',\r\n                    'bathroom': '/map/bathroom',\r\n                    'office': '/map/office'\r\n                }\r\n            },\r\n            'grasp': {\r\n                'objects': {\r\n                    'cup': 'cup_grasp_pose',\r\n                    'book': 'book_grasp_pose',\r\n                    'bottle': 'bottle_grasp_pose',\r\n                    'box': 'box_grasp_pose'\r\n                }\r\n            }\r\n        }\r\n    \r\n    def map_command_to_action(self, command):\r\n        \"\"\"Map a parsed command to an executable action\"\"\"\r\n        if command.action not in self.action_executors:\r\n            raise ValueError(f\"Unknown action: {command.action}\")\r\n        \r\n        # Look up semantic information\r\n        semantic_info = {}\r\n        \r\n        if command.action == 'move' and command.location:\r\n            semantic_info['target_pose'] = self.action_mappings['move']['locations'].get(\r\n                command.location, command.location\r\n            )\r\n        \r\n        elif command.action == 'grasp' and command.target:\r\n            semantic_info['object_name'] = command.target\r\n            semantic_info['grasp_pose'] = self.action_mappings['grasp']['objects'].get(\r\n                command.target, 'default_grasp'\r\n            )\r\n        \r\n        elif command.action == 'place' and command.location:\r\n            semantic_info['placement_surface'] = command.location\r\n        \r\n        # Create action specification\r\n        action_spec = {\r\n            'action_type': command.action,\r\n            'semantic_info': semantic_info,\r\n            'parameters': command.parameters or {}\r\n        }\r\n        \r\n        return action_spec\r\n    \r\n    def execute_move(self, semantic_info):\r\n        \"\"\"Execute move action\"\"\"\r\n        target_pose = semantic_info.get('target_pose')\r\n        if target_pose:\r\n            # Call navigation system\r\n            print(f\"Moving to: {target_pose}\")\r\n            # navigation_client.send_goal(target_pose)\r\n        else:\r\n            print(\"No target pose specified for move action\")\r\n    \r\n    def execute_grasp(self, semantic_info):\r\n        \"\"\"Execute grasp action\"\"\"\r\n        object_name = semantic_info.get('object_name')\r\n        grasp_pose = semantic_info.get('grasp_pose', 'default_grasp')\r\n        \r\n        if object_name:\r\n            print(f\"Grasping {object_name} with {grasp_pose}\")\r\n            # manipulation_client.grasp_object(object_name, grasp_pose)\r\n        else:\r\n            print(\"No object specified for grasp action\")\r\n    \r\n    def execute_place(self, semantic_info):\r\n        \"\"\"Execute place action\"\"\"\r\n        placement_surface = semantic_info.get('placement_surface')\r\n        if placement_surface:\r\n            print(f\"Placing object on {placement_surface}\")\r\n            # manipulation_client.place_object(placement_surface)\r\n        else:\r\n            print(\"No placement surface specified for place action\")\r\n    \r\n    def execute_follow(self, semantic_info):\r\n        \"\"\"Execute follow action\"\"\"\r\n        target = semantic_info.get('target', 'user')\r\n        print(f\"Following {target}\")\r\n        # following_client.start_following(target)\r\n    \r\n    def execute_wait(self, semantic_info):\r\n        \"\"\"Execute wait action\"\"\"\r\n        print(\"Waiting...\")\r\n        # Stop robot motion\r\n        # motion_client.stop()\r\n    \r\n    def execute_greet(self, semantic_info):\r\n        \"\"\"Execute greet action\"\"\"\r\n        target = semantic_info.get('target', 'person')\r\n        print(f\"Greeting {target}\")\r\n        # gesture_client.wave()\r\n        # speech_client.say(f\"Hello, {target}!\")\n"})}),"\n",(0,o.jsx)(e.h3,{id:"2-task-planning-system",children:"2. Task Planning System"}),"\n",(0,o.jsx)(e.p,{children:"Creating complex task plans from voice commands:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"from enum import Enum\r\nfrom dataclasses import dataclass\r\nfrom typing import List, Optional\r\n\r\nclass TaskStatus(Enum):\r\n    PENDING = \"pending\"\r\n    EXECUTING = \"executing\"\r\n    COMPLETED = \"completed\"\r\n    FAILED = \"failed\"\r\n\r\n@dataclass\r\nclass TaskStep:\r\n    action: str\r\n    parameters: dict\r\n    status: TaskStatus = TaskStatus.PENDING\r\n\r\nclass TaskPlanner:\r\n    def __init__(self):\r\n        self.current_tasks = []\r\n        self.task_history = []\r\n    \r\n    def create_task_from_command(self, command):\r\n        \"\"\"Create a task plan from a voice command\"\"\"\r\n        task_steps = []\r\n        \r\n        if command.action == 'fetch_and_place':\r\n            # Complex task: fetch object and place it somewhere\r\n            task_steps.append(TaskStep(\r\n                action='navigate',\r\n                parameters={'target_location': command.parameters.get('fetch_from')}\r\n            ))\r\n            task_steps.append(TaskStep(\r\n                action='grasp',\r\n                parameters={'object': command.target}\r\n            ))\r\n            task_steps.append(TaskStep(\r\n                action='navigate',\r\n                parameters={'target_location': command.parameters.get('place_at')}\r\n            ))\r\n            task_steps.append(TaskStep(\r\n                action='place',\r\n                parameters={'surface': command.parameters.get('place_at')}\r\n            ))\r\n        elif command.action == 'move':\r\n            task_steps.append(TaskStep(\r\n                action='navigate',\r\n                parameters={'target_location': command.location}\r\n            ))\r\n        elif command.action == 'grasp':\r\n            task_steps.append(TaskStep(\r\n                action='navigate_to_object',\r\n                parameters={'object': command.target}\r\n            ))\r\n            task_steps.append(TaskStep(\r\n                action='grasp',\r\n                parameters={'object': command.target}\r\n            ))\r\n        else:\r\n            # Simple one-step tasks\r\n            task_steps.append(TaskStep(\r\n                action=command.action,\r\n                parameters={\r\n                    'target': command.target,\r\n                    'location': command.location,\r\n                    **(command.parameters or {})\r\n                }\r\n            ))\r\n        \r\n        return task_steps\r\n    \r\n    def execute_task(self, task_steps: List[TaskStep]):\r\n        \"\"\"Execute a sequence of task steps\"\"\"\r\n        for i, step in enumerate(task_steps):\r\n            print(f\"Executing step {i+1}/{len(task_steps)}: {step.action}\")\r\n            \r\n            # Execute the step\r\n            success = self.execute_single_step(step)\r\n            \r\n            if not success:\r\n                step.status = TaskStatus.FAILED\r\n                print(f\"Task failed at step {i+1}\")\r\n                return False\r\n            \r\n            step.status = TaskStatus.COMPLETED\r\n            print(f\"Step {i+1} completed\")\r\n        \r\n        return True\r\n    \r\n    def execute_single_step(self, step: TaskStep):\r\n        \"\"\"Execute a single task step\"\"\"\r\n        # This would interface with the robot's action execution system\r\n        # For demonstration, we'll just print the action\r\n        print(f\"Executing: {step.action} with params: {step.parameters}\")\r\n        \r\n        # Simulate execution (in reality, this would call robot services/actions)\r\n        import time\r\n        time.sleep(0.5)  # Simulate action execution time\r\n        \r\n        # Return success/failure (in reality, this would check robot feedback)\r\n        return True\n"})}),"\n",(0,o.jsx)(e.h2,{id:"integration-with-humanoid-control-systems",children:"Integration with Humanoid Control Systems"}),"\n",(0,o.jsx)(e.h3,{id:"1-high-level-voice-command-interface",children:"1. High-Level Voice Command Interface"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom action_msgs.msg import GoalStatus\r\nfrom rclpy.action import ActionClient\r\n\r\nclass VoiceCommandInterface(Node):\r\n    def __init__(self):\r\n        super().__init__(\'voice_command_interface\')\r\n        \r\n        # Publishers and subscribers\r\n        self.status_pub = self.create_publisher(String, \'/voice_command_status\', 10)\r\n        self.speech_pub = self.create_publisher(String, \'/tts_input\', 10)\r\n        \r\n        # Action clients\r\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\r\n        self.manip_client = ActionClient(self, ManipulateObject, \'manipulate_object\')\r\n        \r\n        # Voice processing components\r\n        self.audio_processor = AudioProcessor()\r\n        self.command_parser = CommandParser()\r\n        self.action_mapper = SemanticActionMapper()\r\n        self.task_planner = TaskPlanner()\r\n        \r\n        # State management\r\n        self.is_listening = False\r\n        self.current_task = None\r\n        \r\n        # Start voice processing\r\n        self.voice_timer = self.create_timer(0.1, self.process_voice_commands)\r\n    \r\n    def start_listening(self):\r\n        """Start listening for voice commands"""\r\n        self.is_listening = True\r\n        self.audio_processor.start_listening()\r\n        self.publish_status("Listening for commands...")\r\n    \r\n    def stop_listening(self):\r\n        """Stop listening for voice commands"""\r\n        self.is_listening = False\r\n        self.audio_processor.stop_listening()\r\n        self.publish_status("Stopped listening")\r\n    \r\n    def process_voice_commands(self):\r\n        """Process any queued voice commands"""\r\n        if not self.is_listening:\r\n            return\r\n        \r\n        # Check for new audio\r\n        if not self.audio_processor.audio_queue.empty():\r\n            audio = self.audio_processor.audio_queue.get()\r\n            \r\n            # Convert to text\r\n            text = self.audio_processor.get_audio_text(audio)\r\n            if text:\r\n                self.get_logger().info(f"Heard: {text}")\r\n                \r\n                # Parse the command\r\n                command = self.command_parser.parse_command(text)\r\n                if command:\r\n                    self.get_logger().info(f"Parsed command: {command}")\r\n                    \r\n                    # Map to action\r\n                    action_spec = self.action_mapper.map_command_to_action(command)\r\n                    \r\n                    # Execute or plan the task\r\n                    self.execute_voice_command(command, action_spec)\r\n                else:\r\n                    self.publish_status(f"Command not understood: {text}")\r\n                    self.speak_response(f"Sorry, I didn\'t understand: {text}")\r\n    \r\n    def execute_voice_command(self, command, action_spec):\r\n        """Execute a voice command"""\r\n        if command.action in [\'move\', \'grasp\', \'place\', \'follow\', \'wait\', \'greet\']:\r\n            # Execute directly\r\n            executor = self.action_mapper.action_executors[command.action]\r\n            executor(action_spec.get(\'semantic_info\', {}))\r\n        else:\r\n            # Create and execute task plan\r\n            task_steps = self.task_planner.create_task_from_command(command)\r\n            success = self.task_planner.execute_task(task_steps)\r\n            \r\n            if success:\r\n                self.publish_status(f"Completed task: {command.action}")\r\n                self.speak_response("Task completed successfully")\r\n            else:\r\n                self.publish_status(f"Failed to complete task: {command.action}")\r\n                self.speak_response("Sorry, I couldn\'t complete that task")\r\n    \r\n    def publish_status(self, status):\r\n        """Publish status message"""\r\n        status_msg = String()\r\n        status_msg.data = status\r\n        self.status_pub.publish(status_msg)\r\n    \r\n    def speak_response(self, text):\r\n        """Speak a response using TTS"""\r\n        tts_msg = String()\r\n        tts_msg.data = text\r\n        self.speech_pub.publish(tts_msg)\n'})}),"\n",(0,o.jsx)(e.h2,{id:"advanced-voice-processing-techniques",children:"Advanced Voice Processing Techniques"}),"\n",(0,o.jsx)(e.h3,{id:"1-wake-word-detection",children:"1. Wake Word Detection"}),"\n",(0,o.jsx)(e.p,{children:"For always-listening systems:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import numpy as np\r\nimport collections\r\nimport pyaudio\r\nimport webrtcvad\r\nfrom scipy import signal\r\n\r\nclass WakeWordDetector:\r\n    def __init__(self, wake_words=[\'robot\', \'hey robot\', \'assistant\']):\r\n        self.wake_words = wake_words\r\n        self.vad = webrtcvad.Vad(3)  # Aggressive VAD\r\n        \r\n        # Audio parameters\r\n        self.rate = 16000\r\n        self.chunk_duration_ms = 30\r\n        self.chunk_size = int(self.rate * self.chunk_duration_ms / 1000)\r\n        \r\n        # Initialize PyAudio\r\n        self.p = pyaudio.PyAudio()\r\n        \r\n        # Ring buffer for audio chunks\r\n        self.ring_buffer = collections.deque(\r\n            maxlen=30  # Store 30 chunks (900ms of audio)\r\n        )\r\n        \r\n        # State\r\n        self.is_awake = False\r\n        self.listening_callback = None\r\n    \r\n    def set_listening_callback(self, callback):\r\n        """Set callback to be called when wake word is detected"""\r\n        self.listening_callback = callback\r\n    \r\n    def detect_wake_word(self, audio_chunk):\r\n        """Detect if the audio chunk contains a wake word"""\r\n        # This is a simplified implementation\r\n        # In practice, you\'d use a dedicated wake word detection model\r\n        # like Porcupine, Snowboy, or a custom model\r\n        \r\n        # For demonstration, we\'ll simulate detection\r\n        # by checking if the audio has speech-like characteristics\r\n        if self.vad.is_speech(audio_chunk, self.rate):\r\n            # Simulate successful detection after some time\r\n            import random\r\n            if random.random() < 0.1:  # 10% chance for demo\r\n                return True\r\n        return False\r\n    \r\n    def start_listening(self):\r\n        """Start the wake word detection loop"""\r\n        stream = self.p.open(\r\n            format=pyaudio.paInt16,\r\n            channels=1,\r\n            rate=self.rate,\r\n            input=True,\r\n            frames_per_buffer=self.chunk_size\r\n        )\r\n        \r\n        try:\r\n            while True:\r\n                chunk = stream.read(self.chunk_size, exception_on_overflow=False)\r\n                self.ring_buffer.append(chunk)\r\n                \r\n                if self.detect_wake_word(chunk):\r\n                    if not self.is_awake:\r\n                        self.is_awake = True\r\n                        if self.listening_callback:\r\n                            self.listening_callback()\r\n                        \r\n                        # Reset after wake word detection\r\n                        self.ring_buffer.clear()\r\n                        \r\n                        # Wait for command (simplified)\r\n                        import time\r\n                        time.sleep(5)  # Wait 5 seconds for command\r\n                        \r\n                        self.is_awake = False\r\n        except KeyboardInterrupt:\r\n            pass\r\n        finally:\r\n            stream.stop_stream()\r\n            stream.close()\n'})}),"\n",(0,o.jsx)(e.h3,{id:"2-multi-turn-dialogue-management",children:"2. Multi-Turn Dialogue Management"}),"\n",(0,o.jsx)(e.p,{children:"Handling complex conversations:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'from enum import Enum\r\nfrom dataclasses import dataclass\r\nfrom typing import Dict, Any, Optional\r\n\r\nclass DialogueState(Enum):\r\n    IDLE = "idle"\r\n    LISTENING = "listening"\r\n    PROCESSING = "processing"\r\n    WAITING_FOR_CONFIRMATION = "waiting_for_confirmation"\r\n    EXECUTING = "executing"\r\n\r\n@dataclass\r\nclass DialogueContext:\r\n    state: DialogueState\r\n    current_intent: Optional[str] = None\r\n    entities: Dict[str, Any] = None\r\n    waiting_for: Optional[str] = None\r\n    conversation_id: Optional[str] = None\r\n\r\nclass DialogueManager:\r\n    def __init__(self):\r\n        self.context = DialogueContext(state=DialogueState.IDLE)\r\n        self.command_parser = CommandParser()\r\n        self.context_understanding = ContextualUnderstanding()\r\n        \r\n    def process_input(self, text: str) -> str:\r\n        """Process user input and return system response"""\r\n        if self.context.state == DialogueState.IDLE:\r\n            return self.handle_initial_command(text)\r\n        elif self.context.state == DialogueState.WAITING_FOR_CONFIRMATION:\r\n            return self.handle_confirmation(text)\r\n        else:\r\n            # Handle other states\r\n            return self.handle_follow_up(text)\r\n    \r\n    def handle_initial_command(self, text: str) -> str:\r\n        """Handle the initial command from user"""\r\n        command = self.command_parser.parse_command(text)\r\n        \r\n        if command:\r\n            # Check if we have all required information\r\n            missing_info = self.check_missing_info(command)\r\n            \r\n            if missing_info:\r\n                # Ask for missing information\r\n                self.context.state = DialogueState.WAITING_FOR_CONFIRMATION\r\n                self.context.waiting_for = missing_info\r\n                return f"Could you please specify {missing_info}?"\r\n            else:\r\n                # Execute the command\r\n                self.context.state = DialogueState.PROCESSING\r\n                return self.execute_command(command)\r\n        else:\r\n            return "I didn\'t understand that command. Could you rephrase it?"\r\n    \r\n    def check_missing_info(self, command) -> Optional[str]:\r\n        """Check if the command has missing required information"""\r\n        if command.action == \'move\' and not command.location:\r\n            return "where you\'d like me to go"\r\n        elif command.action == \'grasp\' and not command.target:\r\n            return "what you\'d like me to pick up"\r\n        elif command.action == \'place\' and not command.target:\r\n            return "what you\'d like me to place"\r\n        return None\r\n    \r\n    def handle_confirmation(self, text: str) -> str:\r\n        """Handle user confirmation or additional information"""\r\n        if self.context.waiting_for:\r\n            # Parse the additional information\r\n            # This is a simplified approach\r\n            if \'to\' in text or \'at\' in text:\r\n                # User provided location\r\n                import re\r\n                location_match = re.search(r\'to (.+)|at (.+)\', text.lower())\r\n                if location_match:\r\n                    location = location_match.group(1) or location_match.group(2)\r\n                    # Update context and execute\r\n                    self.context.state = DialogueState.PROCESSING\r\n                    return f"Okay, going to {location}."\r\n            elif \'the\' in text or \'a\' in text:\r\n                # User specified an object\r\n                # Simplified object extraction\r\n                import re\r\n                obj_match = re.search(r\'(the |a |an )(.+)\', text.lower())\r\n                if obj_match:\r\n                    obj = obj_match.group(2)\r\n                    self.context.state = DialogueState.PROCESSING\r\n                    return f"Okay, I\'ll get the {obj}."\r\n        \r\n        return "I\'m not sure I understood. Could you clarify?"\r\n    \r\n    def handle_follow_up(self, text: str) -> str:\r\n        """Handle follow-up questions or commands"""\r\n        if \'yes\' in text.lower() or \'yeah\' in text.lower():\r\n            # Confirm previous action\r\n            return "Okay, proceeding with the action."\r\n        elif \'no\' in text.lower() or \'nope\' in text.lower():\r\n            # Cancel previous action\r\n            self.context.state = DialogueState.IDLE\r\n            return "Okay, I won\'t do that."\r\n        else:\r\n            # New command\r\n            return self.handle_initial_command(text)\r\n    \r\n    def execute_command(self, command) -> str:\r\n        """Execute the parsed command"""\r\n        # This would interface with the action execution system\r\n        if command.action == \'move\':\r\n            return f"Moving to {command.location}."\r\n        elif command.action == \'grasp\':\r\n            return f"Attempting to grasp {command.target}."\r\n        else:\r\n            return f"Executing {command.action} action."\r\n    \r\n    def reset_dialogue(self):\r\n        """Reset the dialogue context"""\r\n        self.context = DialogueContext(state=DialogueState.IDLE)\n'})}),"\n",(0,o.jsx)(e.p,{children:"Voice-to-Action systems are fundamental to creating natural, intuitive interactions between humans and humanoid robots. By enabling robots to understand and respond to spoken commands, these systems make robots more accessible and useful in everyday environments. The integration of robust speech recognition, natural language understanding, and action planning creates powerful capabilities for humanoid robots to assist humans in various tasks."})]})}function d(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(m,{...n})}):m(n)}}}]);