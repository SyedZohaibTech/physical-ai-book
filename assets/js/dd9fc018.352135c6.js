"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[404],{6250(e,n,r){r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module4/capstone-project","title":"Capstone Project - Complete Humanoid Robot System","description":"The capstone project integrates all the concepts learned throughout this course to build a complete humanoid robot system. This project demonstrates how ROS 2, Gazebo/Unity, NVIDIA Isaac, Visual SLAM, Navigation 2, Vision-Language-Action systems, and LLM-based planning work together to create an intelligent, autonomous humanoid robot.","source":"@site/docs/module4/capstone-project.md","sourceDirName":"module4","slug":"/module4/capstone-project","permalink":"/physical-ai-book/docs/module4/capstone-project","draft":false,"unlisted":false,"editUrl":"https://github.com/SyedZohaibTech/physical-ai-book/edit/main/docs/module4/capstone-project.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Capstone Project - Complete Humanoid Robot System"},"sidebar":"tutorialSidebar","previous":{"title":"LLM-Based Task Planning for Humanoid Robots","permalink":"/physical-ai-book/docs/module4/llm-planning"}}');var t=r(4848),a=r(8453);const o={sidebar_position:4,title:"Capstone Project - Complete Humanoid Robot System"},i="Capstone Project - Complete Humanoid Robot System",l={},c=[{value:"Project Overview",id:"project-overview",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"1. High-Level Architecture",id:"1-high-level-architecture",level:3},{value:"2. Software Components",id:"2-software-components",level:3},{value:"Implementation Plan",id:"implementation-plan",level:2},{value:"1. Perception System Implementation",id:"1-perception-system-implementation",level:3},{value:"2. SLAM System Integration",id:"2-slam-system-integration",level:3},{value:"3. Voice-to-Action System",id:"3-voice-to-action-system",level:3},{value:"4. LLM-Based Task Planning System",id:"4-llm-based-task-planning-system",level:3},{value:"5. Navigation System Integration",id:"5-navigation-system-integration",level:3},{value:"6. System Integration Node",id:"6-system-integration-node",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"1. Unit Testing",id:"1-unit-testing",level:3},{value:"2. Integration Testing",id:"2-integration-testing",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"1. System Optimization",id:"1-system-optimization",level:3},{value:"2. Real-time Performance",id:"2-real-time-performance",level:3},{value:"Conclusion",id:"conclusion",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"capstone-project---complete-humanoid-robot-system",children:"Capstone Project - Complete Humanoid Robot System"})}),"\n",(0,t.jsx)(n.p,{children:"The capstone project integrates all the concepts learned throughout this course to build a complete humanoid robot system. This project demonstrates how ROS 2, Gazebo/Unity, NVIDIA Isaac, Visual SLAM, Navigation 2, Vision-Language-Action systems, and LLM-based planning work together to create an intelligent, autonomous humanoid robot."}),"\n",(0,t.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,t.jsx)(n.p,{children:"The capstone project involves developing a humanoid robot system capable of:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understanding and executing natural language commands"}),"\n",(0,t.jsx)(n.li,{children:"Navigating complex environments using SLAM"}),"\n",(0,t.jsx)(n.li,{children:"Manipulating objects based on visual perception"}),"\n",(0,t.jsx)(n.li,{children:"Planning and executing complex tasks using LLMs"}),"\n",(0,t.jsx)(n.li,{children:"Operating safely in human environments"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,t.jsx)(n.h3,{id:"1-high-level-architecture",children:"1. High-Level Architecture"}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\r\n    subgraph "User Interface"\r\n        A[Natural Language Command]\r\n        B[Speech Recognition]\r\n        C[Text Input]\r\n    end\r\n    \r\n    subgraph "Perception Layer"\r\n        D[RGB Cameras]\r\n        E[Depth Sensors]\r\n        F[LIDAR]\r\n        G[IMU]\r\n        H[Force/Torque Sensors]\r\n    end\r\n    \r\n    subgraph "Processing Layer"\r\n        I[ROS 2 Middleware]\r\n        J[SLAM System]\r\n        K[Object Detection]\r\n        L[Human Detection]\r\n        M[LLM Planner]\r\n        N[Navigation System]\r\n        O[Manipulation Planner]\r\n    end\r\n    \r\n    subgraph "Control Layer"\r\n        P[Whole-Body Controller]\r\n        Q[Balance Controller]\r\n        R[Manipulation Controller]\r\n        S[Navigation Controller]\r\n    end\r\n    \r\n    subgraph "Simulation Layer"\r\n        T[Isaac Sim]\r\n        U[Gazebo]\r\n        V[Unity]\r\n    end\r\n    \r\n    subgraph "Hardware Layer"\r\n        W[Humanoid Robot]\r\n        X[Real Sensors]\r\n        Y[Real Actuators]\r\n    end\r\n    \r\n    A --\x3e B\r\n    B --\x3e I\r\n    C --\x3e I\r\n    D --\x3e I\r\n    E --\x3e I\r\n    F --\x3e I\r\n    G --\x3e I\r\n    H --\x3e I\r\n    I --\x3e J\r\n    I --\x3e K\r\n    I --\x3e L\r\n    I --\x3e M\r\n    I --\x3e N\r\n    I --\x3e O\r\n    J --\x3e N\r\n    K --\x3e O\r\n    L --\x3e N\r\n    M --\x3e N\r\n    M --\x3e O\r\n    N --\x3e S\r\n    O --\x3e R\r\n    S --\x3e P\r\n    R --\x3e P\r\n    P --\x3e W\r\n    T --\x3e W\r\n    U --\x3e W\r\n    V --\x3e W\r\n    W --\x3e X\r\n    W --\x3e Y\r\n    X --\x3e I\r\n    Y --\x3e I'}),"\n",(0,t.jsx)(n.h3,{id:"2-software-components",children:"2. Software Components"}),"\n",(0,t.jsx)(n.p,{children:"The complete system consists of several integrated software components:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception System"}),": Processes sensor data to understand the environment"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"SLAM System"}),": Creates and maintains maps of the environment"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision-Language-Action System"}),": Interprets natural language commands"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Task Planning System"}),": Uses LLMs to plan complex tasks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Navigation System"}),": Plans and executes navigation tasks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Manipulation System"}),": Plans and executes manipulation tasks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Control System"}),": Low-level control of robot actuators"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"implementation-plan",children:"Implementation Plan"}),"\n",(0,t.jsx)(n.h3,{id:"1-perception-system-implementation",children:"1. Perception System Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, PointCloud2, Imu, LaserScan\r\nfrom vision_msgs.msg import Detection2DArray\r\nfrom std_msgs.msg import String\r\nimport cv2\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\nimport message_filters\r\n\r\nclass PerceptionSystem(Node):\r\n    def __init__(self):\r\n        super().__init__(\'perception_system\')\r\n        \r\n        # Initialize CV bridge\r\n        self.cv_bridge = CvBridge()\r\n        \r\n        # Create subscribers for different sensors\r\n        self.rgb_sub = self.create_subscription(\r\n            Image, \'/camera/rgb/image_raw\', self.rgb_callback, 10\r\n        )\r\n        \r\n        self.depth_sub = self.create_subscription(\r\n            Image, \'/camera/depth/image_raw\', self.depth_callback, 10\r\n        )\r\n        \r\n        self.lidar_sub = self.create_subscription(\r\n            LaserScan, \'/scan\', self.lidar_callback, 10\r\n        )\r\n        \r\n        self.imu_sub = self.create_subscription(\r\n            Imu, \'/imu/data\', self.imu_callback, 10\r\n        )\r\n        \r\n        # Publishers for processed data\r\n        self.object_detection_pub = self.create_publisher(\r\n            Detection2DArray, \'/object_detections\', 10\r\n        )\r\n        \r\n        self.human_detection_pub = self.create_publisher(\r\n            Detection2DArray, \'/human_detections\', 10\r\n        )\r\n        \r\n        # Initialize perception models\r\n        self.initialize_models()\r\n        \r\n        # State variables\r\n        self.current_rgb_image = None\r\n        self.current_depth_image = None\r\n        self.current_lidar_data = None\r\n        self.current_imu_data = None\r\n        \r\n        # Timer for processing\r\n        self.process_timer = self.create_timer(0.1, self.process_sensor_data)\r\n    \r\n    def initialize_models(self):\r\n        """Initialize perception models"""\r\n        # Load object detection model\r\n        self.object_detector = self.load_object_detector()\r\n        \r\n        # Load human detection model\r\n        self.human_detector = self.load_human_detector()\r\n        \r\n        # Load depth processing model\r\n        self.depth_processor = self.load_depth_processor()\r\n    \r\n    def load_object_detector(self):\r\n        """Load object detection model (e.g., YOLO)"""\r\n        # In practice, this would load a trained model\r\n        # For this example, we\'ll use a mock implementation\r\n        class MockDetector:\r\n            def detect(self, image):\r\n                # Simulate object detection\r\n                detections = Detection2DArray()\r\n                # Add mock detections\r\n                return detections\r\n        \r\n        return MockDetector()\r\n    \r\n    def load_human_detector(self):\r\n        """Load human detection model"""\r\n        class MockHumanDetector:\r\n            def detect(self, image):\r\n                # Simulate human detection\r\n                detections = Detection2DArray()\r\n                # Add mock human detections\r\n                return detections\r\n        \r\n        return MockHumanDetector()\r\n    \r\n    def load_depth_processor(self):\r\n        """Load depth processing model"""\r\n        class MockDepthProcessor:\r\n            def process(self, depth_image):\r\n                # Simulate depth processing\r\n                return np.zeros((depth_image.height, depth_image.width))\r\n        \r\n        return MockDepthProcessor()\r\n    \r\n    def rgb_callback(self, msg):\r\n        """Process RGB camera data"""\r\n        self.current_rgb_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\r\n    \r\n    def depth_callback(self, msg):\r\n        """Process depth camera data"""\r\n        self.current_depth_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'32FC1\')\r\n    \r\n    def lidar_callback(self, msg):\r\n        """Process LIDAR data"""\r\n        self.current_lidar_data = msg\r\n    \r\n    def imu_callback(self, msg):\r\n        """Process IMU data"""\r\n        self.current_imu_data = msg\r\n    \r\n    def process_sensor_data(self):\r\n        """Process all available sensor data"""\r\n        if self.current_rgb_image is not None:\r\n            # Run object detection\r\n            object_detections = self.object_detector.detect(self.current_rgb_image)\r\n            self.object_detection_pub.publish(object_detections)\r\n            \r\n            # Run human detection\r\n            human_detections = self.human_detector.detect(self.current_rgb_image)\r\n            self.human_detection_pub.publish(human_detections)\r\n            \r\n            # Reset image to avoid reprocessing\r\n            self.current_rgb_image = None\r\n        \r\n        if self.current_depth_image is not None:\r\n            # Process depth data\r\n            processed_depth = self.depth_processor.process(self.current_depth_image)\r\n            # Publish processed depth or use in other systems\r\n            self.current_depth_image = None\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-slam-system-integration",children:"2. SLAM System Integration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, PointCloud2, LaserScan\r\nfrom nav_msgs.msg import OccupancyGrid, Odometry\r\nfrom geometry_msgs.msg import PoseStamped\r\nimport numpy as np\r\n\r\nclass SLAMSystem(Node):\r\n    def __init__(self):\r\n        super().__init__(\'slam_system\')\r\n        \r\n        # Subscribers for sensor data\r\n        self.odom_sub = self.create_subscription(\r\n            Odometry, \'/odom\', self.odom_callback, 10\r\n        )\r\n        \r\n        self.lidar_sub = self.create_subscription(\r\n            LaserScan, \'/scan\', self.lidar_callback, 10\r\n        )\r\n        \r\n        self.rgb_sub = self.create_subscription(\r\n            Image, \'/camera/rgb/image_raw\', self.rgb_callback, 10\r\n        )\r\n        \r\n        # Publishers for SLAM outputs\r\n        self.map_pub = self.create_publisher(OccupancyGrid, \'/map\', 10)\r\n        self.pose_pub = self.create_publisher(PoseStamped, \'/slam_pose\', 10)\r\n        \r\n        # SLAM algorithm components\r\n        self.initialize_slam_components()\r\n        \r\n        # State variables\r\n        self.current_pose = np.eye(4)\r\n        self.map = np.zeros((100, 100))  # Placeholder for map\r\n        \r\n        # Timer for map updates\r\n        self.map_timer = self.create_timer(0.5, self.update_map)\r\n    \r\n    def initialize_slam_components(self):\r\n        """Initialize SLAM algorithm components"""\r\n        # Initialize visual SLAM\r\n        self.visual_slam = self.initialize_visual_slam()\r\n        \r\n        # Initialize LIDAR SLAM\r\n        self.lidar_slam = self.initialize_lidar_slam()\r\n        \r\n        # Initialize sensor fusion\r\n        self.sensor_fusion = self.initialize_sensor_fusion()\r\n    \r\n    def initialize_visual_slam(self):\r\n        """Initialize visual SLAM components"""\r\n        class MockVisualSLAM:\r\n            def process_frame(self, image, pose):\r\n                # Simulate visual SLAM processing\r\n                return pose  # Return updated pose\r\n        \r\n        return MockVisualSLAM()\r\n    \r\n    def initialize_lidar_slam(self):\r\n        """Initialize LIDAR SLAM components"""\r\n        class MockLidarSLAM:\r\n            def process_scan(self, scan, pose):\r\n                # Simulate LIDAR SLAM processing\r\n                return pose  # Return updated pose\r\n        \r\n        return MockLidarSLAM()\r\n    \r\n    def initialize_sensor_fusion(self):\r\n        """Initialize sensor fusion components"""\r\n        class MockSensorFusion:\r\n            def fuse_poses(self, visual_pose, lidar_pose, imu_pose):\r\n                # Simulate sensor fusion\r\n                return (visual_pose + lidar_pose + imu_pose) / 3  # Simple average\r\n        \r\n        return MockSensorFusion()\r\n    \r\n    def odom_callback(self, msg):\r\n        """Process odometry data"""\r\n        # Extract pose from odometry message\r\n        pose = np.eye(4)\r\n        pose[0, 3] = msg.pose.pose.position.x\r\n        pose[1, 3] = msg.pose.pose.position.y\r\n        pose[2, 3] = msg.pose.pose.position.z\r\n        \r\n        # Update current pose\r\n        self.current_pose = pose\r\n    \r\n    def lidar_callback(self, msg):\r\n        """Process LIDAR scan"""\r\n        # Process LIDAR data with LIDAR SLAM\r\n        lidar_pose = self.lidar_slam.process_scan(msg, self.current_pose)\r\n        \r\n        # Update current pose\r\n        self.current_pose = lidar_pose\r\n    \r\n    def rgb_callback(self, msg):\r\n        """Process RGB image for visual SLAM"""\r\n        # Convert to OpenCV image\r\n        cv_image = CvBridge().imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\r\n        \r\n        # Process with visual SLAM\r\n        visual_pose = self.visual_slam.process_frame(cv_image, self.current_pose)\r\n        \r\n        # Update current pose\r\n        self.current_pose = visual_pose\r\n    \r\n    def update_map(self):\r\n        """Update and publish the map"""\r\n        # Create occupancy grid message\r\n        map_msg = OccupancyGrid()\r\n        map_msg.header.stamp = self.get_clock().now().to_msg()\r\n        map_msg.header.frame_id = \'map\'\r\n        \r\n        # Set map properties\r\n        map_msg.info.resolution = 0.05  # 5cm resolution\r\n        map_msg.info.width = self.map.shape[1]\r\n        map_msg.info.height = self.map.shape[0]\r\n        map_msg.info.origin.position.x = -self.map.shape[1] * 0.05 / 2\r\n        map_msg.info.origin.position.y = -self.map.shape[0] * 0.05 / 2\r\n        \r\n        # Flatten map data\r\n        map_msg.data = self.map.flatten().astype(np.int8).tolist()\r\n        \r\n        # Publish map\r\n        self.map_pub.publish(map_msg)\r\n        \r\n        # Publish current pose\r\n        pose_msg = PoseStamped()\r\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\r\n        pose_msg.header.frame_id = \'map\'\r\n        pose_msg.pose.position.x = self.current_pose[0, 3]\r\n        pose_msg.pose.position.y = self.current_pose[1, 3]\r\n        pose_msg.pose.position.z = self.current_pose[2, 3]\r\n        \r\n        # Simple orientation (in practice, would use full rotation)\r\n        pose_msg.pose.orientation.w = 1.0\r\n        \r\n        self.pose_pub.publish(pose_msg)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"3-voice-to-action-system",children:"3. Voice-to-Action System"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom action_msgs.msg import GoalStatus\r\nfrom rclpy.action import ActionClient\r\nfrom .msg import NavigateToPose, ManipulateObject\r\n\r\nclass VoiceToActionSystem(Node):\r\n    def __init__(self):\r\n        super().__init__('voice_to_action_system')\r\n        \r\n        # Subscribers\r\n        self.command_sub = self.create_subscription(\r\n            String, '/voice_command', self.command_callback, 10\r\n        )\r\n        \r\n        # Publishers\r\n        self.status_pub = self.create_publisher(String, '/voice_action_status', 10)\r\n        \r\n        # Action clients\r\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\r\n        self.manip_client = ActionClient(self, ManipulateObject, 'manipulate_object')\r\n        \r\n        # Initialize command parser\r\n        self.command_parser = self.initialize_command_parser()\r\n        \r\n        # State\r\n        self.is_executing = False\r\n        self.current_command = None\r\n    \r\n    def initialize_command_parser(self):\r\n        \"\"\"Initialize the command parser\"\"\"\r\n        class CommandParser:\r\n            def __init__(self):\r\n                self.commands = {\r\n                    'go to (.+)': 'navigate',\r\n                    'move to (.+)': 'navigate',\r\n                    'walk to (.+)': 'navigate',\r\n                    'pick up (.+)': 'grasp',\r\n                    'grasp (.+)': 'grasp',\r\n                    'take (.+)': 'grasp',\r\n                    'put (.+) on (.+)': 'place',\r\n                    'place (.+) on (.+)': 'place',\r\n                    'pick (.+) and place on (.+)': 'fetch_and_place'\r\n                }\r\n            \r\n            def parse(self, command):\r\n                import re\r\n                for pattern, action_type in self.commands.items():\r\n                    match = re.match(pattern, command.lower().strip())\r\n                    if match:\r\n                        return {\r\n                            'action': action_type,\r\n                            'params': match.groups()\r\n                        }\r\n                return None\r\n        \r\n        return CommandParser()\r\n    \r\n    def command_callback(self, msg):\r\n        \"\"\"Process voice command\"\"\"\r\n        command_text = msg.data\r\n        \r\n        # Parse the command\r\n        parsed_command = self.command_parser.parse(command_text)\r\n        \r\n        if parsed_command:\r\n            self.get_logger().info(f'Parsed command: {parsed_command}')\r\n            \r\n            # Execute the command\r\n            success = self.execute_command(parsed_command)\r\n            \r\n            if success:\r\n                self.publish_status(f'Successfully executed: {command_text}')\r\n            else:\r\n                self.publish_status(f'Failed to execute: {command_text}')\r\n        else:\r\n            self.publish_status(f'Could not understand command: {command_text}')\r\n    \r\n    def execute_command(self, command):\r\n        \"\"\"Execute a parsed command\"\"\"\r\n        if command['action'] == 'navigate':\r\n            return self.execute_navigate_command(command['params'])\r\n        elif command['action'] == 'grasp':\r\n            return self.execute_grasp_command(command['params'])\r\n        elif command['action'] == 'place':\r\n            return self.execute_place_command(command['params'])\r\n        elif command['action'] == 'fetch_and_place':\r\n            return self.execute_fetch_and_place_command(command['params'])\r\n        else:\r\n            self.get_logger().error(f'Unknown command action: {command[\"action\"]}')\r\n            return False\r\n    \r\n    def execute_navigate_command(self, params):\r\n        \"\"\"Execute navigation command\"\"\"\r\n        location = params[0]\r\n        \r\n        # Convert location to coordinates (this would use a map/semantic localization)\r\n        # For this example, we'll use a simple mapping\r\n        location_coords = {\r\n            'kitchen': (3.0, 2.0, 0.0),\r\n            'living room': (0.0, 0.0, 0.0),\r\n            'bedroom': (-2.0, 1.0, 0.0),\r\n            'bathroom': (-1.0, -2.0, 0.0)\r\n        }\r\n        \r\n        if location in location_coords:\r\n            target_pos = location_coords[location]\r\n            \r\n            # Create navigation goal\r\n            goal_msg = NavigateToPose.Goal()\r\n            goal_msg.pose.header.frame_id = 'map'\r\n            goal_msg.pose.pose.position.x = target_pos[0]\r\n            goal_msg.pose.pose.position.y = target_pos[1]\r\n            goal_msg.pose.pose.position.z = target_pos[2]\r\n            goal_msg.pose.pose.orientation.w = 1.0  # No rotation\r\n            \r\n            # Send navigation goal\r\n            self.nav_client.wait_for_server()\r\n            future = self.nav_client.send_goal_async(goal_msg)\r\n            \r\n            # Wait for result\r\n            rclpy.spin_until_future_complete(self, future)\r\n            goal_handle = future.result()\r\n            \r\n            return goal_handle.status == GoalStatus.STATUS_SUCCEEDED\r\n        else:\r\n            self.get_logger().error(f'Unknown location: {location}')\r\n            return False\r\n    \r\n    def execute_grasp_command(self, params):\r\n        \"\"\"Execute grasp command\"\"\"\r\n        object_name = params[0]\r\n        \r\n        # Create manipulation goal\r\n        goal_msg = ManipulateObject.Goal()\r\n        goal_msg.object_name = object_name\r\n        goal_msg.manipulation_type = 'grasp'\r\n        \r\n        # Send manipulation goal\r\n        self.manip_client.wait_for_server()\r\n        future = self.manip_client.send_goal_async(goal_msg)\r\n        \r\n        # Wait for result\r\n        rclpy.spin_until_future_complete(self, future)\r\n        goal_handle = future.result()\r\n        \r\n        return goal_handle.status == GoalStatus.STATUS_SUCCEEDED\r\n    \r\n    def execute_place_command(self, params):\r\n        \"\"\"Execute place command\"\"\"\r\n        if len(params) < 2:\r\n            self.get_logger().error('Place command requires object and location')\r\n            return False\r\n        \r\n        object_name = params[0]\r\n        location = params[1]\r\n        \r\n        # Create manipulation goal\r\n        goal_msg = ManipulateObject.Goal()\r\n        goal_msg.object_name = object_name\r\n        goal_msg.manipulation_type = 'place'\r\n        goal_msg.placement_location = location\r\n        \r\n        # Send manipulation goal\r\n        self.manip_client.wait_for_server()\r\n        future = self.manip_client.send_goal_async(goal_msg)\r\n        \r\n        # Wait for result\r\n        rclpy.spin_until_future_complete(self, future)\r\n        goal_handle = future.result()\r\n        \r\n        return goal_handle.status == GoalStatus.STATUS_SUCCEEDED\r\n    \r\n    def execute_fetch_and_place_command(self, params):\r\n        \"\"\"Execute fetch and place command\"\"\"\r\n        if len(params) < 2:\r\n            self.get_logger().error('Fetch and place command requires object and location')\r\n            return False\r\n        \r\n        object_name = params[0]\r\n        location = params[1]\r\n        \r\n        # First, navigate to the object\r\n        # This is a simplified version - in practice, you'd need to know where the object is\r\n        # or have a perception system to find it\r\n        \r\n        # For this example, we'll assume the object is at a known location\r\n        # and we're already there\r\n        \r\n        # Then, grasp the object\r\n        grasp_success = self.execute_grasp_command((object_name,))\r\n        if not grasp_success:\r\n            return False\r\n        \r\n        # Finally, place the object\r\n        place_success = self.execute_place_command((object_name, location))\r\n        return place_success\r\n    \r\n    def publish_status(self, status):\r\n        \"\"\"Publish status message\"\"\"\r\n        status_msg = String()\r\n        status_msg.data = status\r\n        self.status_pub.publish(status_msg)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"4-llm-based-task-planning-system",children:"4. LLM-Based Task Planning System"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom .msg import TaskStep, TaskPlan\r\nimport openai\r\nimport json\r\nimport re\r\n\r\nclass LLMTaskPlanner(Node):\r\n    def __init__(self, api_key):\r\n        super().__init__(\'llm_task_planner\')\r\n        \r\n        # Initialize OpenAI client\r\n        openai.api_key = api_key\r\n        \r\n        # Subscribers\r\n        self.task_request_sub = self.create_subscription(\r\n            String, \'/task_request\', self.task_request_callback, 10\r\n        )\r\n        \r\n        # Publishers\r\n        self.task_plan_pub = self.create_publisher(TaskPlan, \'/task_plan\', 10)\r\n        self.status_pub = self.create_publisher(String, \'/llm_planner_status\', 10)\r\n        \r\n        # State\r\n        self.current_plan = None\r\n    \r\n    def task_request_callback(self, msg):\r\n        """Process task request from natural language"""\r\n        task_description = msg.data\r\n        \r\n        try:\r\n            # Plan the task using LLM\r\n            task_plan = self.plan_task_with_llm(task_description)\r\n            \r\n            # Publish the plan\r\n            self.task_plan_pub.publish(task_plan)\r\n            \r\n            self.publish_status(f\'Successfully planned task: {task_description}\')\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error planning task: {e}\')\r\n            self.publish_status(f\'Failed to plan task: {task_description}\')\r\n    \r\n    def plan_task_with_llm(self, task_description):\r\n        """Plan a task using LLM"""\r\n        # Create a prompt for the LLM\r\n        prompt = f"""\r\n        You are a task planner for a humanoid robot. The robot can perform the following actions:\r\n        - navigate_to: Move to a specific location\r\n        - grasp_object: Pick up an object\r\n        - place_object: Put down an object at a location\r\n        - detect_object: Look for a specific object\r\n        - open_container: Open a container\r\n        - close_container: Close a container\r\n        - wait: Wait for a specified time\r\n        - speak: Speak a message\r\n\r\n        Create a detailed step-by-step plan to complete the following task:\r\n        "{task_description}"\r\n\r\n        Each step should be a specific action with parameters. Return the plan as a JSON array of steps.\r\n        """\r\n        \r\n        # Call the LLM\r\n        response = openai.ChatCompletion.create(\r\n            model="gpt-3.5-turbo",\r\n            messages=[{"role": "user", "content": prompt}],\r\n            temperature=0.1,\r\n            functions=[\r\n                {\r\n                    "name": "create_task_plan",\r\n                    "description": "Create a step-by-step plan for a robot task",\r\n                    "parameters": {\r\n                        "type": "object",\r\n                        "properties": {\r\n                            "steps": {\r\n                                "type": "array",\r\n                                "items": {\r\n                                    "type": "object",\r\n                                    "properties": {\r\n                                        "action": {"type": "string", "description": "The action to perform"},\r\n                                        "parameters": {"type": "object", "description": "Parameters for the action"},\r\n                                        "description": {"type": "string", "description": "Description of the step"}\r\n                                    },\r\n                                    "required": ["action", "parameters", "description"]\r\n                                }\r\n                            }\r\n                        },\r\n                        "required": ["steps"]\r\n                    }\r\n                }\r\n            ],\r\n            function_call={"name": "create_task_plan"}\r\n        )\r\n        \r\n        # Extract the plan\r\n        plan_json = json.loads(response.choices[0].message.function_call.arguments)\r\n        steps = plan_json.get("steps", [])\r\n        \r\n        # Convert to TaskPlan message\r\n        task_plan = TaskPlan()\r\n        task_plan.header.stamp = self.get_clock().now().to_msg()\r\n        task_plan.header.frame_id = \'map\'\r\n        \r\n        for step_data in steps:\r\n            task_step = TaskStep()\r\n            task_step.action = step_data["action"]\r\n            task_step.parameters = json.dumps(step_data["parameters"])\r\n            task_step.description = step_data["description"]\r\n            task_plan.steps.append(task_step)\r\n        \r\n        return task_plan\r\n    \r\n    def publish_status(self, status):\r\n        """Publish status message"""\r\n        status_msg = String()\r\n        status_msg.data = status\r\n        self.status_pub.publish(status_msg)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"5-navigation-system-integration",children:"5. Navigation System Integration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom geometry_msgs.msg import PoseStamped, Point\r\nfrom nav_msgs.msg import Path, OccupancyGrid\r\nfrom sensor_msgs.msg import LaserScan\r\nfrom action_msgs.msg import GoalStatus\r\nfrom rclpy.action import ActionServer, GoalResponse, CancelResponse\r\nfrom .action import NavigateToPose\r\n\r\nclass NavigationSystem(Node):\r\n    def __init__(self):\r\n        super().__init__(\'navigation_system\')\r\n        \r\n        # Action server for navigation\r\n        self._action_server = ActionServer(\r\n            self,\r\n            NavigateToPose,\r\n            \'navigate_to_pose\',\r\n            execute_callback=self.execute_navigate_to_pose,\r\n            goal_callback=self.goal_callback,\r\n            cancel_callback=self.cancel_callback\r\n        )\r\n        \r\n        # Subscribers\r\n        self.map_sub = self.create_subscription(\r\n            OccupancyGrid, \'/map\', self.map_callback, 10\r\n        )\r\n        \r\n        self.laser_sub = self.create_subscription(\r\n            LaserScan, \'/scan\', self.laser_callback, 10\r\n        )\r\n        \r\n        self.odom_sub = self.create_subscription(\r\n            PoseStamped, \'/slam_pose\', self.odom_callback, 10\r\n        )\r\n        \r\n        # Publishers\r\n        self.path_pub = self.create_publisher(Path, \'/global_plan\', 10)\r\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n        \r\n        # Navigation components\r\n        self.global_planner = self.initialize_global_planner()\r\n        self.local_planner = self.initialize_local_planner()\r\n        \r\n        # State\r\n        self.current_map = None\r\n        self.current_pose = None\r\n        self.current_goal = None\r\n        self.is_navigating = False\r\n        \r\n        # Timer for navigation control\r\n        self.nav_timer = self.create_timer(0.1, self.navigation_control)\r\n    \r\n    def initialize_global_planner(self):\r\n        """Initialize global path planner"""\r\n        class MockGlobalPlanner:\r\n            def plan(self, start_pose, goal_pose, occupancy_grid):\r\n                # Simulate path planning\r\n                # In practice, this would use A*, Dijkstra, or other algorithms\r\n                path = Path()\r\n                path.header.frame_id = \'map\'\r\n                \r\n                # Create a simple straight-line path\r\n                for i in range(10):\r\n                    pose = PoseStamped()\r\n                    pose.header.frame_id = \'map\'\r\n                    pose.pose.position.x = start_pose.position.x + \\\r\n                        (goal_pose.position.x - start_pose.position.x) * i / 10\r\n                    pose.pose.position.y = start_pose.position.y + \\\r\n                        (goal_pose.position.y - start_pose.position.y) * i / 10\r\n                    pose.pose.orientation.w = 1.0\r\n                    path.poses.append(pose)\r\n                \r\n                return path\r\n        \r\n        return MockGlobalPlanner()\r\n    \r\n    def initialize_local_planner(self):\r\n        """Initialize local path planner"""\r\n        class MockLocalPlanner:\r\n            def plan(self, global_path, current_pose, laser_scan):\r\n                # Simulate local path planning with obstacle avoidance\r\n                # In practice, this would use DWA, TEB, or other local planners\r\n                twist = Twist()\r\n                \r\n                # Simple proportional controller for demonstration\r\n                if global_path.poses:\r\n                    target_pose = global_path.poses[0].pose  # First waypoint\r\n                    dx = target_pose.position.x - current_pose.position.x\r\n                    dy = target_pose.position.y - current_pose.position.y\r\n                    \r\n                    # Calculate desired angle\r\n                    desired_angle = math.atan2(dy, dx)\r\n                    \r\n                    # Calculate current angle (simplified)\r\n                    current_angle = 0  # Would get from orientation\r\n                    angle_error = desired_angle - current_angle\r\n                    \r\n                    # Proportional control\r\n                    twist.linear.x = min(0.5, max(0.1, math.sqrt(dx*dx + dy*dy) * 0.5))\r\n                    twist.angular.z = angle_error * 1.0\r\n                \r\n                return twist\r\n        \r\n        return MockLocalPlanner()\r\n    \r\n    def goal_callback(self, goal_request):\r\n        """Accept or reject navigation goal"""\r\n        self.get_logger().info(\'Received navigation goal\')\r\n        return GoalResponse.ACCEPT\r\n    \r\n    def cancel_callback(self, goal_handle):\r\n        """Accept or reject goal cancellation"""\r\n        self.get_logger().info(\'Received request to cancel navigation goal\')\r\n        return CancelResponse.ACCEPT\r\n    \r\n    async def execute_navigate_to_pose(self, goal_handle):\r\n        """Execute navigation to pose action"""\r\n        self.get_logger().info(\'Executing navigation to pose\')\r\n        \r\n        goal = goal_handle.request.pose\r\n        \r\n        # Set navigation goal\r\n        self.current_goal = goal\r\n        self.is_navigating = True\r\n        \r\n        # Plan global path\r\n        if self.current_map and self.current_pose:\r\n            global_path = self.global_planner.plan(\r\n                self.current_pose, \r\n                goal.pose, \r\n                self.current_map\r\n            )\r\n            \r\n            # Publish global path\r\n            self.path_pub.publish(global_path)\r\n            \r\n            # Navigate until goal is reached\r\n            while self.is_navigating and rclpy.ok():\r\n                # Local planning and control happens in navigation_control timer\r\n                # Check if we\'ve reached the goal\r\n                if self.has_reached_goal(goal.pose):\r\n                    break\r\n                \r\n                # Check for cancellation\r\n                if goal_handle.is_cancel_requested:\r\n                    goal_handle.canceled()\r\n                    self.is_navigating = False\r\n                    result = NavigateToPose.Result()\r\n                    result.success = False\r\n                    return result\r\n                \r\n                # Sleep briefly to allow other callbacks to run\r\n                await asyncio.sleep(0.1)\r\n        \r\n        # Mark goal as succeeded\r\n        goal_handle.succeed()\r\n        result = NavigateToPose.Result()\r\n        result.success = True\r\n        return result\r\n    \r\n    def has_reached_goal(self, goal_pose):\r\n        """Check if robot has reached the goal"""\r\n        if not self.current_pose:\r\n            return False\r\n        \r\n        # Calculate distance to goal\r\n        dx = goal_pose.position.x - self.current_pose.position.x\r\n        dy = goal_pose.position.y - self.current_pose.position.y\r\n        distance = math.sqrt(dx*dx + dy*dy)\r\n        \r\n        # Check if within tolerance\r\n        return distance < 0.2  # 20cm tolerance\r\n    \r\n    def map_callback(self, msg):\r\n        """Process map data"""\r\n        self.current_map = msg\r\n    \r\n    def laser_callback(self, msg):\r\n        """Process laser scan data"""\r\n        # Store for local planning\r\n        pass\r\n    \r\n    def odom_callback(self, msg):\r\n        """Process odometry data"""\r\n        self.current_pose = msg.pose\r\n    \r\n    def navigation_control(self):\r\n        """Navigation control loop"""\r\n        if not self.is_navigating or not self.current_pose or not self.current_goal:\r\n            return\r\n        \r\n        # Get global path if we don\'t have one\r\n        if not hasattr(self, \'current_global_path\') or not self.current_global_path.poses:\r\n            if self.current_map:\r\n                self.current_global_path = self.global_planner.plan(\r\n                    self.current_pose,\r\n                    self.current_goal.pose,\r\n                    self.current_map\r\n                )\r\n        \r\n        # Plan local trajectory\r\n        if hasattr(self, \'current_global_path\'):\r\n            cmd_vel = self.local_planner.plan(\r\n                self.current_global_path,\r\n                self.current_pose,\r\n                self.current_laser_scan  # Would be stored from laser_callback\r\n            )\r\n            \r\n            # Publish velocity command\r\n            self.cmd_vel_pub.publish(cmd_vel)\r\n        \r\n        # Check if goal is reached\r\n        if self.has_reached_goal(self.current_goal.pose):\r\n            self.is_navigating = False\n'})}),"\n",(0,t.jsx)(n.h3,{id:"6-system-integration-node",children:"6. System Integration Node"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist\r\n\r\nclass SystemIntegrator(Node):\r\n    def __init__(self):\r\n        super().__init__('system_integrator')\r\n        \r\n        # Publishers for system-wide commands\r\n        self.system_status_pub = self.create_publisher(String, '/system_status', 10)\r\n        self.emergency_stop_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n        \r\n        # Subscribers for system status\r\n        self.voice_status_sub = self.create_subscription(\r\n            String, '/voice_action_status', self.voice_status_callback, 10\r\n        )\r\n        \r\n        self.nav_status_sub = self.create_subscription(\r\n            String, '/navigation_status', self.nav_status_callback, 10\r\n        )\r\n        \r\n        self.slam_status_sub = self.create_subscription(\r\n            String, '/slam_status', self.slam_status_callback, 10\r\n        )\r\n        \r\n        # System state\r\n        self.system_state = {\r\n            'voice_system': 'idle',\r\n            'navigation_system': 'idle',\r\n            'slam_system': 'active',\r\n            'perception_system': 'active'\r\n        }\r\n        \r\n        # Timer for system monitoring\r\n        self.monitor_timer = self.create_timer(1.0, self.system_monitor)\r\n        \r\n        # Timer for safety checks\r\n        self.safety_timer = self.create_timer(0.1, self.safety_check)\r\n    \r\n    def voice_status_callback(self, msg):\r\n        \"\"\"Update voice system status\"\"\"\r\n        self.system_state['voice_system'] = msg.data\r\n    \r\n    def nav_status_callback(self, msg):\r\n        \"\"\"Update navigation system status\"\"\"\r\n        self.system_state['navigation_system'] = msg.data\r\n    \r\n    def slam_status_callback(self, msg):\r\n        \"\"\"Update SLAM system status\"\"\"\r\n        self.system_state['slam_system'] = msg.data\r\n    \r\n    def system_monitor(self):\r\n        \"\"\"Monitor system health\"\"\"\r\n        status_msg = String()\r\n        status_msg.data = f\"System Status: {self.system_state}\"\r\n        self.system_status_pub.publish(status_msg)\r\n    \r\n    def safety_check(self):\r\n        \"\"\"Perform safety checks\"\"\"\r\n        # Check for system errors that require emergency stop\r\n        if any('error' in status.lower() for status in self.system_state.values()):\r\n            # Emergency stop\r\n            stop_cmd = Twist()\r\n            self.emergency_stop_pub.publish(stop_cmd)\r\n            self.get_logger().error('Emergency stop activated due to system error')\n"})}),"\n",(0,t.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,t.jsx)(n.h3,{id:"1-unit-testing",children:"1. Unit Testing"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import unittest\r\nimport rclpy\r\nfrom rclpy.executors import SingleThreadedExecutor\r\nfrom your_robot_package.perception_system import PerceptionSystem\r\nfrom your_robot_package.slam_system import SLAMSystem\r\nfrom your_robot_package.voice_to_action import VoiceToActionSystem\r\n\r\nclass TestPerceptionSystem(unittest.TestCase):\r\n    def setUp(self):\r\n        rclpy.init()\r\n        self.node = PerceptionSystem()\r\n        self.executor = SingleThreadedExecutor()\r\n        self.executor.add_node(self.node)\r\n    \r\n    def tearDown(self):\r\n        self.node.destroy_node()\r\n        rclpy.shutdown()\r\n    \r\n    def test_object_detection(self):\r\n        """Test object detection functionality"""\r\n        # Publish test image\r\n        # Check if detections are published\r\n        self.assertTrue(True)  # Placeholder\r\n    \r\n    def test_human_detection(self):\r\n        """Test human detection functionality"""\r\n        # Publish test image\r\n        # Check if human detections are published\r\n        self.assertTrue(True)  # Placeholder\r\n\r\nclass TestSLAMSystem(unittest.TestCase):\r\n    def setUp(self):\r\n        rclpy.init()\r\n        self.node = SLAMSystem()\r\n        self.executor = SingleThreadedExecutor()\r\n        self.executor.add_node(self.node)\r\n    \r\n    def tearDown(self):\r\n        self.node.destroy_node()\r\n        rclpy.shutdown()\r\n    \r\n    def test_map_generation(self):\r\n        """Test map generation"""\r\n        # Simulate sensor data\r\n        # Check if map is generated\r\n        self.assertTrue(True)  # Placeholder\r\n\r\nclass TestVoiceToActionSystem(unittest.TestCase):\r\n    def setUp(self):\r\n        rclpy.init()\r\n        self.node = VoiceToActionSystem()\r\n        self.executor = SingleThreadedExecutor()\r\n        self.executor.add_node(self.node)\r\n    \r\n    def tearDown(self):\r\n        self.node.destroy_node()\r\n        rclpy.shutdown()\r\n    \r\n    def test_command_parsing(self):\r\n        """Test command parsing"""\r\n        # Test various commands\r\n        self.assertTrue(True)  # Placeholder\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-integration-testing",children:"2. Integration Testing"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import unittest\r\nimport rclpy\r\nfrom rclpy.executors import SingleThreadedExecutor\r\nfrom your_robot_package.system_integrator import SystemIntegrator\r\n\r\nclass TestSystemIntegration(unittest.TestCase):\r\n    def setUp(self):\r\n        rclpy.init()\r\n        self.system_integrator = SystemIntegrator()\r\n        self.executor = SingleThreadedExecutor()\r\n        self.executor.add_node(self.system_integrator)\r\n    \r\n    def tearDown(self):\r\n        self.system_integrator.destroy_node()\r\n        rclpy.shutdown()\r\n    \r\n    def test_complete_task_execution(self):\r\n        """Test complete task from voice command to execution"""\r\n        # Simulate voice command\r\n        # Verify all systems respond appropriately\r\n        # Check if task is completed successfully\r\n        self.assertTrue(True)  # Placeholder\r\n    \r\n    def test_error_recovery(self):\r\n        """Test system behavior when errors occur"""\r\n        # Simulate error in one subsystem\r\n        # Verify other subsystems respond appropriately\r\n        # Check if system recovers gracefully\r\n        self.assertTrue(True)  # Placeholder\n'})}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"1-system-optimization",children:"1. System Optimization"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import psutil\r\nimport time\r\nfrom threading import Thread\r\n\r\nclass SystemOptimizer:\r\n    def __init__(self, robot_nodes):\r\n        self.nodes = robot_nodes\r\n        self.monitoring_thread = Thread(target=self.monitor_resources)\r\n        self.monitoring_thread.daemon = True\r\n        self.monitoring_thread.start()\r\n    \r\n    def monitor_resources(self):\r\n        """Monitor system resources and adjust as needed"""\r\n        while True:\r\n            # Check CPU usage\r\n            cpu_percent = psutil.cpu_percent(interval=1)\r\n            \r\n            # Check memory usage\r\n            memory_percent = psutil.virtual_memory().percent\r\n            \r\n            # Check if resources are running low\r\n            if cpu_percent > 80 or memory_percent > 80:\r\n                self.throttle_non_critical_processes()\r\n            else:\r\n                self.restore_normal_operation()\r\n            \r\n            time.sleep(5)  # Check every 5 seconds\r\n    \r\n    def throttle_non_critical_processes(self):\r\n        """Reduce processing rate for non-critical processes"""\r\n        # Reduce perception processing rate\r\n        # Reduce planning complexity\r\n        # Prioritize safety and navigation\r\n        pass\r\n    \r\n    def restore_normal_operation(self):\r\n        """Restore normal processing rates"""\r\n        # Restore perception processing rate\r\n        # Restore planning complexity\r\n        pass\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-real-time-performance",children:"2. Real-time Performance"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import time\r\nimport threading\r\nfrom collections import deque\r\n\r\nclass RealTimeScheduler:\r\n    def __init__(self):\r\n        self.tasks = {\r\n            'safety': {'priority': 1, 'period': 0.01, 'last_run': 0},  # 100Hz\r\n            'balance': {'priority': 2, 'period': 0.02, 'last_run': 0},  # 50Hz\r\n            'navigation': {'priority': 3, 'period': 0.1, 'last_run': 0},  # 10Hz\r\n            'perception': {'priority': 4, 'period': 0.2, 'last_run': 0},  # 5Hz\r\n            'planning': {'priority': 5, 'period': 1.0, 'last_run': 0}   # 1Hz\r\n        }\r\n        \r\n        self.task_queue = deque()\r\n        self.scheduler_thread = threading.Thread(target=self.scheduler_loop)\r\n        self.scheduler_thread.daemon = True\r\n        self.scheduler_thread.start()\r\n    \r\n    def scheduler_loop(self):\r\n        \"\"\"Real-time scheduler loop\"\"\"\r\n        while True:\r\n            current_time = time.time()\r\n            \r\n            # Check which tasks are due\r\n            for task_name, task_info in self.tasks.items():\r\n                if current_time - task_info['last_run'] >= task_info['period']:\r\n                    self.task_queue.append((task_info['priority'], task_name))\r\n                    task_info['last_run'] = current_time\r\n            \r\n            # Execute tasks in priority order\r\n            sorted_tasks = sorted(self.task_queue, key=lambda x: x[0])\r\n            for priority, task_name in sorted_tasks:\r\n                self.execute_task(task_name)\r\n            \r\n            # Clear the queue\r\n            self.task_queue.clear()\r\n            \r\n            # Sleep briefly to prevent busy waiting\r\n            time.sleep(0.001)\r\n    \r\n    def execute_task(self, task_name):\r\n        \"\"\"Execute a specific task\"\"\"\r\n        # This would call the appropriate robot system\r\n        pass\n"})}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(n.p,{children:"The capstone project demonstrates the integration of multiple complex systems to create an intelligent humanoid robot. Key achievements include:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception Integration"}),": Combining multiple sensor modalities for environmental understanding"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"SLAM Implementation"}),": Creating and maintaining maps of the environment"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural Interaction"}),": Enabling natural language communication with users"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Intelligent Planning"}),": Using LLMs for complex task planning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safe Navigation"}),": Implementing robust navigation in human environments"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System Integration"}),": Coordinating all subsystems for coherent behavior"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This project showcases the potential of modern robotics technology to create capable, intelligent humanoid robots that can assist humans in various tasks while operating safely in human environments."})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},8453(e,n,r){r.d(n,{R:()=>o,x:()=>i});var s=r(6540);const t={},a=s.createContext(t);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);