"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[612],{3502(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"module4/llm-planning","title":"LLM-Based Task Planning for Humanoid Robots","description":"Large Language Models (LLMs) have revolutionized the field of artificial intelligence, offering unprecedented capabilities in natural language understanding and reasoning. When applied to humanoid robotics, LLMs can significantly enhance task planning capabilities, enabling robots to interpret complex instructions, reason about their environment, and generate sophisticated action sequences.","source":"@site/docs/module4/llm-planning.md","sourceDirName":"module4","slug":"/module4/llm-planning","permalink":"/docs/module4/llm-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/SyedZohaibTech/physical-ai-book/edit/main/docs/module4/llm-planning.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"LLM-Based Task Planning for Humanoid Robots"},"sidebar":"tutorialSidebar","previous":{"title":"Voice-to-Action Systems for Humanoid Robots","permalink":"/docs/module4/voice-to-action"},"next":{"title":"Capstone Project - Complete Humanoid Robot System","permalink":"/docs/module4/capstone-project"}}');var a=t(4848),s=t(8453);const o={sidebar_position:3,title:"LLM-Based Task Planning for Humanoid Robots"},i="LLM-Based Task Planning for Humanoid Robots",l={},c=[{value:"Introduction to LLMs in Robotics",id:"introduction-to-llms-in-robotics",level:2},{value:"What are Large Language Models?",id:"what-are-large-language-models",level:3},{value:"LLMs in Robotics Context",id:"llms-in-robotics-context",level:3},{value:"Architecture for LLM-Based Planning",id:"architecture-for-llm-based-planning",level:2},{value:"1. Integration Architecture",id:"1-integration-architecture",level:3},{value:"2. Planning Pipeline",id:"2-planning-pipeline",level:3},{value:"Implementing LLM-Based Task Planning",id:"implementing-llm-based-task-planning",level:2},{value:"1. Environment Perception Interface",id:"1-environment-perception-interface",level:3},{value:"2. LLM Planner Implementation",id:"2-llm-planner-implementation",level:3},{value:"3. Knowledge Base Integration",id:"3-knowledge-base-integration",level:3},{value:"4. Advanced Planning with Context",id:"4-advanced-planning-with-context",level:3},{value:"Handling Complex Tasks",id:"handling-complex-tasks",level:2},{value:"1. Multi-Step Task Planning",id:"1-multi-step-task-planning",level:3},{value:"2. Handling Ambiguity",id:"2-handling-ambiguity",level:3},{value:"Integration with Robot Control Systems",id:"integration-with-robot-control-systems",level:2},{value:"1. Action Execution Interface",id:"1-action-execution-interface",level:3},{value:"2. High-Level Task Manager",id:"2-high-level-task-manager",level:3},{value:"Performance Optimization and Safety",id:"performance-optimization-and-safety",level:2},{value:"1. Plan Validation",id:"1-plan-validation",level:3},{value:"2. Error Recovery",id:"2-error-recovery",level:3}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"llm-based-task-planning-for-humanoid-robots",children:"LLM-Based Task Planning for Humanoid Robots"})}),"\n",(0,a.jsx)(n.p,{children:"Large Language Models (LLMs) have revolutionized the field of artificial intelligence, offering unprecedented capabilities in natural language understanding and reasoning. When applied to humanoid robotics, LLMs can significantly enhance task planning capabilities, enabling robots to interpret complex instructions, reason about their environment, and generate sophisticated action sequences."}),"\n",(0,a.jsx)(n.h2,{id:"introduction-to-llms-in-robotics",children:"Introduction to LLMs in Robotics"}),"\n",(0,a.jsx)(n.h3,{id:"what-are-large-language-models",children:"What are Large Language Models?"}),"\n",(0,a.jsx)(n.p,{children:"Large Language Models are neural networks trained on vast amounts of text data to understand and generate human-like language. These models, such as GPT, Claude, and others, demonstrate remarkable abilities in:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Natural language understanding"}),"\n",(0,a.jsx)(n.li,{children:"Logical reasoning"}),"\n",(0,a.jsx)(n.li,{children:"Knowledge retrieval"}),"\n",(0,a.jsx)(n.li,{children:"Instruction following"}),"\n",(0,a.jsx)(n.li,{children:"Contextual adaptation"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"llms-in-robotics-context",children:"LLMs in Robotics Context"}),"\n",(0,a.jsx)(n.p,{children:"In robotics, LLMs serve as high-level cognitive processors that can:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Interpret complex natural language commands"}),"\n",(0,a.jsx)(n.li,{children:"Plan multi-step tasks"}),"\n",(0,a.jsx)(n.li,{children:"Reason about affordances and constraints"}),"\n",(0,a.jsx)(n.li,{children:"Generate executable action sequences"}),"\n",(0,a.jsx)(n.li,{children:"Handle ambiguous or incomplete instructions"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"architecture-for-llm-based-planning",children:"Architecture for LLM-Based Planning"}),"\n",(0,a.jsx)(n.h3,{id:"1-integration-architecture",children:"1. Integration Architecture"}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TD\r\n    A[Natural Language Command] --\x3e B[LLM Planner]\r\n    B --\x3e C[Environment Perception]\r\n    C --\x3e D[Knowledge Base]\r\n    B --\x3e E[Task Decomposition]\r\n    E --\x3e F[Action Sequencing]\r\n    F --\x3e G[Robot Controller]\r\n    D --\x3e B\r\n    G --\x3e H[Execution Feedback]\r\n    H --\x3e B"}),"\n",(0,a.jsx)(n.h3,{id:"2-planning-pipeline",children:"2. Planning Pipeline"}),"\n",(0,a.jsx)(n.p,{children:"The LLM-based planning pipeline consists of several key components:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Input Processing"}),": Interpreting natural language commands"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"World Modeling"}),": Creating a representation of the environment"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Task Decomposition"}),": Breaking down complex tasks into subtasks"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Action Sequencing"}),": Generating executable action sequences"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Execution Monitoring"}),": Tracking execution and adapting to changes"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"implementing-llm-based-task-planning",children:"Implementing LLM-Based Task Planning"}),"\n",(0,a.jsx)(n.h3,{id:"1-environment-perception-interface",children:"1. Environment Perception Interface"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import json\r\nfrom dataclasses import dataclass\r\nfrom typing import List, Dict, Any, Optional\r\nfrom enum import Enum\r\n\r\nclass ObjectType(Enum):\r\n    FURNITURE = "furniture"\r\n    MANIPULABLE = "manipulable"\r\n    SURFACE = "surface"\r\n    PERSON = "person"\r\n\r\n@dataclass\r\nclass ObjectInfo:\r\n    name: str\r\n    object_type: ObjectType\r\n    position: Dict[str, float]\r\n    properties: Dict[str, Any]\r\n\r\n@dataclass\r\nclass EnvironmentState:\r\n    objects: List[ObjectInfo]\r\n    robot_position: Dict[str, float]\r\n    robot_state: Dict[str, Any]\r\n\r\nclass EnvironmentPerceptor:\r\n    def __init__(self):\r\n        self.current_state = EnvironmentState(\r\n            objects=[],\r\n            robot_position={"x": 0.0, "y": 0.0, "z": 0.0},\r\n            robot_state={}\r\n        )\r\n    \r\n    def get_current_state(self) -> EnvironmentState:\r\n        """Get current environment state from perception systems"""\r\n        # This would interface with actual perception systems\r\n        # For simulation, return mock data\r\n        return self.current_state\r\n    \r\n    def update_state(self, new_objects: List[ObjectInfo]):\r\n        """Update environment state with new object information"""\r\n        self.current_state.objects = new_objects\r\n    \r\n    def find_object_by_name(self, name: str) -> Optional[ObjectInfo]:\r\n        """Find an object by its name"""\r\n        for obj in self.current_state.objects:\r\n            if obj.name.lower() == name.lower():\r\n                return obj\r\n        return None\r\n    \r\n    def find_objects_by_type(self, obj_type: ObjectType) -> List[ObjectInfo]:\r\n        """Find all objects of a specific type"""\r\n        return [obj for obj in self.current_state.objects if obj.object_type == obj_type]\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-llm-planner-implementation",children:"2. LLM Planner Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import openai\r\nimport json\r\nimport re\r\nfrom typing import List, Dict, Any\r\nfrom dataclasses import dataclass\r\n\r\n@dataclass\r\nclass TaskStep:\r\n    action: str\r\n    parameters: Dict[str, Any]\r\n    description: str\r\n\r\nclass LLMTaskPlanner:\r\n    def __init__(self, api_key: str, model: str = "gpt-3.5-turbo"):\r\n        openai.api_key = api_key\r\n        self.model = model\r\n        self.environment_perceptor = EnvironmentPerceptor()\r\n    \r\n    def plan_task(self, natural_language_goal: str, environment_state: EnvironmentState) -> List[TaskStep]:\r\n        """Plan a task based on natural language goal and environment state"""\r\n        \r\n        # Create a prompt for the LLM\r\n        prompt = self._create_planning_prompt(natural_language_goal, environment_state)\r\n        \r\n        # Call the LLM\r\n        response = openai.ChatCompletion.create(\r\n            model=self.model,\r\n            messages=[\r\n                {"role": "system", "content": self._get_system_prompt()},\r\n                {"role": "user", "content": prompt}\r\n            ],\r\n            temperature=0.1,  # Low temperature for consistency\r\n            functions=[\r\n                {\r\n                    "name": "create_task_plan",\r\n                    "description": "Create a plan for executing a robot task",\r\n                    "parameters": {\r\n                        "type": "object",\r\n                        "properties": {\r\n                            "steps": {\r\n                                "type": "array",\r\n                                "items": {\r\n                                    "type": "object",\r\n                                    "properties": {\r\n                                        "action": {"type": "string", "description": "The action to perform"},\r\n                                        "parameters": {"type": "object", "description": "Parameters for the action"},\r\n                                        "description": {"type": "string", "description": "Description of the step"}\r\n                                    },\r\n                                    "required": ["action", "parameters", "description"]\r\n                                }\r\n                            }\r\n                        },\r\n                        "required": ["steps"]\r\n                    }\r\n                }\r\n            ],\r\n            function_call={"name": "create_task_plan"}\r\n        )\r\n        \r\n        # Extract the plan from the response\r\n        plan_json = json.loads(response.choices[0].message.function_call.arguments)\r\n        steps = plan_json.get("steps", [])\r\n        \r\n        # Convert to TaskStep objects\r\n        task_steps = []\r\n        for step_data in steps:\r\n            task_step = TaskStep(\r\n                action=step_data["action"],\r\n                parameters=step_data["parameters"],\r\n                description=step_data["description"]\r\n            )\r\n            task_steps.append(task_step)\r\n        \r\n        return task_steps\r\n    \r\n    def _create_planning_prompt(self, goal: str, env_state: EnvironmentState) -> str:\r\n        """Create a prompt for the LLM with environment context"""\r\n        objects_list = []\r\n        for obj in env_state.objects:\r\n            obj_desc = f"- {obj.name} ({obj.object_type.value}) at position {obj.position}"\r\n            if obj.properties:\r\n                obj_desc += f" with properties: {obj.properties}"\r\n            objects_list.append(obj_desc)\r\n        \r\n        prompt = f"""\r\nGoal: {goal}\r\n\r\nCurrent Environment:\r\nRobot Position: {env_state.robot_position}\r\nAvailable Objects:\r\n{chr(10).join(objects_list)}\r\n\r\nPlease create a step-by-step plan to achieve this goal. Consider the available objects and their properties.\r\n"""\r\n        return prompt\r\n    \r\n    def _get_system_prompt(self) -> str:\r\n        """Get the system prompt for the LLM"""\r\n        return """\r\nYou are a helpful assistant that creates detailed task plans for a humanoid robot. \r\nThe robot can perform actions like:\r\n- navigate_to: Move to a specific location\r\n- grasp_object: Pick up an object\r\n- place_object: Put down an object at a location\r\n- open_container: Open a container\r\n- close_container: Close a container\r\n- detect_object: Look for a specific object\r\n- follow_person: Follow a person\r\n- wait: Wait for a specified time\r\n\r\nCreate a sequence of these actions to achieve the user\'s goal. \r\nConsider the robot\'s current environment and available objects.\r\nEach step should be specific and executable.\r\n"""\n'})}),"\n",(0,a.jsx)(n.h3,{id:"3-knowledge-base-integration",children:"3. Knowledge Base Integration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import json\r\nfrom typing import Dict, List, Any\r\n\r\nclass KnowledgeBase:\r\n    def __init__(self, knowledge_file: str = None):\r\n        if knowledge_file:\r\n            with open(knowledge_file, \'r\') as f:\r\n                self.knowledge = json.load(f)\r\n        else:\r\n            # Default knowledge for humanoid robots\r\n            self.knowledge = {\r\n                "object_affordances": {\r\n                    "cup": ["grasp", "carry", "place"],\r\n                    "book": ["grasp", "carry", "place", "open"],\r\n                    "box": ["grasp", "carry", "place", "open", "close"],\r\n                    "door": ["open", "close", "navigate_through"],\r\n                    "chair": ["navigate_to", "move_around"],\r\n                    "table": ["navigate_to", "place_on"],\r\n                    "kitchen_counter": ["navigate_to", "place_on"]\r\n                },\r\n                "locations": {\r\n                    "kitchen": {\r\n                        "objects": ["cup", "plate", "fridge", "kitchen_counter"],\r\n                        "actions": ["navigate_to", "grasp_object", "place_object"]\r\n                    },\r\n                    "living_room": {\r\n                        "objects": ["sofa", "coffee_table", "tv"],\r\n                        "actions": ["navigate_to", "grasp_object", "place_object"]\r\n                    },\r\n                    "bedroom": {\r\n                        "objects": ["bed", "nightstand", "wardrobe"],\r\n                        "actions": ["navigate_to", "grasp_object", "place_object"]\r\n                    }\r\n                },\r\n                "task_templates": {\r\n                    "fetch_object": [\r\n                        {"action": "navigate_to", "params": {"location": "{location}"}},\r\n                        {"action": "detect_object", "params": {"object": "{object}"}},\r\n                        {"action": "grasp_object", "params": {"object": "{object}"}},\r\n                        {"action": "navigate_to", "params": {"location": "{destination}"}},\r\n                        {"action": "place_object", "params": {"object": "{object}", "location": "{destination}"}}\r\n                    ]\r\n                }\r\n            }\r\n    \r\n    def get_affordances(self, object_type: str) -> List[str]:\r\n        """Get possible actions for an object type"""\r\n        return self.knowledge["object_affordances"].get(object_type, [])\r\n    \r\n    def get_location_info(self, location: str) -> Dict[str, Any]:\r\n        """Get information about a location"""\r\n        return self.knowledge["locations"].get(location, {})\r\n    \r\n    def get_task_template(self, task_type: str) -> List[Dict[str, Any]]:\r\n        """Get a template for a specific task type"""\r\n        return self.knowledge["task_templates"].get(task_type, [])\r\n    \r\n    def update_knowledge(self, new_knowledge: Dict[str, Any]):\r\n        """Update the knowledge base with new information"""\r\n        self.knowledge.update(new_knowledge)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"4-advanced-planning-with-context",children:"4. Advanced Planning with Context"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class ContextAwarePlanner:\r\n    def __init__(self):\r\n        self.knowledge_base = KnowledgeBase()\r\n        self.environment_perceptor = EnvironmentPerceptor()\r\n        self.llm_planner = LLMTaskPlanner(api_key="your-api-key")\r\n        \r\n        # Context tracking\r\n        self.context_history = []\r\n        self.current_task = None\r\n        self.task_history = []\r\n    \r\n    def plan_with_context(self, goal: str, user_context: Dict[str, Any] = None) -> List[TaskStep]:\r\n        """Plan a task considering context and history"""\r\n        # Get current environment state\r\n        env_state = self.environment_perceptor.get_current_state()\r\n        \r\n        # Augment the goal with context\r\n        contextualized_goal = self._augment_goal_with_context(goal, user_context, env_state)\r\n        \r\n        # Plan the task\r\n        task_steps = self.llm_planner.plan_task(contextualized_goal, env_state)\r\n        \r\n        # Update context\r\n        self.context_history.append({\r\n            "goal": goal,\r\n            "context": user_context,\r\n            "environment": env_state,\r\n            "plan": task_steps\r\n        })\r\n        \r\n        return task_steps\r\n    \r\n    def _augment_goal_with_context(self, goal: str, user_context: Dict[str, Any], env_state: EnvironmentState) -> str:\r\n        """Augment the goal with relevant context information"""\r\n        context_parts = []\r\n        \r\n        # Add user preferences if available\r\n        if user_context:\r\n            if user_context.get("preferred_hand", "right") == "left":\r\n                context_parts.append("The user prefers using the left hand for manipulation.")\r\n            \r\n            if "allergies" in user_context:\r\n                context_parts.append(f"The user has allergies to: {\', \'.join(user_context[\'allergies\'])}")\r\n        \r\n        # Add temporal context\r\n        import datetime\r\n        current_time = datetime.datetime.now()\r\n        time_context = f"Current time is {current_time.strftime(\'%H:%M\')}. "\r\n        if 6 <= current_time.hour < 12:\r\n            time_context += "It\'s morning, so be mindful of quiet activities."\r\n        elif 12 <= current_time.hour < 18:\r\n            time_context += "It\'s daytime, normal activity level is acceptable."\r\n        elif 18 <= current_time.hour < 22:\r\n            time_context += "It\'s evening, consider dimming lights if appropriate."\r\n        else:\r\n            time_context += "It\'s nighttime, be extra careful with noise."\r\n        \r\n        context_parts.append(time_context)\r\n        \r\n        # Add environment context\r\n        if env_state.robot_state.get("battery_level", 100) < 20:\r\n            context_parts.append("Robot battery is low, prioritize efficient paths.")\r\n        \r\n        # Combine goal with context\r\n        full_context = " ".join(context_parts)\r\n        if full_context:\r\n            return f"{full_context} Goal: {goal}"\r\n        else:\r\n            return goal\r\n    \r\n    def adapt_plan_to_feedback(self, original_plan: List[TaskStep], feedback: Dict[str, Any]) -> List[TaskStep]:\r\n        """Adapt the plan based on execution feedback"""\r\n        # Handle different types of feedback\r\n        if feedback.get("action_failed"):\r\n            failed_step_idx = feedback.get("failed_step_index", 0)\r\n            failure_reason = feedback.get("failure_reason", "unknown")\r\n            \r\n            # Modify the plan to handle the failure\r\n            new_plan = original_plan.copy()\r\n            \r\n            if failure_reason == "object_not_found":\r\n                # Add a detection step before the failed grasp\r\n                detect_step = TaskStep(\r\n                    action="detect_object",\r\n                    parameters={"object": original_plan[failed_step_idx].parameters.get("object")},\r\n                    description="Detect the object before grasping"\r\n                )\r\n                new_plan.insert(failed_step_idx, detect_step)\r\n            \r\n            elif failure_reason == "path_blocked":\r\n                # Plan a new path\r\n                navigate_step = TaskStep(\r\n                    action="navigate_to",\r\n                    parameters=original_plan[failed_step_idx].parameters,\r\n                    description="Navigate with obstacle avoidance"\r\n                )\r\n                new_plan[failed_step_idx] = navigate_step\r\n            \r\n            return new_plan\r\n        \r\n        return original_plan\n'})}),"\n",(0,a.jsx)(n.h2,{id:"handling-complex-tasks",children:"Handling Complex Tasks"}),"\n",(0,a.jsx)(n.h3,{id:"1-multi-step-task-planning",children:"1. Multi-Step Task Planning"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class MultiStepTaskPlanner:\r\n    def __init__(self):\r\n        self.context_aware_planner = ContextAwarePlanner()\r\n        self.knowledge_base = KnowledgeBase()\r\n    \r\n    def plan_complex_task(self, high_level_goal: str) -> List[TaskStep]:\r\n        """Plan a complex task by decomposing into subtasks"""\r\n        # Decompose the high-level goal\r\n        subtasks = self._decompose_task(high_level_goal)\r\n        \r\n        # Plan each subtask\r\n        all_steps = []\r\n        for subtask in subtasks:\r\n            steps = self.context_aware_planner.plan_with_context(subtask)\r\n            all_steps.extend(steps)\r\n        \r\n        return all_steps\r\n    \r\n    def _decompose_task(self, goal: str) -> List[str]:\r\n        """Decompose a complex goal into subtasks"""\r\n        # This would typically use an LLM to decompose tasks\r\n        # For demonstration, we\'ll use a simple rule-based approach\r\n        \r\n        if "tidy up the living room" in goal.lower():\r\n            return [\r\n                "Find all objects not in their designated places in the living room",\r\n                "Pick up the book and place it on the bookshelf",\r\n                "Pick up the cup and place it in the kitchen",\r\n                "Fluff the couch pillows"\r\n            ]\r\n        elif "prepare coffee" in goal.lower():\r\n            return [\r\n                "Navigate to the kitchen",\r\n                "Find the coffee maker",\r\n                "Find a cup",\r\n                "Grasp the cup",\r\n                "Place the cup under the coffee maker",\r\n                "Operate the coffee maker",\r\n                "Wait for coffee to brew",\r\n                "Grasp the coffee cup",\r\n                "Navigate to the user"\r\n            ]\r\n        elif "set the table for dinner" in goal.lower():\r\n            return [\r\n                "Navigate to the kitchen",\r\n                "Find plates for four people",\r\n                "Find forks for four people", \r\n                "Find knives for four people",\r\n                "Navigate to the dining table",\r\n                "Place plates on the table",\r\n                "Place forks on the table",\r\n                "Place knives on the table"\r\n            ]\r\n        else:\r\n            # For unknown complex tasks, return the original goal\r\n            return [goal]\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-handling-ambiguity",children:"2. Handling Ambiguity"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class AmbiguityResolver:\r\n    def __init__(self):\r\n        self.knowledge_base = KnowledgeBase()\r\n    \r\n    def resolve_ambiguity(self, command: str, env_state: EnvironmentState) -> str:\r\n        """Resolve ambiguous commands based on context"""\r\n        # Check for ambiguous references\r\n        if "it" in command.lower() or "that" in command.lower() or "there" in command.lower():\r\n            # Try to resolve based on recent context\r\n            resolved_command = self._resolve_pronouns_and_deixis(command, env_state)\r\n            return resolved_command\r\n        \r\n        # Check for underspecified goals\r\n        if self._is_underspecified(command):\r\n            # Add default specifications\r\n            return self._add_defaults(command, env_state)\r\n        \r\n        return command\r\n    \r\n    def _resolve_pronouns_and_deixis(self, command: str, env_state: EnvironmentState) -> str:\r\n        """Resolve pronouns and spatial references"""\r\n        # Simple resolution - in practice, this would be more sophisticated\r\n        resolved = command.lower()\r\n        \r\n        # Replace "it" with the most recently mentioned object\r\n        # This is a simplified approach\r\n        if "it" in resolved:\r\n            # Find the most recently mentioned object in the environment\r\n            if env_state.objects:\r\n                last_object = env_state.objects[-1].name\r\n                resolved = resolved.replace("it", f"the {last_object}")\r\n        \r\n        # Replace "there" with a specific location\r\n        if "there" in resolved:\r\n            # For simplicity, replace with the nearest surface\r\n            surfaces = [obj for obj in env_state.objects if obj.object_type == ObjectType.SURFACE]\r\n            if surfaces:\r\n                nearest_surface = surfaces[0].name\r\n                resolved = resolved.replace("there", f"on the {nearest_surface}")\r\n        \r\n        return resolved\r\n    \r\n    def _is_underspecified(self, command: str) -> bool:\r\n        """Check if a command is underspecified"""\r\n        underspecified_indicators = [\r\n            "somewhere", "anywhere", "a", "some", \r\n            "thing", "object", "place", "location"\r\n        ]\r\n        \r\n        command_lower = command.lower()\r\n        for indicator in underspecified_indicators:\r\n            if indicator in command_lower:\r\n                return True\r\n        \r\n        return False\r\n    \r\n    def _add_defaults(self, command: str, env_state: EnvironmentState) -> str:\r\n        """Add default specifications to underspecified commands"""\r\n        # Add defaults based on environment\r\n        if "place" in command.lower() and "table" not in command.lower():\r\n            # Default to the nearest table\r\n            tables = [obj for obj in env_state.objects if "table" in obj.name.lower()]\r\n            if tables:\r\n                default_table = tables[0].name\r\n                return command.replace("place", f"place on the {default_table}")\r\n        \r\n        return command\n'})}),"\n",(0,a.jsx)(n.h2,{id:"integration-with-robot-control-systems",children:"Integration with Robot Control Systems"}),"\n",(0,a.jsx)(n.h3,{id:"1-action-execution-interface",children:"1. Action Execution Interface"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom action_msgs.msg import GoalStatus\r\nfrom rclpy.action import ActionClient\r\nfrom std_msgs.msg import String\r\n\r\nclass ActionExecutor(Node):\r\n    def __init__(self):\r\n        super().__init__(\'action_executor\')\r\n        \r\n        # Publishers and subscribers\r\n        self.status_pub = self.create_publisher(String, \'/action_status\', 10)\r\n        \r\n        # Action clients for robot capabilities\r\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\r\n        self.manip_client = ActionClient(self, ManipulateObject, \'manipulate_object\')\r\n        self.detect_client = ActionClient(self, DetectObject, \'detect_object\')\r\n        \r\n        # State tracking\r\n        self.current_action = None\r\n        self.action_queue = []\r\n    \r\n    def execute_task_steps(self, task_steps: List[TaskStep]) -> bool:\r\n        """Execute a sequence of task steps"""\r\n        for step in task_steps:\r\n            success = self.execute_single_step(step)\r\n            if not success:\r\n                self.get_logger().error(f"Failed to execute step: {step}")\r\n                return False\r\n        \r\n        return True\r\n    \r\n    def execute_single_step(self, step: TaskStep) -> bool:\r\n        """Execute a single task step"""\r\n        self.current_action = step\r\n        \r\n        if step.action == "navigate_to":\r\n            return self._execute_navigate(step.parameters)\r\n        elif step.action == "grasp_object":\r\n            return self._execute_grasp(step.parameters)\r\n        elif step.action == "place_object":\r\n            return self._execute_place(step.parameters)\r\n        elif step.action == "detect_object":\r\n            return self._execute_detect(step.parameters)\r\n        elif step.action == "open_container":\r\n            return self._execute_open_container(step.parameters)\r\n        elif step.action == "close_container":\r\n            return self._execute_close_container(step.parameters)\r\n        elif step.action == "wait":\r\n            return self._execute_wait(step.parameters)\r\n        else:\r\n            self.get_logger().error(f"Unknown action: {step.action}")\r\n            return False\r\n    \r\n    def _execute_navigate(self, params: Dict[str, Any]) -> bool:\r\n        """Execute navigation action"""\r\n        # Create navigation goal\r\n        goal_msg = NavigateToPose.Goal()\r\n        goal_msg.pose.header.frame_id = params.get("frame_id", "map")\r\n        \r\n        # Set position\r\n        position = params.get("position", {"x": 0.0, "y": 0.0, "z": 0.0})\r\n        goal_msg.pose.pose.position.x = position["x"]\r\n        goal_msg.pose.pose.position.y = position["y"]\r\n        goal_msg.pose.pose.position.z = position["z"]\r\n        \r\n        # Set orientation (simplified)\r\n        orientation = params.get("orientation", {"w": 1.0})\r\n        goal_msg.pose.pose.orientation.w = orientation.get("w", 1.0)\r\n        goal_msg.pose.pose.orientation.x = orientation.get("x", 0.0)\r\n        goal_msg.pose.pose.orientation.y = orientation.get("y", 0.0)\r\n        goal_msg.pose.pose.orientation.z = orientation.get("z", 0.0)\r\n        \r\n        # Send goal\r\n        self.nav_client.wait_for_server()\r\n        future = self.nav_client.send_goal_async(goal_msg)\r\n        \r\n        # Wait for result\r\n        rclpy.spin_until_future_complete(self, future)\r\n        goal_handle = future.result()\r\n        \r\n        if goal_handle.status == GoalStatus.STATUS_SUCCEEDED:\r\n            self.publish_status(f"Successfully navigated to {params}")\r\n            return True\r\n        else:\r\n            self.publish_status(f"Failed to navigate to {params}")\r\n            return False\r\n    \r\n    def _execute_grasp(self, params: Dict[str, Any]) -> bool:\r\n        """Execute grasp action"""\r\n        # Implementation for grasping an object\r\n        object_name = params.get("object", "")\r\n        grasp_pose = params.get("grasp_pose", "default")\r\n        \r\n        # Create manipulation goal\r\n        goal_msg = ManipulateObject.Goal()\r\n        goal_msg.object_name = object_name\r\n        goal_msg.manipulation_type = "grasp"\r\n        goal_msg.grasp_pose = grasp_pose\r\n        \r\n        # Send goal\r\n        self.manip_client.wait_for_server()\r\n        future = self.manip_client.send_goal_async(goal_msg)\r\n        \r\n        # Wait for result\r\n        rclpy.spin_until_future_complete(self, future)\r\n        goal_handle = future.result()\r\n        \r\n        if goal_handle.status == GoalStatus.STATUS_SUCCEEDED:\r\n            self.publish_status(f"Successfully grasped {object_name}")\r\n            return True\r\n        else:\r\n            self.publish_status(f"Failed to grasp {object_name}")\r\n            return False\r\n    \r\n    def _execute_place(self, params: Dict[str, Any]) -> bool:\r\n        """Execute place action"""\r\n        # Implementation for placing an object\r\n        object_name = params.get("object", "")\r\n        placement_location = params.get("location", "")\r\n        \r\n        # Create manipulation goal\r\n        goal_msg = ManipulateObject.Goal()\r\n        goal_msg.object_name = object_name\r\n        goal_msg.manipulation_type = "place"\r\n        goal_msg.placement_location = placement_location\r\n        \r\n        # Send goal\r\n        self.manip_client.wait_for_server()\r\n        future = self.manip_client.send_goal_async(goal_msg)\r\n        \r\n        # Wait for result\r\n        rclpy.spin_until_future_complete(self, future)\r\n        goal_handle = future.result()\r\n        \r\n        if goal_handle.status == GoalStatus.STATUS_SUCCEEDED:\r\n            self.publish_status(f"Successfully placed {object_name} at {placement_location}")\r\n            return True\r\n        else:\r\n            self.publish_status(f"Failed to place {object_name} at {placement_location}")\r\n            return False\r\n    \r\n    def publish_status(self, status: str):\r\n        """Publish action status"""\r\n        status_msg = String()\r\n        status_msg.data = status\r\n        self.status_pub.publish(status_msg)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-high-level-task-manager",children:"2. High-Level Task Manager"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class TaskManager:\r\n    def __init__(self):\r\n        self.llm_planner = ContextAwarePlanner()\r\n        self.action_executor = ActionExecutor()\r\n        self.ambiguity_resolver = AmbiguityResolver()\r\n        self.multi_step_planner = MultiStepTaskPlanner()\r\n        \r\n        # Task state\r\n        self.current_task_id = 0\r\n        self.active_tasks = {}\r\n    \r\n    def execute_natural_language_task(self, command: str, user_context: Dict[str, Any] = None) -> bool:\r\n        """Execute a task from natural language command"""\r\n        try:\r\n            # Resolve ambiguities in the command\r\n            env_state = self.action_executor.environment_perceptor.get_current_state()\r\n            resolved_command = self.ambiguity_resolver.resolve_ambiguity(command, env_state)\r\n            \r\n            # Plan the task using LLM\r\n            task_steps = self.llm_planner.plan_with_context(resolved_command, user_context)\r\n            \r\n            # Execute the planned steps\r\n            success = self.action_executor.execute_task_steps(task_steps)\r\n            \r\n            if success:\r\n                self.action_executor.publish_status(f"Successfully completed task: {command}")\r\n            else:\r\n                self.action_executor.publish_status(f"Failed to complete task: {command}")\r\n            \r\n            return success\r\n            \r\n        except Exception as e:\r\n            self.action_executor.publish_status(f"Error executing task: {str(e)}")\r\n            return False\r\n    \r\n    def execute_complex_task(self, high_level_goal: str, user_context: Dict[str, Any] = None) -> bool:\r\n        """Execute a complex task by decomposing it"""\r\n        try:\r\n            # Decompose the complex task\r\n            task_steps = self.multi_step_planner.plan_complex_task(high_level_goal)\r\n            \r\n            # Execute the steps\r\n            success = self.action_executor.execute_task_steps(task_steps)\r\n            \r\n            if success:\r\n                self.action_executor.publish_status(f"Successfully completed complex task: {high_level_goal}")\r\n            else:\r\n                self.action_executor.publish_status(f"Failed to complete complex task: {high_level_goal}")\r\n            \r\n            return success\r\n            \r\n        except Exception as e:\r\n            self.action_executor.publish_status(f"Error executing complex task: {str(e)}")\r\n            return False\r\n    \r\n    def handle_execution_feedback(self, feedback: Dict[str, Any]):\r\n        """Handle feedback from execution and adapt plan if needed"""\r\n        if feedback.get("error"):\r\n            # Adapt the plan based on feedback\r\n            new_plan = self.llm_planner.adapt_plan_to_feedback(\r\n                self.action_executor.current_plan,\r\n                feedback\r\n            )\r\n            \r\n            # Execute the adapted plan\r\n            self.action_executor.execute_task_steps(new_plan)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"performance-optimization-and-safety",children:"Performance Optimization and Safety"}),"\n",(0,a.jsx)(n.h3,{id:"1-plan-validation",children:"1. Plan Validation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class PlanValidator:\r\n    def __init__(self):\r\n        self.knowledge_base = KnowledgeBase()\r\n    \r\n    def validate_plan(self, task_steps: List[TaskStep], env_state: EnvironmentState) -> Dict[str, Any]:\r\n        """Validate a plan for feasibility and safety"""\r\n        issues = []\r\n        \r\n        # Check for physical feasibility\r\n        for i, step in enumerate(task_steps):\r\n            if step.action == "grasp_object":\r\n                obj_name = step.parameters.get("object")\r\n                if obj_name:\r\n                    obj_info = env_state.find_object_by_name(obj_name)\r\n                    if not obj_info:\r\n                        issues.append(f"Step {i}: Object \'{obj_name}\' not found in environment")\r\n                    elif "grasp" not in self.knowledge_base.get_affordances(obj_info.object_type.value):\r\n                        issues.append(f"Step {i}: Object \'{obj_name}\' cannot be grasped")\r\n            \r\n            elif step.action == "navigate_to":\r\n                # Check if navigation is possible to the location\r\n                # This would involve checking the navigation system\r\n                pass\r\n        \r\n        # Check for safety constraints\r\n        for i, step in enumerate(task_steps):\r\n            if step.action == "grasp_object":\r\n                obj_name = step.parameters.get("object")\r\n                obj_info = env_state.find_object_by_name(obj_name)\r\n                if obj_info and obj_info.properties.get("fragile", False):\r\n                    # Check if next action is safe for fragile object\r\n                    next_action = task_steps[i+1].action if i+1 < len(task_steps) else None\r\n                    if next_action == "navigate_to" and step.parameters.get("speed", "normal") == "fast":\r\n                        issues.append(f"Step {i}: Fast navigation with fragile object is unsafe")\r\n        \r\n        return {\r\n            "is_valid": len(issues) == 0,\r\n            "issues": issues,\r\n            "suggestions": self._generate_suggestions(issues)\r\n        }\r\n    \r\n    def _generate_suggestions(self, issues: List[str]) -> List[str]:\r\n        """Generate suggestions to fix plan issues"""\r\n        suggestions = []\r\n        \r\n        for issue in issues:\r\n            if "not found" in issue:\r\n                suggestions.append("Re-scan the environment to locate the object")\r\n            elif "cannot be grasped" in issue:\r\n                suggestions.append("Use a different manipulation strategy or tool")\r\n            elif "unsafe" in issue:\r\n                suggestions.append("Reduce speed or use protective measures")\r\n        \r\n        return suggestions\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-error-recovery",children:"2. Error Recovery"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class ErrorRecovery:\r\n    def __init__(self):\r\n        self.llm_planner = ContextAwarePlanner()\r\n    \r\n    def recover_from_error(self, error_type: str, error_context: Dict[str, Any]) -> List[TaskStep]:\r\n        """Generate recovery steps for different error types"""\r\n        recovery_plan = []\r\n        \r\n        if error_type == "object_not_found":\r\n            # Look for the object in nearby locations\r\n            recovery_plan.append(TaskStep(\r\n                action="detect_object",\r\n                parameters={"object": error_context.get("object_name", "unknown")},\r\n                description="Look for the object in the vicinity"\r\n            ))\r\n            \r\n            # If still not found, ask for clarification\r\n            recovery_plan.append(TaskStep(\r\n                action="request_human_assistance",\r\n                parameters={"query": f"Could you help me find the {error_context.get(\'object_name\', \'object\')}?"},\r\n                description="Request human assistance to locate object"\r\n            ))\r\n        \r\n        elif error_type == "navigation_failed":\r\n            # Try an alternative path\r\n            recovery_plan.append(TaskStep(\r\n                action="calculate_alternative_path",\r\n                parameters=error_context.get("navigation_params", {}),\r\n                description="Calculate an alternative navigation path"\r\n            ))\r\n            \r\n            # Retry navigation\r\n            recovery_plan.append(TaskStep(\r\n                action="navigate_to",\r\n                parameters=error_context.get("navigation_params", {}),\r\n                description="Navigate using alternative path"\r\n            ))\r\n        \r\n        elif error_type == "grasp_failed":\r\n            # Try different grasp approach\r\n            recovery_plan.append(TaskStep(\r\n                action="plan_alternative_grasp",\r\n                parameters=error_context.get("grasp_params", {}),\r\n                description="Plan an alternative grasp approach"\r\n            ))\r\n            \r\n            # Retry grasp\r\n            recovery_plan.append(TaskStep(\r\n                action="grasp_object",\r\n                parameters=error_context.get("grasp_params", {}),\r\n                description="Attempt grasp with alternative approach"\r\n            ))\r\n        \r\n        return recovery_plan\n'})}),"\n",(0,a.jsx)(n.p,{children:"LLM-based task planning represents a significant advancement in humanoid robotics, enabling robots to understand and execute complex natural language commands. By combining the reasoning capabilities of large language models with the physical capabilities of humanoid robots, we can create more intuitive and flexible robotic systems that can adapt to various tasks and environments."})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},8453(e,n,t){t.d(n,{R:()=>o,x:()=>i});var r=t(6540);const a={},s=r.createContext(a);function o(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);