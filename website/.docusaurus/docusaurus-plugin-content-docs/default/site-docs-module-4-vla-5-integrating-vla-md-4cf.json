{
  "id": "module-4-vla/integrating-vla",
  "title": "5. Integrating Visual Perception into VLA",
  "description": "Our VLA pipeline can now understand spoken language and translate it into a sequence of abstract actions. However, the actions currently rely on predefined locations (\"kitchen\", \"bedroom\") and abstract object names (\"mug\"). For true autonomy, our robot needs to visually perceive these objects and their locations in the real world.",
  "source": "@site/docs/module-4-vla/5-integrating-vla.md",
  "sourceDirName": "module-4-vla",
  "slug": "/module-4-vla/integrating-vla",
  "permalink": "/physical-ai-book/docs/module-4-vla/integrating-vla",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/physical-ai-book/physical-ai-book/tree/main/docs/module-4-vla/5-integrating-vla.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 5,
  "frontMatter": {
    "title": "5. Integrating Visual Perception into VLA",
    "sidebar_label": "Integrating VLA",
    "sidebar_position": 5
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Cognitive Planning Pipelines",
    "permalink": "/physical-ai-book/docs/module-4-vla/cognitive-planning-pipelines"
  },
  "next": {
    "title": "Exercises",
    "permalink": "/physical-ai-book/docs/module-4-vla/exercises"
  }
}