{
  "id": "module4/introduction",
  "title": "Introduction to Vision-Language-Action Systems",
  "description": "Vision-Language-Action (VLA) systems represent the next frontier in robotics, where robots can perceive their environment through vision, understand human instructions through language, and execute complex actions to achieve goals. This integration enables robots to operate in human environments with unprecedented flexibility and adaptability.",
  "source": "@site/docs/module4/introduction.md",
  "sourceDirName": "module4",
  "slug": "/module4/introduction",
  "permalink": "/docs/module4/introduction",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-book/edit/main/docs/module4/introduction.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 1,
  "frontMatter": {
    "sidebar_position": 1,
    "title": "Introduction to Vision-Language-Action Systems"
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Navigation 2 (Nav2) for Humanoid Robots",
    "permalink": "/docs/module3/nav2-planning"
  },
  "next": {
    "title": "Voice-to-Action Systems for Humanoid Robots",
    "permalink": "/docs/module4/voice-to-action"
  }
}