{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","href":"/docs/intro","label":"Introduction","docId":"intro","unlisted":false},{"type":"category","label":"Module 1: ROS 2 - Robotic Nervous System","collapsed":false,"items":[{"type":"link","href":"/docs/module1/introduction","label":"Introduction to ROS 2","docId":"module1/introduction","unlisted":false},{"type":"link","href":"/docs/module1/ros2-setup","label":"ROS 2 Setup and Installation","docId":"module1/ros2-setup","unlisted":false},{"type":"link","href":"/docs/module1/nodes-topics-services","label":"Nodes, Topics, and Services","docId":"module1/nodes-topics-services","unlisted":false},{"type":"link","href":"/docs/module1/python-rclpy","label":"Python Integration with rclpy","docId":"module1/python-rclpy","unlisted":false},{"type":"link","href":"/docs/module1/urdf-humanoids","label":"URDF for Humanoid Robots","docId":"module1/urdf-humanoids","unlisted":false}],"collapsible":true},{"type":"category","label":"Module 2: Gazebo & Unity - Digital Twin","collapsed":false,"items":[{"type":"link","href":"/docs/module2/introduction","label":"Introduction to Gazebo & Unity Digital Twin","docId":"module2/introduction","unlisted":false},{"type":"link","href":"/docs/module2/gazebo-environment","label":"Gazebo Environment Setup and Configuration","docId":"module2/gazebo-environment","unlisted":false},{"type":"link","href":"/docs/module2/physics-simulation","label":"Physics Simulation for Humanoid Robots","docId":"module2/physics-simulation","unlisted":false},{"type":"link","href":"/docs/module2/unity-rendering","label":"Unity Rendering for Humanoid Perception","docId":"module2/unity-rendering","unlisted":false},{"type":"link","href":"/docs/module2/sensor-simulation","label":"Sensor Simulation for Humanoid Robots","docId":"module2/sensor-simulation","unlisted":false}],"collapsible":true},{"type":"category","label":"Module 3: NVIDIA Isaac - AI Robot Brain","collapsed":false,"items":[{"type":"link","href":"/docs/module3/introduction","label":"Introduction to NVIDIA Isaac","docId":"module3/introduction","unlisted":false},{"type":"link","href":"/docs/module3/isaac-sim","label":"Isaac Sim for Humanoid Robotics","docId":"module3/isaac-sim","unlisted":false},{"type":"link","href":"/docs/module3/isaac-ros","label":"Isaac ROS Integration","docId":"module3/isaac-ros","unlisted":false},{"type":"link","href":"/docs/module3/visual-slam","label":"Visual SLAM for Humanoid Navigation","docId":"module3/visual-slam","unlisted":false},{"type":"link","href":"/docs/module3/nav2-planning","label":"Navigation 2 (Nav2) for Humanoid Robots","docId":"module3/nav2-planning","unlisted":false}],"collapsible":true},{"type":"category","label":"Module 4: Vision-Language-Action","collapsed":false,"items":[{"type":"link","href":"/docs/module4/introduction","label":"Introduction to Vision-Language-Action Systems","docId":"module4/introduction","unlisted":false},{"type":"link","href":"/docs/module4/voice-to-action","label":"Voice-to-Action Systems for Humanoid Robots","docId":"module4/voice-to-action","unlisted":false},{"type":"link","href":"/docs/module4/llm-planning","label":"LLM-Based Task Planning for Humanoid Robots","docId":"module4/llm-planning","unlisted":false},{"type":"link","href":"/docs/module4/capstone-project","label":"Capstone Project - Complete Humanoid Robot System","docId":"module4/capstone-project","unlisted":false}],"collapsible":true}]},"docs":{"intro":{"id":"intro","title":"Introduction","description":"Welcome to the frontier of artificial intelligence and robotics. This open-source textbook is your guide to building intelligent humanoid robots that can perceive, understand, and interact with the complex physical world. As we stand on the cusp of a new era in robotics, the ability to bridge the gap between AI algorithms and physical embodiment has never been more critical.","sidebar":"tutorialSidebar"},"module1/introduction":{"id":"module1/introduction","title":"Introduction to ROS 2","description":"ROS 2 (Robot Operating System 2) is the next generation of the most widely used middleware for robotics development. It provides a structured communication layer above the host operating systems of a heterogenous compute cluster, enabling distributed computing for robotic applications.","sidebar":"tutorialSidebar"},"module1/nodes-topics-services":{"id":"module1/nodes-topics-services","title":"Nodes, Topics, and Services","description":"Understanding the communication patterns in ROS 2 is fundamental to building distributed robotic systems. This chapter explores the three primary communication mechanisms.","sidebar":"tutorialSidebar"},"module1/python-rclpy":{"id":"module1/python-rclpy","title":"Python Integration with rclpy","description":"The rclpy library is the Python client library for ROS 2, providing the Python API to interact with ROS 2 concepts like nodes, topics, services, parameters, and actions.","sidebar":"tutorialSidebar"},"module1/ros2-setup":{"id":"module1/ros2-setup","title":"ROS 2 Setup and Installation","description":"This chapter guides you through setting up ROS 2 on Ubuntu 22.04 LTS, which is the recommended platform for robotics development.","sidebar":"tutorialSidebar"},"module1/urdf-humanoids":{"id":"module1/urdf-humanoids","title":"URDF for Humanoid Robots","description":"URDF (Unified Robot Description Format) is an XML format for representing a robot model. For humanoid robots, URDF describes the kinematic and dynamic properties of the robot's body structure.","sidebar":"tutorialSidebar"},"module2/gazebo-environment":{"id":"module2/gazebo-environment","title":"Gazebo Environment Setup and Configuration","description":"Setting up Gazebo for humanoid robotics simulation requires careful configuration to ensure accurate physics simulation and proper integration with ROS 2. This chapter covers the complete setup process for creating effective simulation environments.","sidebar":"tutorialSidebar"},"module2/introduction":{"id":"module2/introduction","title":"Introduction to Gazebo & Unity Digital Twin","description":"Digital twins are virtual replicas of physical systems that enable testing, validation, and optimization of robotic systems in a safe, controlled environment. In humanoid robotics, digital twins are essential for developing complex behaviors without risking expensive hardware or human safety.","sidebar":"tutorialSidebar"},"module2/physics-simulation":{"id":"module2/physics-simulation","title":"Physics Simulation for Humanoid Robots","description":"Physics simulation is the cornerstone of realistic humanoid robot simulation. Accurate physics modeling enables the development of stable locomotion, manipulation, and interaction behaviors that can transfer from simulation to the real world.","sidebar":"tutorialSidebar"},"module2/sensor-simulation":{"id":"module2/sensor-simulation","title":"Sensor Simulation for Humanoid Robots","description":"Sensor simulation is critical for humanoid robotics, as these robots rely on diverse sensors to perceive their environment, maintain balance, and execute complex tasks. This chapter covers the simulation of various sensor types essential for humanoid robot operation.","sidebar":"tutorialSidebar"},"module2/unity-rendering":{"id":"module2/unity-rendering","title":"Unity Rendering for Humanoid Perception","description":"Unity's advanced rendering capabilities provide high-fidelity visual simulation essential for humanoid robot perception systems. This chapter explores how to leverage Unity's rendering pipeline to create photorealistic environments that support computer vision and perception algorithm development.","sidebar":"tutorialSidebar"},"module3/introduction":{"id":"module3/introduction","title":"Introduction to NVIDIA Isaac","description":"The NVIDIA Isaac platform represents a revolutionary approach to developing AI-powered robots. Combining high-performance computing hardware with sophisticated software frameworks, Isaac provides the tools needed to create intelligent, autonomous robots capable of complex perception, navigation, and manipulation tasks.","sidebar":"tutorialSidebar"},"module3/isaac-ros":{"id":"module3/isaac-ros","title":"Isaac ROS Integration","description":"Isaac ROS is a collection of hardware-accelerated packages that bring the power of NVIDIA GPUs to the ROS/ROS 2 ecosystem. These packages provide optimized implementations of common robotic algorithms, enabling real-time processing of complex sensor data and decision-making for humanoid robots.","sidebar":"tutorialSidebar"},"module3/isaac-sim":{"id":"module3/isaac-sim","title":"Isaac Sim for Humanoid Robotics","description":"Isaac Sim is NVIDIA's high-fidelity simulation environment built on the Omniverse platform, specifically designed for robotics applications. It provides photorealistic rendering, accurate physics simulation, and seamless integration with the ROS/ROS 2 ecosystem, making it ideal for developing complex humanoid robots.","sidebar":"tutorialSidebar"},"module3/nav2-planning":{"id":"module3/nav2-planning","title":"Navigation 2 (Nav2) for Humanoid Robots","description":"Navigation 2 (Nav2) is the latest navigation stack for ROS 2, designed to provide robust, reliable, and efficient navigation for mobile robots. For humanoid robots, Nav2 provides the essential capabilities needed to navigate complex, human-centric environments safely and efficiently.","sidebar":"tutorialSidebar"},"module3/visual-slam":{"id":"module3/visual-slam","title":"Visual SLAM for Humanoid Navigation","description":"Visual Simultaneous Localization and Mapping (SLAM) is a critical technology for humanoid robots operating in unknown or dynamic environments. It enables robots to build maps of their surroundings while simultaneously determining their position within those maps, all using visual information from cameras.","sidebar":"tutorialSidebar"},"module4/capstone-project":{"id":"module4/capstone-project","title":"Capstone Project - Complete Humanoid Robot System","description":"The capstone project integrates all the concepts learned throughout this course to build a complete humanoid robot system. This project demonstrates how ROS 2, Gazebo/Unity, NVIDIA Isaac, Visual SLAM, Navigation 2, Vision-Language-Action systems, and LLM-based planning work together to create an intelligent, autonomous humanoid robot.","sidebar":"tutorialSidebar"},"module4/introduction":{"id":"module4/introduction","title":"Introduction to Vision-Language-Action Systems","description":"Vision-Language-Action (VLA) systems represent the next frontier in robotics, where robots can perceive their environment through vision, understand human instructions through language, and execute complex actions to achieve goals. This integration enables robots to operate in human environments with unprecedented flexibility and adaptability.","sidebar":"tutorialSidebar"},"module4/llm-planning":{"id":"module4/llm-planning","title":"LLM-Based Task Planning for Humanoid Robots","description":"Large Language Models (LLMs) have revolutionized the field of artificial intelligence, offering unprecedented capabilities in natural language understanding and reasoning. When applied to humanoid robotics, LLMs can significantly enhance task planning capabilities, enabling robots to interpret complex instructions, reason about their environment, and generate sophisticated action sequences.","sidebar":"tutorialSidebar"},"module4/voice-to-action":{"id":"module4/voice-to-action","title":"Voice-to-Action Systems for Humanoid Robots","description":"Voice-to-Action systems enable humanoid robots to understand spoken language and convert it into executable actions. This capability is crucial for natural human-robot interaction, allowing users to command robots using everyday language rather than specialized interfaces.","sidebar":"tutorialSidebar"}}}}