{"allContent":{"docusaurus-plugin-content-docs":{"default":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/docs","tagsPath":"/docs/tags","editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs","isLast":true,"routePriority":-1,"sidebarFilePath":"C:\\Users\\Pcw\\physical-ai-book\\website\\sidebars.js","contentPath":"C:\\Users\\Pcw\\physical-ai-book\\website\\docs","docs":[{"id":"intro","title":"Introduction","description":"Welcome to the frontier of artificial intelligence and robotics. This open-source textbook is your guide to building intelligent humanoid robots that can perceive, understand, and interact with the complex physical world. As we stand on the cusp of a new era in robotics, the ability to bridge the gap between AI algorithms and physical embodiment has never been more critical.","source":"@site/docs/intro.mdx","sourceDirName":".","slug":"/intro","permalink":"/docs/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs/intro.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Introduction"},"sidebar":"tutorialSidebar","next":{"title":"Introduction to ROS 2","permalink":"/docs/module1/introduction"}},{"id":"module1/introduction","title":"Introduction to ROS 2","description":"ROS 2 (Robot Operating System 2) is the next generation of the most widely used middleware for robotics development. It provides a structured communication layer above the host operating systems of a heterogenous compute cluster, enabling distributed computing for robotic applications.","source":"@site/docs/module1/introduction.md","sourceDirName":"module1","slug":"/module1/introduction","permalink":"/docs/module1/introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs/module1/introduction.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Introduction to ROS 2"},"sidebar":"tutorialSidebar","previous":{"title":"Introduction","permalink":"/docs/intro"},"next":{"title":"ROS 2 Setup and Installation","permalink":"/docs/module1/ros2-setup"}},{"id":"module1/nodes-topics-services","title":"Nodes, Topics, and Services","description":"Understanding the communication patterns in ROS 2 is fundamental to building distributed robotic systems. This chapter explores the three primary communication mechanisms.","source":"@site/docs/module1/nodes-topics-services.md","sourceDirName":"module1","slug":"/module1/nodes-topics-services","permalink":"/docs/module1/nodes-topics-services","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs/module1/nodes-topics-services.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Nodes, Topics, and Services"},"sidebar":"tutorialSidebar","previous":{"title":"ROS 2 Setup and Installation","permalink":"/docs/module1/ros2-setup"},"next":{"title":"Python Integration with rclpy","permalink":"/docs/module1/python-rclpy"}},{"id":"module1/python-rclpy","title":"Python Integration with rclpy","description":"The rclpy library is the Python client library for ROS 2, providing the Python API to interact with ROS 2 concepts like nodes, topics, services, parameters, and actions.","source":"@site/docs/module1/python-rclpy.md","sourceDirName":"module1","slug":"/module1/python-rclpy","permalink":"/docs/module1/python-rclpy","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs/module1/python-rclpy.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Python Integration with rclpy"},"sidebar":"tutorialSidebar","previous":{"title":"Nodes, Topics, and Services","permalink":"/docs/module1/nodes-topics-services"},"next":{"title":"URDF for Humanoid Robots","permalink":"/docs/module1/urdf-humanoids"}},{"id":"module1/ros2-setup","title":"ROS 2 Setup and Installation","description":"This chapter guides you through setting up ROS 2 on Ubuntu 22.04 LTS, which is the recommended platform for robotics development.","source":"@site/docs/module1/ros2-setup.md","sourceDirName":"module1","slug":"/module1/ros2-setup","permalink":"/docs/module1/ros2-setup","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs/module1/ros2-setup.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"ROS 2 Setup and Installation"},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to ROS 2","permalink":"/docs/module1/introduction"},"next":{"title":"Nodes, Topics, and Services","permalink":"/docs/module1/nodes-topics-services"}},{"id":"module1/urdf-humanoids","title":"URDF for Humanoid Robots","description":"URDF (Unified Robot Description Format) is an XML format for representing a robot model. For humanoid robots, URDF describes the kinematic and dynamic properties of the robot's body structure.","source":"@site/docs/module1/urdf-humanoids.md","sourceDirName":"module1","slug":"/module1/urdf-humanoids","permalink":"/docs/module1/urdf-humanoids","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs/module1/urdf-humanoids.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"URDF for Humanoid Robots"},"sidebar":"tutorialSidebar","previous":{"title":"Python Integration with rclpy","permalink":"/docs/module1/python-rclpy"},"next":{"title":"Introduction to Gazebo & Unity Digital Twin","permalink":"/docs/module2/introduction"}},{"id":"module2/gazebo-environment","title":"Gazebo Environment Setup and Configuration","description":"Setting up Gazebo for humanoid robotics simulation requires careful configuration to ensure accurate physics simulation and proper integration with ROS 2. This chapter covers the complete setup process for creating effective simulation environments.","source":"@site/docs/module2/gazebo-environment.md","sourceDirName":"module2","slug":"/module2/gazebo-environment","permalink":"/docs/module2/gazebo-environment","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs/module2/gazebo-environment.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Gazebo Environment Setup and Configuration"},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to Gazebo & Unity Digital Twin","permalink":"/docs/module2/introduction"},"next":{"title":"Physics Simulation for Humanoid Robots","permalink":"/docs/module2/physics-simulation"}},{"id":"module2/introduction","title":"Introduction to Gazebo & Unity Digital Twin","description":"Digital twins are virtual replicas of physical systems that enable testing, validation, and optimization of robotic systems in a safe, controlled environment. In humanoid robotics, digital twins are essential for developing complex behaviors without risking expensive hardware or human safety.","source":"@site/docs/module2/introduction.md","sourceDirName":"module2","slug":"/module2/introduction","permalink":"/docs/module2/introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs/module2/introduction.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Introduction to Gazebo & Unity Digital Twin"},"sidebar":"tutorialSidebar","previous":{"title":"URDF for Humanoid Robots","permalink":"/docs/module1/urdf-humanoids"},"next":{"title":"Gazebo Environment Setup and Configuration","permalink":"/docs/module2/gazebo-environment"}},{"id":"module2/physics-simulation","title":"Physics Simulation for Humanoid Robots","description":"Physics simulation is the cornerstone of realistic humanoid robot simulation. Accurate physics modeling enables the development of stable locomotion, manipulation, and interaction behaviors that can transfer from simulation to the real world.","source":"@site/docs/module2/physics-simulation.md","sourceDirName":"module2","slug":"/module2/physics-simulation","permalink":"/docs/module2/physics-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs/module2/physics-simulation.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Physics Simulation for Humanoid Robots"},"sidebar":"tutorialSidebar","previous":{"title":"Gazebo Environment Setup and Configuration","permalink":"/docs/module2/gazebo-environment"},"next":{"title":"Unity Rendering for Humanoid Perception","permalink":"/docs/module2/unity-rendering"}},{"id":"module2/sensor-simulation","title":"Sensor Simulation for Humanoid Robots","description":"Sensor simulation is critical for humanoid robotics, as these robots rely on diverse sensors to perceive their environment, maintain balance, and execute complex tasks. This chapter covers the simulation of various sensor types essential for humanoid robot operation.","source":"@site/docs/module2/sensor-simulation.md","sourceDirName":"module2","slug":"/module2/sensor-simulation","permalink":"/docs/module2/sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs/module2/sensor-simulation.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"Sensor Simulation for Humanoid Robots"},"sidebar":"tutorialSidebar","previous":{"title":"Unity Rendering for Humanoid Perception","permalink":"/docs/module2/unity-rendering"},"next":{"title":"Introduction to NVIDIA Isaac","permalink":"/docs/module3/introduction"}},{"id":"module2/unity-rendering","title":"Unity Rendering for Humanoid Perception","description":"Unity's advanced rendering capabilities provide high-fidelity visual simulation essential for humanoid robot perception systems. This chapter explores how to leverage Unity's rendering pipeline to create photorealistic environments that support computer vision and perception algorithm development.","source":"@site/docs/module2/unity-rendering.md","sourceDirName":"module2","slug":"/module2/unity-rendering","permalink":"/docs/module2/unity-rendering","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs/module2/unity-rendering.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Unity Rendering for Humanoid Perception"},"sidebar":"tutorialSidebar","previous":{"title":"Physics Simulation for Humanoid Robots","permalink":"/docs/module2/physics-simulation"},"next":{"title":"Sensor Simulation for Humanoid Robots","permalink":"/docs/module2/sensor-simulation"}},{"id":"module3/introduction","title":"Introduction to NVIDIA Isaac","description":"The NVIDIA Isaac platform represents a revolutionary approach to developing AI-powered robots. Combining high-performance computing hardware with sophisticated software frameworks, Isaac provides the tools needed to create intelligent, autonomous robots capable of complex perception, navigation, and manipulation tasks.","source":"@site/docs/module3/introduction.md","sourceDirName":"module3","slug":"/module3/introduction","permalink":"/docs/module3/introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs/module3/introduction.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Introduction to NVIDIA Isaac"},"sidebar":"tutorialSidebar","previous":{"title":"Sensor Simulation for Humanoid Robots","permalink":"/docs/module2/sensor-simulation"},"next":{"title":"Isaac Sim for Humanoid Robotics","permalink":"/docs/module3/isaac-sim"}},{"id":"module3/isaac-ros","title":"Isaac ROS Integration","description":"Isaac ROS is a collection of hardware-accelerated packages that bring the power of NVIDIA GPUs to the ROS/ROS 2 ecosystem. These packages provide optimized implementations of common robotic algorithms, enabling real-time processing of complex sensor data and decision-making for humanoid robots.","source":"@site/docs/module3/isaac-ros.md","sourceDirName":"module3","slug":"/module3/isaac-ros","permalink":"/docs/module3/isaac-ros","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs/module3/isaac-ros.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Isaac ROS Integration"},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim for Humanoid Robotics","permalink":"/docs/module3/isaac-sim"},"next":{"title":"Visual SLAM for Humanoid Navigation","permalink":"/docs/module3/visual-slam"}},{"id":"module3/isaac-sim","title":"Isaac Sim for Humanoid Robotics","description":"Isaac Sim is NVIDIA's high-fidelity simulation environment built on the Omniverse platform, specifically designed for robotics applications. It provides photorealistic rendering, accurate physics simulation, and seamless integration with the ROS/ROS 2 ecosystem, making it ideal for developing complex humanoid robots.","source":"@site/docs/module3/isaac-sim.md","sourceDirName":"module3","slug":"/module3/isaac-sim","permalink":"/docs/module3/isaac-sim","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs/module3/isaac-sim.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Isaac Sim for Humanoid Robotics"},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to NVIDIA Isaac","permalink":"/docs/module3/introduction"},"next":{"title":"Isaac ROS Integration","permalink":"/docs/module3/isaac-ros"}},{"id":"module3/nav2-planning","title":"Navigation 2 (Nav2) for Humanoid Robots","description":"Navigation 2 (Nav2) is the latest navigation stack for ROS 2, designed to provide robust, reliable, and efficient navigation for mobile robots. For humanoid robots, Nav2 provides the essential capabilities needed to navigate complex, human-centric environments safely and efficiently.","source":"@site/docs/module3/nav2-planning.md","sourceDirName":"module3","slug":"/module3/nav2-planning","permalink":"/docs/module3/nav2-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs/module3/nav2-planning.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"Navigation 2 (Nav2) for Humanoid Robots"},"sidebar":"tutorialSidebar","previous":{"title":"Visual SLAM for Humanoid Navigation","permalink":"/docs/module3/visual-slam"},"next":{"title":"Introduction to Vision-Language-Action Systems","permalink":"/docs/module4/introduction"}},{"id":"module3/visual-slam","title":"Visual SLAM for Humanoid Navigation","description":"Visual Simultaneous Localization and Mapping (SLAM) is a critical technology for humanoid robots operating in unknown or dynamic environments. It enables robots to build maps of their surroundings while simultaneously determining their position within those maps, all using visual information from cameras.","source":"@site/docs/module3/visual-slam.md","sourceDirName":"module3","slug":"/module3/visual-slam","permalink":"/docs/module3/visual-slam","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs/module3/visual-slam.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Visual SLAM for Humanoid Navigation"},"sidebar":"tutorialSidebar","previous":{"title":"Isaac ROS Integration","permalink":"/docs/module3/isaac-ros"},"next":{"title":"Navigation 2 (Nav2) for Humanoid Robots","permalink":"/docs/module3/nav2-planning"}},{"id":"module4/capstone-project","title":"Capstone Project - Complete Humanoid Robot System","description":"The capstone project integrates all the concepts learned throughout this course to build a complete humanoid robot system. This project demonstrates how ROS 2, Gazebo/Unity, NVIDIA Isaac, Visual SLAM, Navigation 2, Vision-Language-Action systems, and LLM-based planning work together to create an intelligent, autonomous humanoid robot.","source":"@site/docs/module4/capstone-project.md","sourceDirName":"module4","slug":"/module4/capstone-project","permalink":"/docs/module4/capstone-project","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs/module4/capstone-project.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Capstone Project - Complete Humanoid Robot System"},"sidebar":"tutorialSidebar","previous":{"title":"LLM-Based Task Planning for Humanoid Robots","permalink":"/docs/module4/llm-planning"}},{"id":"module4/introduction","title":"Introduction to Vision-Language-Action Systems","description":"Vision-Language-Action (VLA) systems represent the next frontier in robotics, where robots can perceive their environment through vision, understand human instructions through language, and execute complex actions to achieve goals. This integration enables robots to operate in human environments with unprecedented flexibility and adaptability.","source":"@site/docs/module4/introduction.md","sourceDirName":"module4","slug":"/module4/introduction","permalink":"/docs/module4/introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs/module4/introduction.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Introduction to Vision-Language-Action Systems"},"sidebar":"tutorialSidebar","previous":{"title":"Navigation 2 (Nav2) for Humanoid Robots","permalink":"/docs/module3/nav2-planning"},"next":{"title":"Voice-to-Action Systems for Humanoid Robots","permalink":"/docs/module4/voice-to-action"}},{"id":"module4/llm-planning","title":"LLM-Based Task Planning for Humanoid Robots","description":"Large Language Models (LLMs) have revolutionized the field of artificial intelligence, offering unprecedented capabilities in natural language understanding and reasoning. When applied to humanoid robotics, LLMs can significantly enhance task planning capabilities, enabling robots to interpret complex instructions, reason about their environment, and generate sophisticated action sequences.","source":"@site/docs/module4/llm-planning.md","sourceDirName":"module4","slug":"/module4/llm-planning","permalink":"/docs/module4/llm-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs/module4/llm-planning.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"LLM-Based Task Planning for Humanoid Robots"},"sidebar":"tutorialSidebar","previous":{"title":"Voice-to-Action Systems for Humanoid Robots","permalink":"/docs/module4/voice-to-action"},"next":{"title":"Capstone Project - Complete Humanoid Robot System","permalink":"/docs/module4/capstone-project"}},{"id":"module4/voice-to-action","title":"Voice-to-Action Systems for Humanoid Robots","description":"Voice-to-Action systems enable humanoid robots to understand spoken language and convert it into executable actions. This capability is crucial for natural human-robot interaction, allowing users to command robots using everyday language rather than specialized interfaces.","source":"@site/docs/module4/voice-to-action.md","sourceDirName":"module4","slug":"/module4/voice-to-action","permalink":"/docs/module4/voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-book/edit/main/docs/module4/voice-to-action.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Voice-to-Action Systems for Humanoid Robots"},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to Vision-Language-Action Systems","permalink":"/docs/module4/introduction"},"next":{"title":"LLM-Based Task Planning for Humanoid Robots","permalink":"/docs/module4/llm-planning"}}],"drafts":[],"sidebars":{"tutorialSidebar":[{"type":"doc","id":"intro","label":"Introduction","translatable":true},{"type":"category","label":"Module 1: ROS 2 - Robotic Nervous System","collapsed":false,"items":[{"type":"doc","id":"module1/introduction"},{"type":"doc","id":"module1/ros2-setup"},{"type":"doc","id":"module1/nodes-topics-services"},{"type":"doc","id":"module1/python-rclpy"},{"type":"doc","id":"module1/urdf-humanoids"}],"collapsible":true},{"type":"category","label":"Module 2: Gazebo & Unity - Digital Twin","collapsed":false,"items":[{"type":"doc","id":"module2/introduction"},{"type":"doc","id":"module2/gazebo-environment"},{"type":"doc","id":"module2/physics-simulation"},{"type":"doc","id":"module2/unity-rendering"},{"type":"doc","id":"module2/sensor-simulation"}],"collapsible":true},{"type":"category","label":"Module 3: NVIDIA Isaac - AI Robot Brain","collapsed":false,"items":[{"type":"doc","id":"module3/introduction"},{"type":"doc","id":"module3/isaac-sim"},{"type":"doc","id":"module3/isaac-ros"},{"type":"doc","id":"module3/visual-slam"},{"type":"doc","id":"module3/nav2-planning"}],"collapsible":true},{"type":"category","label":"Module 4: Vision-Language-Action","collapsed":false,"items":[{"type":"doc","id":"module4/introduction"},{"type":"doc","id":"module4/voice-to-action"},{"type":"doc","id":"module4/llm-planning"},{"type":"doc","id":"module4/capstone-project"}],"collapsible":true}]}}]}},"docusaurus-plugin-content-blog":{"default":{"blogSidebarTitle":"Recent posts","blogPosts":[],"blogListPaginated":[],"blogTags":{},"blogTagsListPath":"/blog/tags"}},"docusaurus-plugin-content-pages":{"default":[{"type":"jsx","permalink":"/","source":"@site/src/pages/index.js"}]},"docusaurus-plugin-debug":{},"docusaurus-plugin-svgr":{},"docusaurus-theme-classic":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}