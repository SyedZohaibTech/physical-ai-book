"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[2003],{769:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-2-digital-twin/sensor-simulation","title":"4. Simulating Sensors","description":"A robot is blind and deaf without its sensors. A critical function of a digital twin is to generate realistic sensor data that our perception and navigation algorithms can consume. In this chapter, we\'ll explore how to add and configure common robotic sensors in our simulators.","source":"@site/docs/module-2-digital-twin/4-sensor-simulation.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/sensor-simulation","permalink":"/physical-ai-book/docs/module-2-digital-twin/sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-ai-book/physical-ai-book/tree/main/docs/module-2-digital-twin/4-sensor-simulation.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"4. Simulating Sensors","sidebar_label":"Sensor Simulation","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Unity for HRI","permalink":"/physical-ai-book/docs/module-2-digital-twin/unity-for-hri"},"next":{"title":"Complete Humanoid Simulation","permalink":"/physical-ai-book/docs/module-2-digital-twin/complete-humanoid-simulation"}}');var i=a(4848),r=a(8453);const o={title:"4. Simulating Sensors",sidebar_label:"Sensor Simulation",sidebar_position:4},t="4. Simulating Sensors: The Robot's Senses",l={},c=[{value:"1. Cameras",id:"1-cameras",level:2},{value:"Gazebo",id:"gazebo",level:3},{value:"Unity",id:"unity",level:3},{value:"2. LiDAR (Laser Scanners)",id:"2-lidar-laser-scanners",level:2},{value:"Gazebo",id:"gazebo-1",level:3},{value:"3. IMUs (Inertial Measurement Units)",id:"3-imus-inertial-measurement-units",level:2},{value:"Gazebo",id:"gazebo-2",level:3},{value:"4. Depth Cameras",id:"4-depth-cameras",level:2},{value:"Gazebo",id:"gazebo-3",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"4-simulating-sensors-the-robots-senses",children:"4. Simulating Sensors: The Robot's Senses"})}),"\n",(0,i.jsx)(n.p,{children:"A robot is blind and deaf without its sensors. A critical function of a digital twin is to generate realistic sensor data that our perception and navigation algorithms can consume. In this chapter, we'll explore how to add and configure common robotic sensors in our simulators."}),"\n",(0,i.jsx)(n.p,{children:"The process is always the same:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Attach"})," a sensor to a link in your robot's URDF or SDF model."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Add"})," a plugin that tells the simulator how the sensor works and where to publish its data."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Subscribe"})," to the corresponding ROS 2 topic in your code to receive the simulated data."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"1-cameras",children:"1. Cameras"}),"\n",(0,i.jsx)(n.p,{children:'Cameras are the "eyes" of the robot, providing rich visual information about the world.'}),"\n",(0,i.jsx)(n.h3,{id:"gazebo",children:"Gazebo"}),"\n",(0,i.jsxs)(n.p,{children:["In Gazebo, you use the ",(0,i.jsx)(n.code,{children:"gazebo_ros_camera"})," plugin. You add a ",(0,i.jsx)(n.code,{children:"<sensor>"}),' tag to your robot description file, attach it to a link (like the robot\'s "head"), and configure its properties.']}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="head_link">\n  <sensor type="camera" name="head_camera_sensor">\n    <update_rate>30.0</update_rate>\n    <camera name="head">\n      <horizontal_fov>1.396</horizontal_fov>\n      <image>\n        <width>800</width>\n        <height>600</height>\n        <format>R8G8B8</format>\n      </image>\n    </camera>\n    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n      <ros>\n        <namespace>head_camera</namespace>\n        <argument>--ros-args -r image_raw:=image_raw</argument>\n      </ros>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,i.jsx)(n.p,{children:"This SDF snippet does the following:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Creates a camera sensor and attaches it to ",(0,i.jsx)(n.code,{children:"head_link"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"Sets the update rate to 30 Hz."}),"\n",(0,i.jsx)(n.li,{children:"Defines the image properties (resolution, field of view)."}),"\n",(0,i.jsxs)(n.li,{children:["Loads the ",(0,i.jsx)(n.code,{children:"libgazebo_ros_camera.so"})," plugin."]}),"\n",(0,i.jsxs)(n.li,{children:["Tells the plugin to publish the images to the ",(0,i.jsx)(n.code,{children:"/head_camera/image_raw"})," topic."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Your ROS 2 perception nodes can now subscribe to ",(0,i.jsx)(n.code,{children:"/head_camera/image_raw"})," to get ",(0,i.jsx)(n.code,{children:"sensor_msgs/msg/Image"})," data, just as if it were a real camera."]}),"\n",(0,i.jsx)(n.h3,{id:"unity",children:"Unity"}),"\n",(0,i.jsxs)(n.p,{children:["In Unity, you simply add a ",(0,i.jsx)(n.code,{children:"Camera"})," object to your scene and attach it to your robot model. The Unity Robotics Hub provides a ",(0,i.jsx)(n.code,{children:"ROSPublisher"})," script that can be configured to publish the camera's rendered image to a ROS 2 topic. This is particularly powerful because you get the benefit of Unity's high-quality rendering pipeline for your simulated camera feed."]}),"\n",(0,i.jsx)(n.h2,{id:"2-lidar-laser-scanners",children:"2. LiDAR (Laser Scanners)"}),"\n",(0,i.jsx)(n.p,{children:"LiDAR is a crucial sensor for navigation and mapping, providing precise distance measurements."}),"\n",(0,i.jsx)(n.h3,{id:"gazebo-1",children:"Gazebo"}),"\n",(0,i.jsxs)(n.p,{children:["The process is very similar to the camera, but this time we use the ",(0,i.jsx)(n.code,{children:"gazebo_ros_ray_sensor"}),' plugin. "Ray sensor" is the generic Gazebo term for sensors that work by casting rays into the world, like LiDAR and sonar.']}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="base_link">\n  <sensor type="ray" name="lidar_sensor">\n    <pose>0 0 0.1 0 0 0</pose>\n    <visualize>true</visualize>\n    <update_rate>20</update_rate>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>720</samples>\n          <resolution>1</resolution>\n          <min_angle>-1.57</min_angle>\n          <max_angle>1.57</max_angle>\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.10</min>\n        <max>10.0</max>\n      </range>\n    </ray>\n    <plugin name="gazebo_ros_ray_sensor" filename="libgazebo_ros_ray_sensor.so">\n      <ros>\n        <namespace>lidar</namespace>\n        <argument>--ros-args -r scan:=scan</argument>\n      </ros>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,i.jsxs)(n.p,{children:["This plugin simulates a laser scanner by casting 720 rays in a 180-degree arc and then publishes the results as a ",(0,i.jsx)(n.code,{children:"sensor_msgs/msg/LaserScan"})," message to the ",(0,i.jsx)(n.code,{children:"/lidar/scan"})," topic."]}),"\n",(0,i.jsx)(n.h2,{id:"3-imus-inertial-measurement-units",children:"3. IMUs (Inertial Measurement Units)"}),"\n",(0,i.jsx)(n.p,{children:"An IMU is a device that measures a robot's orientation, angular velocity, and linear acceleration. It's essential for balance and estimating the robot's state."}),"\n",(0,i.jsx)(n.h3,{id:"gazebo-2",children:"Gazebo"}),"\n",(0,i.jsxs)(n.p,{children:["Gazebo provides the ",(0,i.jsx)(n.code,{children:"gazebo_ros_imu_sensor"})," plugin."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="imu_link">\n  <sensor name="imu_sensor" type="imu">\n    <update_rate>100</update_rate>\n    <plugin name="imu_plugin" filename="libgazebo_ros_imu_sensor.so">\n      <ros>\n        <namespace>imu</namespace>\n        <argument>--ros-args -r out:=data</argument>\n      </ros>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,i.jsxs)(n.p,{children:["This plugin simulates the noise and biases of a real IMU and publishes a ",(0,i.jsx)(n.code,{children:"sensor_msgs/msg/Imu"})," message to the ",(0,i.jsx)(n.code,{children:"/imu/data"})," topic."]}),"\n",(0,i.jsx)(n.h2,{id:"4-depth-cameras",children:"4. Depth Cameras"}),"\n",(0,i.jsx)(n.p,{children:"A depth camera is a special type of camera where each pixel represents a distance instead of a color. They are extremely useful for 3D perception."}),"\n",(0,i.jsx)(n.h3,{id:"gazebo-3",children:"Gazebo"}),"\n",(0,i.jsxs)(n.p,{children:["You can use the same ",(0,i.jsx)(n.code,{children:"gazebo_ros_camera"})," plugin as a regular camera, but you change the image format and add depth camera-specific settings. The plugin can publish multiple synchronized topics:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"/camera/image_raw"})," (the color image)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"/camera/depth/image_raw"})," (the depth image)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"/camera/points"})," (a ",(0,i.jsx)(n.code,{children:"PointCloud2"})," message generated from the depth data)"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This ability to generate perfect, per-pixel depth information and point clouds is one of the great advantages of simulation."}),"\n",(0,i.jsx)(n.p,{children:"In the next chapter, we'll put this all together to build a complete simulation of our humanoid robot that can be controlled and monitored entirely through ROS 2."})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>t});var s=a(6540);const i={},r=s.createContext(i);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);