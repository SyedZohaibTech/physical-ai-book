"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[1057],{8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var i=t(6540);const o={},s=i.createContext(o);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(s.Provider,{value:n},e.children)}},9592:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-4-vla/integrating-vla","title":"5. Integrating Visual Perception into VLA","description":"Our VLA pipeline can now understand spoken language and translate it into a sequence of abstract actions. However, the actions currently rely on predefined locations (\\"kitchen\\", \\"bedroom\\") and abstract object names (\\"mug\\"). For true autonomy, our robot needs to visually perceive these objects and their locations in the real world.","source":"@site/docs/module-4-vla/5-integrating-vla.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/integrating-vla","permalink":"/physical-ai-book/docs/module-4-vla/integrating-vla","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-ai-book/physical-ai-book/tree/main/docs/module-4-vla/5-integrating-vla.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"5. Integrating Visual Perception into VLA","sidebar_label":"Integrating VLA","sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Cognitive Planning Pipelines","permalink":"/physical-ai-book/docs/module-4-vla/cognitive-planning-pipelines"},"next":{"title":"Exercises","permalink":"/physical-ai-book/docs/module-4-vla/exercises"}}');var o=t(4848),s=t(8453);const a={title:"5. Integrating Visual Perception into VLA",sidebar_label:"Integrating VLA",sidebar_position:5},r="5. Integrating Visual Perception into VLA",c={},l=[{value:"The Role of Visual Perception",id:"the-role-of-visual-perception",level:2},{value:"Visual-Language Models (VLMs) and Object Grounding",id:"visual-language-models-vlms-and-object-grounding",level:2},{value:"Object Detection Pipeline",id:"object-detection-pipeline",level:2},{value:"Components:",id:"components",level:3},{value:"Updating the Cognitive Planning Pipeline",id:"updating-the-cognitive-planning-pipeline",level:2},{value:"Modifying the Action Executor",id:"modifying-the-action-executor",level:3},{value:"Example: A Simplified Object Database",id:"example-a-simplified-object-database",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"5-integrating-visual-perception-into-vla",children:"5. Integrating Visual Perception into VLA"})}),"\n",(0,o.jsxs)(n.p,{children:['Our VLA pipeline can now understand spoken language and translate it into a sequence of abstract actions. However, the actions currently rely on predefined locations ("kitchen", "bedroom") and abstract object names ("mug"). For true autonomy, our robot needs to ',(0,o.jsx)(n.strong,{children:"visually perceive"})," these objects and their locations in the real world."]}),"\n",(0,o.jsx)(n.p,{children:"This chapter bridges the gap between language commands and visual understanding, allowing our robot to dynamically identify and interact with objects."}),"\n",(0,o.jsx)(n.h2,{id:"the-role-of-visual-perception",children:"The Role of Visual Perception"}),"\n",(0,o.jsx)(n.p,{children:'When a human says, "Go to the red cup," the robot needs to:'}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:'Identify "red cup"'}),": This requires an object detection or segmentation model."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:'Locate "red cup"'}),": This requires extracting its 3D position in the robot's coordinate frame."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:'Associate "red cup" with a goal'}),": Pass its 3D pose to a navigation or manipulation skill."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"visual-language-models-vlms-and-object-grounding",children:"Visual-Language Models (VLMs) and Object Grounding"}),"\n",(0,o.jsxs)(n.p,{children:["Modern approaches often use ",(0,o.jsx)(n.strong,{children:"Visual-Language Models (VLMs)"})," that can take both text and images as input. These models can perform tasks like:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object Grounding"}),': Given an image and a text query ("red cup"), highlight the region in the image corresponding to the object.']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Referring Expression Comprehension"}),": Identify specific objects based on descriptive phrases."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"For our purposes, we'll simplify and integrate a separate object detection pipeline that will feed its results into our cognitive planner."}),"\n",(0,o.jsx)(n.h2,{id:"object-detection-pipeline",children:"Object Detection Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"We can use a pre-trained object detection model (like YOLO, EfficientDet, or even a foundation model like Segment Anything Model (SAM) combined with Grounding DINO) to identify objects in the robot's camera feed."}),"\n",(0,o.jsx)(n.p,{children:"This pipeline typically looks like this:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-mermaid",children:"graph TD\n    subgraph Robot Perception\n        A[Camera (Image Topic)] --\x3e B(Object Detection Node);\n        B -- Detected Objects (Bounding Boxes, Classes) --\x3e C(3D Pose Estimation Node);\n    end\n    C -- 3D Object Poses --\x3e D[Object Database / World Model];\n"})}),"\n",(0,o.jsx)(n.h3,{id:"components",children:"Components:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object Detection Node"}),": Subscribes to the robot's camera image topic (",(0,o.jsx)(n.code,{children:"/camera/image_raw"}),'). It runs an object detection model and publishes a list of detected objects, including their class (e.g., "cup", "book") and 2D bounding boxes in the image.']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"3D Pose Estimation Node"}),": Takes the 2D bounding boxes from the object detection node and combines them with depth information (from a depth camera or stereo camera) to estimate the 3D position and orientation (pose) of the detected objects relative to the robot."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object Database / World Model"}),": A component (can be a simple list or a more complex knowledge graph) that stores the 3D poses and properties of all known objects in the environment. This is what our LLM-driven cognitive planner will query."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"updating-the-cognitive-planning-pipeline",children:"Updating the Cognitive Planning Pipeline"}),"\n",(0,o.jsxs)(n.p,{children:["Now, let's integrate this visual perception into our existing pipeline. The key change is that when the LLM suggests an action like ",(0,o.jsx)(n.code,{children:"pick_up(mug)"}),', our action executor no longer relies on an abstract "mug" but needs to query the object database for the ',(0,o.jsx)(n.em,{children:"actual 3D pose"})," of a mug in the environment."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-mermaid",children:'graph TD\n    subgraph "Human Interface"\n        A[Voice Command (Whisper)]\n    end\n    subgraph "Cognitive Layer"\n        B(LLM Planner)\n        C(Action Executor)\n        D{Object Database (from Perception)}\n    end\n    subgraph "Robot Execution"\n        E(Navigation / Manipulation Skills)\n    end\n    \n    A -- Transcribed Text --\x3e B;\n    B -- Abstract Plan (e.g., pick_up(mug)) --\x3e C;\n    C -- "Query mug\'s pose?" --\x3e D;\n    D -- "Mug at [x,y,z]" --\x3e C;\n    C -- Execute concrete ROS 2 actions --\x3e E;\n'})}),"\n",(0,o.jsx)(n.h3,{id:"modifying-the-action-executor",children:"Modifying the Action Executor"}),"\n",(0,o.jsxs)(n.p,{children:["Our ",(0,o.jsx)(n.code,{children:"ActionExecutor"})," node needs to be updated to:"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Maintain an internal representation"})," of known objects and their 3D poses (this could be a simple dictionary). This representation would be updated by subscribing to a ",(0,o.jsx)(n.code,{children:"/detected_objects"})," topic published by our perception nodes."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Modify skill execution"}),": When it receives an action like ",(0,o.jsx)(n.code,{children:"pick_up(object_name)"}),", it will:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Look up ",(0,o.jsx)(n.code,{children:"object_name"})," in its internal object database to get its 3D pose."]}),"\n",(0,o.jsxs)(n.li,{children:["Pass this 3D pose to the underlying ",(0,o.jsx)(n.code,{children:"pick_up"})," skill (e.g., a manipulation action server)."]}),"\n",(0,o.jsx)(n.li,{children:'If the object is not found, it might ask the LLM for clarification or execute a "search" behavior.'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This integration makes the robot truly intelligent. Instead of relying on a predefined world, it actively senses and understands its surroundings to fulfill commands."}),"\n",(0,o.jsx)(n.h2,{id:"example-a-simplified-object-database",children:"Example: A Simplified Object Database"}),"\n",(0,o.jsxs)(n.p,{children:["Let's assume we have a perception node that detects objects and publishes their 3D poses to a ",(0,o.jsx)(n.code,{children:"/detected_objects"})," topic (e.g., ",(0,o.jsx)(n.code,{children:"geometry_msgs/msg/PoseStamped"}),"). Our ",(0,o.jsx)(n.code,{children:"ActionExecutor"})," would subscribe to this."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Inside ActionExecutor __init__\n# ...\nself.object_poses = {} # Dictionary to store object_name -> PoseStamped\nself.object_sub = self.create_subscription(\n    PoseStamped, # Assuming one object at a time for simplicity\n    \'/detected_objects\',\n    self.object_detection_callback,\n    10\n)\n\ndef object_detection_callback(self, msg):\n    # In a real system, msg would contain object name, class, etc.\n    # For simplicity, let\'s assume it\'s always "mug" for now, or infer from topic name.\n    # We would need a custom message type for multiple objects.\n    object_name = "mug" # This needs to be smarter\n    self.object_poses[object_name] = msg.pose\n    self.get_logger().info(f"Updated pose for {object_name}: {msg.pose.position.x}, {msg.pose.position.y}")\n\nasync def execute_pick_up(self, object_name):\n    if object_name in self.object_poses:\n        target_pose = self.object_poses[object_name]\n        self.get_logger().info(f"Executing pick_up for {object_name} at {target_pose.position.x}, {target_pose.position.y}")\n        # Call a manipulation action client with target_pose\n        # ... (simplified for concept)\n        await self.execute_manipulation_action(target_pose)\n    else:\n        self.get_logger().warn(f"Object \'{object_name}\' not found in current perception.")\n        # Perhaps trigger a search behavior or ask LLM for clarification\n'})}),"\n",(0,o.jsx)(n.p,{children:"This updated action executor shows how the robot can dynamically retrieve object information, making its actions context-aware and responsive to the environment. In the next chapter, we'll bring all these components together in a complete humanoid demonstration."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);