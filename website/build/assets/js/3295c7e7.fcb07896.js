"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[3892],{6148:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vla/llm-to-ros-actions","title":"3. LLM to ROS Actions: The Cognitive Core","description":"We can now turn spoken commands into text. The next crucial step in our VLA pipeline is to convert this natural language text into a sequence of executable robot actions. This is the cognitive core of our robot\'s intelligence, typically handled by a Large Language Model (LLM).","source":"@site/docs/module-4-vla/3-llm-to-ros-actions.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/llm-to-ros-actions","permalink":"/physical-ai-book/docs/module-4-vla/llm-to-ros-actions","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-ai-book/physical-ai-book/tree/main/docs/module-4-vla/3-llm-to-ros-actions.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"3. LLM to ROS Actions: The Cognitive Core","sidebar_label":"LLM to ROS Actions","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Whisper Voice Commands","permalink":"/physical-ai-book/docs/module-4-vla/whisper-voice-commands"},"next":{"title":"Cognitive Planning Pipelines","permalink":"/physical-ai-book/docs/module-4-vla/cognitive-planning-pipelines"}}');var s=o(4848),i=o(8453);const r={title:"3. LLM to ROS Actions: The Cognitive Core",sidebar_label:"LLM to ROS Actions",sidebar_position:3},a="3. LLM to ROS Actions: The Cognitive Core",l={},c=[{value:"The Role of the LLM in Robotics",id:"the-role-of-the-llm-in-robotics",level:2},{value:"Prompt Engineering for Robot Actions",id:"prompt-engineering-for-robot-actions",level:2},{value:"Example Prompt (Simplified)",id:"example-prompt-simplified",level:3},{value:"Mock LLM for the Textbook",id:"mock-llm-for-the-textbook",level:2},{value:"Setup and Running",id:"setup-and-running",level:3}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"3-llm-to-ros-actions-the-cognitive-core",children:"3. LLM to ROS Actions: The Cognitive Core"})}),"\n",(0,s.jsxs)(n.p,{children:["We can now turn spoken commands into text. The next crucial step in our VLA pipeline is to convert this natural language text into a sequence of executable robot actions. This is the ",(0,s.jsx)(n.strong,{children:"cognitive core"})," of our robot's intelligence, typically handled by a Large Language Model (LLM)."]}),"\n",(0,s.jsx)(n.h2,{id:"the-role-of-the-llm-in-robotics",children:"The Role of the LLM in Robotics"}),"\n",(0,s.jsx)(n.p,{children:"An LLM's strength lies in its ability to understand and generate human-like text, reason about tasks, and even generate code. In robotics, we can leverage these capabilities to:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Task Decomposition"}),': Break down a complex, high-level instruction ("Make me coffee") into a series of smaller, more manageable sub-tasks ("Go to kitchen," "Get mug," "Brew coffee").']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Sequencing"}),": Order these sub-tasks logically to achieve the overall goal."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Parameter Grounding"}),': Extract key entities and parameters from the natural language command (e.g., "red mug," "table," "5 meters") and map them to the robot\'s capabilities and environment.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Error Handling"}),": Potentially reason about failures and suggest recovery actions."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prompt-engineering-for-robot-actions",children:"Prompt Engineering for Robot Actions"}),"\n",(0,s.jsxs)(n.p,{children:["The key to getting an LLM to generate robot actions is ",(0,s.jsx)(n.strong,{children:"prompt engineering"}),". We provide the LLM with a carefully crafted prompt that includes:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"System Role"}),': Defining the LLM\'s role (e.g., "You are a helpful robot assistant.").']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Available Tools/Actions"}),": Listing the robot's capabilities (e.g., ",(0,s.jsx)(n.code,{children:"navigate_to(location)"}),", ",(0,s.jsx)(n.code,{children:"pick_up(object_name)"}),")."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context"}),": Information about the robot's current state and environment."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"User Command"}),": The natural language instruction."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Desired Output Format"}),": Specifying how the LLM should output the action sequence (e.g., a JSON list of actions)."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"example-prompt-simplified",children:"Example Prompt (Simplified)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'You are a helpful robot assistant. Your goal is to translate human commands into a sequence of executable robot actions.\n\nAvailable actions:\n- navigate_to(location_name: str) -> bool: Navigates the robot to a predefined location. Returns True on success.\n- pick_up(object_name: str) -> bool: Picks up a specified object. Returns True on success.\n- say(phrase: str) -> None: Makes the robot speak a phrase.\n\nCurrent known locations: "kitchen", "bedroom", "living_room"\nCurrent known objects: "mug", "book", "water_bottle"\n\nHuman command: "Go to the kitchen and grab the mug."\n\nPlease output a JSON list of actions the robot should perform.\n'})}),"\n",(0,s.jsx)(n.p,{children:"The LLM, given this prompt, might respond with:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'[\n    {"action": "navigate_to", "args": {"location_name": "kitchen"}},\n    {"action": "pick_up", "args": {"object_name": "mug"}}\n]\n'})}),"\n",(0,s.jsx)(n.h2,{id:"mock-llm-for-the-textbook",children:"Mock LLM for the Textbook"}),"\n",(0,s.jsxs)(n.p,{children:["As discussed in the Research Outcomes, we will use a ",(0,s.jsx)(n.strong,{children:"mock LLM"})," implementation for this textbook. This avoids the complexity of setting up API keys, managing costs, and dealing with potential latency issues of real LLM APIs. The focus remains on the ",(0,s.jsx)(n.em,{children:"integration"})," of the LLM's output with the robot's systems."]}),"\n",(0,s.jsx)(n.p,{children:"Our mock LLM will be a Python function that takes a text command and returns a predefined sequence of actions based on simple string matching."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# ~/ros2_ws/src/llm_planner_pkg/llm_planner_pkg/mock_llm_planner.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom custom_interfaces.srv import GetActionPlan # Our custom service from contracts.md\nimport json\n\nclass MockLLMPlanner(Node):\n\n    def __init__(self):\n        super().__init__(\'mock_llm_planner\')\n        self.srv = self.create_service(GetActionPlan, \'get_action_plan\', self.get_action_plan_callback)\n        self.get_logger().info(\'Mock LLM Planner Service Ready.\')\n\n    def get_action_plan_callback(self, request, response):\n        self.get_logger().info(f\'Received command: "{request.command}"\')\n        \n        action_plan = []\n\n        if "go to kitchen and grab the mug" in request.command.lower():\n            action_plan.append("navigate_to(kitchen)")\n            action_plan.append("pick_up(mug)")\n        elif "go to bedroom" in request.command.lower():\n            action_plan.append("navigate_to(bedroom)")\n        elif "tell me a joke" in request.command.lower():\n            action_plan.append("say(Why did the robot cross the road? To get to the other bytes!)")\n        else:\n            action_plan.append("say(I\'m sorry, I don\'t understand that command.)")\n        \n        response.action_plan = action_plan\n        self.get_logger().info(f\'Responding with plan: {action_plan}\')\n        return response\n\ndef main(args=None):\n    rclpy.init(args=args)\n    mock_llm_planner = MockLLMPlanner()\n    rclpy.spin(mock_llm_planner)\n    mock_llm_planner.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"setup-and-running",children:"Setup and Running"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Define Custom Service"}),": Ensure you have defined the ",(0,s.jsx)(n.code,{children:"custom_interfaces/srv/GetActionPlan.srv"})," as specified in ",(0,s.jsx)(n.code,{children:"contracts.md"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Create ROS 2 package"}),":","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\nros2 pkg create --build-type ament_python llm_planner_pkg --dependencies rclpy custom_interfaces\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Place code"}),": Put the ",(0,s.jsx)(n.code,{children:"mock_llm_planner.py"})," file into ",(0,s.jsx)(n.code,{children:"~/ros2_ws/src/llm_planner_pkg/llm_planner_pkg/"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["Update ",(0,s.jsx)(n.code,{children:"setup.py"})]}),": Add an entry point for your node in ",(0,s.jsx)(n.code,{children:"~/ros2_ws/src/llm_planner_pkg/setup.py"})," under ",(0,s.jsx)(n.code,{children:"entry_points"}),":","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"entry_points={\n    'console_scripts': [\n        'mock_llm_planner = llm_planner_pkg.mock_llm_planner:main',\n    ],\n},\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Build"}),":","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws\ncolcon build\nsource install/setup.bash\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Run"}),":","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 run llm_planner_pkg mock_llm_planner\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Test the Service"}),": In another terminal, call the service:","\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 service call /get_action_plan custom_interfaces/srv/GetActionPlan \"command: 'Go to the kitchen and grab the mug.'\"\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This mock planner acts as a stand-in for a powerful LLM, demonstrating how a robot can receive a natural language command and internally generate a structured plan. In the next chapter, we'll build the system that executes these plans."})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>a});var t=o(6540);const s={},i=t.createContext(s);function r(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);