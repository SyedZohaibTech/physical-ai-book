"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[3205],{46:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>a,default:()=>g,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4-vla/cognitive-planning-pipelines","title":"4. Cognitive Planning Pipelines","description":"We now have the ability to translate voice commands into text using Whisper and to generate a sequence of abstract actions from an LLM (or our mock LLM). The next challenge is to build a Cognitive Planning Pipeline that takes this abstract action plan and translates it into concrete, executable ROS 2 commands for our robot.","source":"@site/docs/module-4-vla/4-cognitive-planning-pipelines.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/cognitive-planning-pipelines","permalink":"/physical-ai-book/docs/module-4-vla/cognitive-planning-pipelines","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-ai-book/physical-ai-book/tree/main/docs/module-4-vla/4-cognitive-planning-pipelines.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"4. Cognitive Planning Pipelines","sidebar_label":"Cognitive Planning Pipelines","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"LLM to ROS Actions","permalink":"/physical-ai-book/docs/module-4-vla/llm-to-ros-actions"},"next":{"title":"Integrating VLA","permalink":"/physical-ai-book/docs/module-4-vla/integrating-vla"}}');var o=t(4848),s=t(8453);const l={title:"4. Cognitive Planning Pipelines",sidebar_label:"Cognitive Planning Pipelines",sidebar_position:4},a="4. Cognitive Planning Pipelines",r={},c=[{value:"The Role of the Action Executor",id:"the-role-of-the-action-executor",level:2},{value:"Designing Robot Skills",id:"designing-robot-skills",level:2},{value:"Example: A Simple Action Executor",id:"example-a-simple-action-executor",level:3},{value:"Setup and Running",id:"setup-and-running",level:3},{value:"Testing the Pipeline",id:"testing-the-pipeline",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"4-cognitive-planning-pipelines",children:"4. Cognitive Planning Pipelines"})}),"\n",(0,o.jsxs)(n.p,{children:["We now have the ability to translate voice commands into text using Whisper and to generate a sequence of abstract actions from an LLM (or our mock LLM). The next challenge is to build a ",(0,o.jsx)(n.strong,{children:"Cognitive Planning Pipeline"})," that takes this abstract action plan and translates it into concrete, executable ROS 2 commands for our robot."]}),"\n",(0,o.jsx)(n.p,{children:"This pipeline is the bridge between high-level intelligence and low-level robot execution."}),"\n",(0,o.jsx)(n.h2,{id:"the-role-of-the-action-executor",children:"The Role of the Action Executor"}),"\n",(0,o.jsx)(n.p,{children:"The action executor is a ROS 2 node that:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Subscribes"})," to the ",(0,o.jsx)(n.code,{children:"/voice_command"})," topic from the Whisper node."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Calls"})," the LLM service (our ",(0,o.jsx)(n.code,{children:"mock_llm_planner"}),") to get an action plan."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Executes"})," each action in the plan by calling the appropriate ROS 2 service, action client, or publishing to a topic."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Provides Feedback"}),": In a more advanced system, it would monitor the execution of each action and provide feedback or handle failures."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"designing-robot-skills",children:"Designing Robot Skills"}),"\n",(0,o.jsxs)(n.p,{children:["For the action executor to work, we need to define a set of ",(0,o.jsx)(n.strong,{children:"robot skills"}),". Each skill corresponds to a capability of the robot that can be invoked by the action executor. These skills wrap the underlying ROS 2 services, actions, or topic publications."]}),"\n",(0,o.jsxs)(n.p,{children:["For example, if the LLM generates the action ",(0,o.jsx)(n.code,{children:"navigate_to(kitchen)"}),", the action executor needs a ",(0,o.jsx)(n.code,{children:"navigate_to"})," skill that:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'Translates "kitchen" into a specific target pose for Nav2.'}),"\n",(0,o.jsxs)(n.li,{children:["Sends a goal to the Nav2 action server (",(0,o.jsx)(n.code,{children:"/navigate_to_pose"}),")."]}),"\n",(0,o.jsx)(n.li,{children:"Waits for the result."}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Similarly, for ",(0,o.jsx)(n.code,{children:"pick_up(mug)"}),", we need a ",(0,o.jsx)(n.code,{children:"pick_up"})," skill that:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Locates the mug using perception (which we'll cover in the next chapter)."}),"\n",(0,o.jsx)(n.li,{children:"Plans a grasping trajectory."}),"\n",(0,o.jsx)(n.li,{children:"Sends commands to the robot's arm to execute the grasp."}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"example-a-simple-action-executor",children:"Example: A Simple Action Executor"}),"\n",(0,o.jsxs)(n.p,{children:["Let's build a simplified action executor that can handle the mock LLM's output. For now, we'll implement ",(0,o.jsx)(n.code,{children:"navigate_to"})," and ",(0,o.jsx)(n.code,{children:"say"})," using ROS 2 services and topics."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# ~/ros2_ws/src/action_executor_pkg/action_executor_pkg/executor_node.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom custom_interfaces.srv import GetActionPlan # From our LLM planner\nfrom rclpy.action import ActionClient # For Nav2\nfrom nav2_msgs.action import NavigateToPose # For Nav2\nfrom geometry_msgs.msg import PoseStamped # For Nav2 goal\n\nimport json\nimport time\n\nclass ActionExecutor(Node):\n\n    def __init__(self):\n        super().__init__('action_executor')\n        self.get_logger().info('Action Executor Node Started.')\n\n        # Subscribe to voice commands\n        self.voice_command_subscription = self.create_subscription(\n            String,\n            '/voice_command',\n            self.voice_command_callback,\n            10\n        )\n\n        # Create client for LLM service\n        self.llm_client = self.create_client(GetActionPlan, 'get_action_plan')\n        while not self.llm_client.wait_for_service(timeout_sec=1.0):\n            self.get_logger().info('LLM service not available, waiting...')\n\n        # Create client for Nav2 NavigateToPose action\n        self.nav_action_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n        while not self.nav_action_client.wait_for_server(timeout_sec=1.0):\n            self.get_logger().info('Navigation action server not available, waiting...')\n        \n        # Publisher for robot speech (mock for now)\n        self.speech_publisher = self.create_publisher(String, '/robot_speech', 10)\n\n        self.get_logger().info('All services and action servers are connected.')\n\n    async def voice_command_callback(self, msg):\n        self.get_logger().info(f'Received voice command: \"{msg.data}\"')\n        \n        # 1. Call LLM to get action plan\n        llm_request = GetActionPlan.Request()\n        llm_request.command = msg.data\n        \n        self.get_logger().info('Requesting action plan from LLM...')\n        llm_response = await self.llm_client.call_async(llm_request)\n        \n        if llm_response.action_plan:\n            self.get_logger().info(f'Received plan: {llm_response.action_plan}')\n            # 2. Execute each action in the plan\n            for action_str in llm_response.action_plan:\n                self.get_logger().info(f'Executing: {action_str}')\n                if action_str.startswith(\"navigate_to\"):\n                    location = action_str.split('(')[1].split(')')[0]\n                    await self.execute_navigate_to(location)\n                elif action_str.startswith(\"say\"):\n                    phrase = action_str.split('(')[1].split(')')[0]\n                    self.execute_say(phrase)\n                else:\n                    self.get_logger().warn(f\"Unknown action: {action_str}\")\n        else:\n            self.get_logger().warn(\"LLM returned an empty plan.\")\n\n    async def execute_navigate_to(self, location):\n        self.get_logger().info(f\"Executing navigation to: {location}\")\n        # Mocking specific poses for locations (in a real system, this would come from a map)\n        if location == \"kitchen\":\n            target_x, target_y = 5.0, 0.0\n        elif location == \"bedroom\":\n            target_x, target_y = 0.0, 5.0\n        else:\n            self.get_logger().warn(f\"Unknown location: {location}\")\n            return\n\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = 'map'\n        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()\n        goal_msg.pose.pose.position.x = target_x\n        goal_msg.pose.pose.position.y = target_y\n        goal_msg.pose.pose.orientation.w = 1.0 # No rotation\n\n        self.get_logger().info(f\"Sending navigation goal to Nav2: {target_x}, {target_y}\")\n        future = self.nav_action_client.send_goal_async(goal_msg)\n        goal_handle = await future\n\n        if not goal_handle.accepted:\n            self.get_logger().error('Goal rejected :(')\n            return\n\n        self.get_logger().info('Goal accepted :)')\n        result_future = goal_handle.get_result_async()\n        result = await result_future\n        \n        if result.status == ActionClient.GoalStatus.SUCCEEDED:\n            self.get_logger().info(f'Navigation to {location} succeeded!')\n        else:\n            self.get_logger().error(f'Navigation to {location} failed with status: {result.status}')\n\n\n    def execute_say(self, phrase):\n        self.get_logger().info(f\"Robot says: {phrase}\")\n        msg = String()\n        msg.data = phrase\n        self.speech_publisher.publish(msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    action_executor = ActionExecutor()\n    # Use rclpy.spin_until_future_complete for async operations\n    rclpy.spin(action_executor) \n    action_executor.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"setup-and-running",children:"Setup and Running"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Custom Interface"}),": Ensure ",(0,o.jsx)(n.code,{children:"custom_interfaces"})," package and ",(0,o.jsx)(n.code,{children:"GetActionPlan.srv"})," are built."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Create ROS 2 package"}),":","\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\nros2 pkg create --build-type ament_python action_executor_pkg --dependencies rclpy std_msgs custom_interfaces nav2_msgs geometry_msgs\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Place code"}),": Put ",(0,o.jsx)(n.code,{children:"executor_node.py"})," into ",(0,o.jsx)(n.code,{children:"~/ros2_ws/src/action_executor_pkg/action_executor_pkg/"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsxs)(n.strong,{children:["Update ",(0,o.jsx)(n.code,{children:"setup.py"})]}),": Add an entry point."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Build"}),": ",(0,o.jsx)(n.code,{children:"colcon build"})," and ",(0,o.jsx)(n.code,{children:"source install/setup.bash"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Run"}),": You'll need to run several nodes simultaneously:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Your Whisper node (",(0,o.jsx)(n.code,{children:"ros2 run voice_command_pkg whisper_node"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:["Your Mock LLM Planner (",(0,o.jsx)(n.code,{children:"ros2 run llm_planner_pkg mock_llm_planner"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Crucially, a running Nav2 stack"})," (e.g., in Isaac Sim or Gazebo)"]}),"\n",(0,o.jsxs)(n.li,{children:["Your Action Executor (",(0,o.jsx)(n.code,{children:"ros2 run action_executor_pkg executor_node"}),")"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"testing-the-pipeline",children:"Testing the Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"With all components running:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:'Speak a command into your microphone, e.g., "Go to the kitchen".'}),"\n",(0,o.jsxs)(n.li,{children:["The Whisper node transcribes it and publishes to ",(0,o.jsx)(n.code,{children:"/voice_command"}),"."]}),"\n",(0,o.jsx)(n.li,{children:"The Action Executor subscribes to this, calls the LLM service with the text."}),"\n",(0,o.jsxs)(n.li,{children:["The Mock LLM returns ",(0,o.jsx)(n.code,{children:'["navigate_to(kitchen)"]'}),"."]}),"\n",(0,o.jsxs)(n.li,{children:['The Action Executor translates "kitchen" into a ',(0,o.jsx)(n.code,{children:"NavigateToPose"})," goal and sends it to Nav2."]}),"\n",(0,o.jsx)(n.li,{children:"Nav2 (in your simulation) plans and executes the movement."}),"\n",(0,o.jsxs)(n.li,{children:['If you speak "Tell me a joke", the robot will "say" the joke via ',(0,o.jsx)(n.code,{children:"/robot_speech"})," topic."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This complete pipeline demonstrates how a robot can receive natural language input, parse it into a plan, and execute that plan using its underlying ROS 2 capabilities. The modular design allows you to easily swap out components, for instance, replacing the mock LLM with a real one."})]})}function g(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>l,x:()=>a});var i=t(6540);const o={},s=i.createContext(o);function l(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:l(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);